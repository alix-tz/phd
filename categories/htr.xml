<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="https://alix-tz.github.io/phd/assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>A research (b)log (Posts about HTR)</title><link>https://alix-tz.github.io/phd/</link><description></description><atom:link href="https://alix-tz.github.io/phd/categories/htr.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="https://alix-tz.github.io/phd/"&gt;Alix Chagué&lt;/a&gt; CC-BY</copyright><lastBuildDate>Sat, 21 Jun 2025 20:10:18 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>024 - The messy backstage of a literature review</title><link>https://alix-tz.github.io/phd/posts/024/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;A few weeks ago, I began a thorough review of articles published in four digital humanities venues to track mentions of automatic text recognition and understand how, where, and why scholars use it. Although I wish I had started sooner in my doctoral journey, I stay positive holding on to the idea that "it's never too late." I learn a lot about Digital Humanities as a field of research and gain a better understanding of ATR's presence in the field.&lt;/p&gt;
&lt;p&gt;While catching up on our dissertation progress, I was telling Roch Delanney about the survey I'm conducting, my goals for it, and how I selected and sorted the articles. Roch suggested that I share my method more widely. It seems a little clumsy at times, but I am also able to use many different skills I have learned and sharpened over the years so I think it is indeed interesting to share a bit of my &lt;em&gt;cuisine&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Perimeter of the literature review&lt;/h3&gt;
&lt;p&gt;My literature review focuses on four publication venues. I think they are, collectively, representative of research in the Digital Humanities: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://academic.oup.com/dsh"&gt;&lt;em&gt;Digital Scholarship in the Humanities&lt;/em&gt;&lt;/a&gt; (DSH), which is presented by the Alliance of Digital Humanities Organizations (ADHO) as an international, peer-reviewed journal published by Oxford University Press on behalf of ADHO and the European Association for Digital Humanities (EADH). It was published under the title &lt;em&gt;Literary and Linguistic Computing: The Journal of Digital Scholarship in the Humanities&lt;/em&gt; until 2014. I counted a total of 174 volumes for a total of 1741 articles (excluding retracted articles, book reviews, editorials and committee reports) published since 1985 until the first half of 2025.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://dhq.digitalhumanities.org/"&gt;&lt;em&gt;Digital Humanities Quarterly&lt;/em&gt;&lt;/a&gt; (DHQ) is an open-access peer-reviewed journal, probably more representative of research in North America. It is published by the Association for Computers and the Humanities (ACH). I counted a total of 790 articles published since its first issue in 2007. Most articles are in English.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;a href="https://jdmdh.episciences.org/"&gt;&lt;em&gt;Journal of Data Mining and Digital Humanities&lt;/em&gt;&lt;/a&gt; (JDMDH), is published by Episciences since 2017. Contrary to DHQ, its focus is more European-centric, and it has a special volume dedicated specifically to automatic text recognition (directed by Ariane Pinche and Peter Stokes). I found a total of 162 articles published in JDMDH, including the special volume on ATR.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lastly, the proceedings from the more recent &lt;em&gt;Computational Humanities Research&lt;/em&gt; (CHR) conferences (see the &lt;a href="https://2024.computational-humanities-research.org/"&gt;2024 conference proceedings&lt;/a&gt; for example) offer a perspective on research focused on more intensively computational methods in the Humanities. The conference is held annually since 2021. I found a total of 214 articles in the proceedings.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Aside from DSH, that I can access thanks to the library of the University of Montréal, all the other journals are in open access. &lt;/p&gt;
&lt;h3&gt;Collecting the articles and their metadata&lt;/h3&gt;
&lt;p&gt;For JDMDH, articles are not centralized on the journal website but rather published on platforms like &lt;a href="https://hal.archives-ouvertes.fr/"&gt;HAL&lt;/a&gt; or &lt;a href="https://arxiv.org/"&gt;arXiv&lt;/a&gt; and sometimes &lt;a href="https://zenodo.org/"&gt;Zenodo&lt;/a&gt;. Getting an overview of the articles published in JDMDH is not straightforward, but it is possible to browse the articles per &lt;a href="https://jdmdh.episciences.org/browse/volumes"&gt;volumes&lt;/a&gt;. I opened and downloaded each article in each volume, as well as collected the article entries in Zotero using the Zotero connector. The process was cumbersome and required many clicks, but the variety of publishing platforms deterred me from writing a script to automate the downloading process.  &lt;/p&gt;
&lt;p&gt;CHR, on the other hand, was very easy to scrape, partly because there are only four volumes of proceedings so far. For each series of proceeding, the index of all articles is compatible with the batch import scenario of the Zotero connector. To collect the PDFs, I used a section of the HTML page and regular expressions to identify the links to the PDF files, creating a list of URLs. Finally, I used a Python script to download the PDFs to my computer.  &lt;/p&gt;
&lt;p&gt;For example, in &lt;a href="https://ceur-ws.org/Vol-2989/"&gt;https://ceur-ws.org/Vol-2989/&lt;/a&gt;, the &lt;code&gt;ul&lt;/code&gt; contains simple HTML elements pointing to the PDF files, such as: &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;h3&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"CEURSESSION"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Presented papers&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;h3&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;

&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;ul&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;li&lt;/span&gt; &lt;span class="na"&gt;id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"long_paper5"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="na"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"long_paper5.pdf"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"CEURTITLE"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Entity Matching in Digital Humanities Knowledge
      Graphs&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"CEURPAGES"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;1-15&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;br&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"CEURAUTHOR"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Juriaan Baas&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;,
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"CEURAUTHOR"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Mehdi M. Dastani&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;,
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"CEURAUTHOR"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Ad J. Feelders&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;li&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
...
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;All I had to do was copy and paste this entire list into a text editor (I like to use &lt;a href="https://www.sublimetext.com/"&gt;Sublime Text&lt;/a&gt; in such a situation). Then, I used a simple regular expression like &lt;code&gt;href=".+?"&lt;/code&gt; to select the value in the &lt;code&gt;a&lt;/code&gt; element, which contains the links to the PDF files. I kept only the selected text and then rebuilt the complete URL with a couple of replacements such as &lt;code&gt;href="&lt;/code&gt; -&amp;gt; &lt;code&gt;"https://ceur-ws.org/Vol-2989/&lt;/code&gt; and &lt;code&gt;"\n&lt;/code&gt; -&amp;gt; &lt;code&gt;",\n&lt;/code&gt;. At this point I just added square brackets around the selection, et voilà! I had a Python list ready to be passed to a script like the one below to download the files:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;list_of_urls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"https://ceur-ws.org/Vol-2723/short8.pdf"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s2"&gt;"https://ceur-ws.org/Vol-2723/long35.pdf"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s2"&gt;"https://ceur-ws.org/Vol-2723/long44.pdf"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="c1"&gt;#...&lt;/span&gt;
                &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;requests&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;tqdm&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;tqdm&lt;/span&gt; &lt;span class="c1"&gt;# it makes  progress bar so I know how long I can take to make a tea while the script runs&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tqdm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_of_urls&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;status_code&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"/"&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;-&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"/"&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
        &lt;span class="c1"&gt;#print(filename)&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"wb"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Failed to download: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# This cool down is to be polite to the server&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I used a similar approach for downloading the articles from DHQ because the &lt;a href="https://dhq.digitalhumanities.org/index/title.html"&gt;Index of Titles&lt;/a&gt; lists all of the published articles on a single page. I first downloaded the HTML pages of the articles (DHQ publishes articles in HTML format as well as PDF). I also used regular expressions to extract the list of links and used a Python script to download the files.  &lt;/p&gt;
&lt;p&gt;Unfortunately, the Zotero connector only works on each article page individually, but not for batch-import on the index page. I investigated a bit to understand why it was so, and found that in the source code of each article page, there is a &lt;code&gt;span&lt;/code&gt; element identified with the class &lt;code&gt;Z3988&lt;/code&gt; that the Zotero connector uses to extract the metadata and create an entry in Zotero. In DHQ, these spans look like this:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"Z3988"&lt;/span&gt; &lt;span class="na"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"url_ver=Z39.88-2004&amp;amp;amp;ctx_ver=Z39.88-2004&amp;amp;amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;amp;amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;amp;amp;rft.genre=article&amp;amp;amp;rft.atitle=Academics%20Retire%20and%20Servers%20Die%3A%20Adventures%20in%20the%20Hosting%20and%20Storage%20of%20Digital%20Humanities%20Projects&amp;amp;amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;amp;amp;rft.stitle=DHQ&amp;amp;amp;rft.issn=1938-4122&amp;amp;amp;rft.date=2023-05-26&amp;amp;amp;rft.volume=017&amp;amp;amp;rft.issue=1&amp;amp;amp;rft.aulast=Cummings&amp;amp;amp;rft.aufirst=James&amp;amp;amp;rft.au=James%20Cummings"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I understood recently, while discussing with Margot Mellet, that Z3988 is a reference to the &lt;a href="https://groups.niso.org/higherlogic/ws/public/download/14833/z39_88_2004_r2010.pdf"&gt;OpenURL Framework Standard (ISO Z 39.88-2004)&lt;/a&gt;, which is used by the Zotero connector. Also, I should note that such spans are not systematically used in online journals. JDMDH for example doesn't use them, and serves the metadata in a different way.  &lt;/p&gt;
&lt;p&gt;Since I had already downloaded all the articles from DHQ as HTML files, I wrote a simple Python script that found all of such spans for each downloaded article and aggregated them in a single, very simple HTML file. Then, I simply opened this page in my browser after emulating a local server&lt;sup id="fnref:python_server"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/024/#fn:python_server"&gt;1&lt;/a&gt;&lt;/sup&gt; (with a command like &lt;code&gt;python -m http.server&lt;/code&gt;), and I was able to use the Zotero connector to import all the articles in a single click. It was very satisfying! The only downside is that I couldn't collect the articles' abstracts because there weren't included in the spans.  &lt;/p&gt;
&lt;p&gt;DSH was different from the rest of the journals. Because of the longevity of the journal and the amount of articles it published, it was quite overwhelming. Unfortunately, it is a paywalled journal and I couldn't figure out how to make the proxy of the University of Montreal library work with my Python scripts and the command line. As a result, I had to manually download the articles,&lt;sup id="fnref:proxy_dsh"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/024/#fn:proxy_dsh"&gt;2&lt;/a&gt;&lt;/sup&gt; but only when they were relevant! Since DSH has a fairly good search engine that allows to do multi-keyword searches, I only downloaded articles matching my search criteria (143 in total).&lt;/p&gt;
&lt;p&gt;Additionally, I went through each of the 174 issues of DSH to batch-import the article references in Zotero. It was tedious but I figured I might be able to use these metadata for other projects in the future.  &lt;/p&gt;
&lt;h3&gt;Filtering the articles&lt;/h3&gt;
&lt;p&gt;For DHQ, JDMDH and CHR, I ran a keyword surch using the command &lt;a href="https://www.man7.org/linux/man-pages/man1/grep.1.html"&gt;&lt;code&gt;grep&lt;/code&gt;&lt;/a&gt; on the content of the articles. I didn't want to limit my search to the titles, abstract or keywords because I really wanted to include anecdotal mentions of automatic text recognition in my results.  &lt;/p&gt;
&lt;p&gt;To use grep, I created a file (pattern.txt) with the keywords I was looking for:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;HTR
OCR
text recognition
ATR
Transkribus
eScriptorium
automatic transcription
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I converted the PDFs into text files using the command &lt;a href="https://man.archlinux.org/man/pdftotext.1.en"&gt;pdftotext&lt;/a&gt;. This was necessary because grep cannot search inside a PDF directly. I didn't need to do this conversion for DHQ, since I had download HTML files from that journal. &lt;/p&gt;
&lt;p&gt;The commands to search inside the PDFs of one of the journals would look like this:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;ls&lt;span class="w"&gt; &lt;/span&gt;*.pdf&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;xargs&lt;span class="w"&gt; &lt;/span&gt;-n1&lt;span class="w"&gt; &lt;/span&gt;pdftotext&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# to convert PDFs to text files&lt;/span&gt;
grep&lt;span class="w"&gt; &lt;/span&gt;-i&lt;span class="w"&gt; &lt;/span&gt;-w&lt;span class="w"&gt; &lt;/span&gt;-m5&lt;span class="w"&gt; &lt;/span&gt;-H&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;../pattern.txt&lt;span class="w"&gt; &lt;/span&gt;*.txt&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# to search for the keywords in the text files and display the first 5 matches&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After controlling how grep matched the keywords, I used &lt;code&gt;grep -l -f ../pattern.txt *.txt&lt;/code&gt; to list the files that matched the keywords. This list was used to sort the documents into two folders, according to whether or not they matched my research.&lt;/p&gt;
&lt;p&gt;In the case of DSH, I directly used the search engine to combine the keywords, using the "OR" operator. I set the full text of the articles as the scope of my research: &lt;a href="https://academic.oup.com/dsh/search-results?allJournals=1&amp;amp;f_ContentType=Journal+Article&amp;amp;fl_SiteID=5447&amp;amp;cqb=[{%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22automatic%20transcription%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22transkribus%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22text%20recognition%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22escriptorium%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22OCR%22}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22HTR%22}]}]&amp;amp;qb={%22_text_1-exact%22:%22automatic%20transcription%22,%22qOp2%22:%22OR%22,%22_text_2-exact%22:%22transkribus%22,%22qOp3%22:%22OR%22,%22_text_3-exact%22:%22text%20recognition%22,%22qOp4%22:%22OR%22,%22_text_4-exact%22:%22escriptorium%22,%22qOp5%22:%22OR%22,%22_text_5%22:%22OCR%22,%22qOp6%22:%22OR%22,%22_text_6%22:%22HTR%22}&amp;amp;page=1"&gt;https://academic.oup.com/dsh/search-results?allJournals=1&amp;amp;f_ContentType=Journal+Article&amp;amp;fl_SiteID=5447&amp;amp;cqb=[{%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22automatic%20transcription%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22transkribus%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22text%20recognition%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22escriptorium%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22OCR%22}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22HTR%22}]}]&amp;amp;qb={%22_text_1-exact%22:%22automatic%20transcription%22,%22qOp2%22:%22OR%22,%22_text_2-exact%22:%22transkribus%22,%22qOp3%22:%22OR%22,%22_text_3-exact%22:%22text%20recognition%22,%22qOp4%22:%22OR%22,%22_text_4-exact%22:%22escriptorium%22,%22qOp5%22:%22OR%22,%22_text_5%22:%22OCR%22,%22qOp6%22:%22OR%22,%22_text_6%22:%22HTR%22}&amp;amp;page=1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In both cases, the search was not case sensitive, in order to catch a maximum of occurrences of keywords like "automatic text recognition" or "Text Recognition" or "text recognition", etc. However, it meant that sometimes I found false positives: "democracy" often matches with "ocr", so does "theatre" with "atr". Since DSH's search engine returns the match in context, I was able to ignore these false positives. For the other journals, I had to manually check where the matches were. Usually, I combined this control with the next step of my investigation.  &lt;/p&gt;
&lt;h4&gt;Hits per journal&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;JDMDH: 47 hits (out of 162 articles)&lt;/li&gt;
&lt;li&gt;DHQ: 93 hits (out of 790 articles)&lt;/li&gt;
&lt;li&gt;DSH: 143 relevant hits (out of 1741 articles)&lt;/li&gt;
&lt;li&gt;CHR: 65 hits (out of 214 articles)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;em&gt;Dépouillement&lt;/em&gt; and analysis&lt;/h3&gt;
&lt;p&gt;To this date, I am still in the process of reading the articles and taking notes on the occurrences of my keywords. &lt;/p&gt;
&lt;p&gt;I use Zotero to keep track of the articles I read and to confirm whether they are false positives. Sometimes, I leave out articles that are irrelevant, even if they mention a keyword I was looking for. For example, &lt;a href="https://doi.org/10.1093/llc/fqac089"&gt;Liu &amp;amp; Zhu (2023)&lt;/a&gt;&lt;sup id="fnref:liu_zhu_2023"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/024/#fn:liu_zhu_2023"&gt;3&lt;/a&gt;&lt;/sup&gt; contains the string "OCR" but it only appears in a title in their bibliography, for work they refer to in a context where OCR is not relevant to their argument. With tags in Zotero, I clearly identify such articles as "to be left out" from my analysis, but I don't remove them from the collection.  &lt;/p&gt;
&lt;p&gt;I use different tags to identify the various occurrences of the technology in the articles. For example, I distinguish between firsthand applications of ATR and the reuse of data produced by ATR before the experimentation presented by the authors. Typically, there are many mentions of documents that were OCRed by libraries and used by scholars to conduct their research. Overall, with this analysis, I am trying to add more depth to the observations made by &lt;a href="https://doi.org/10.48550/arXiv.2304.13530"&gt;Tarride et al (2023)&lt;/a&gt;&lt;sup id="fnref:tarride_et_al_2023"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/024/#fn:tarride_et_al_2023"&gt;4&lt;/a&gt;&lt;/sup&gt; in which they pragmatically considered three situations leading to the use of ATR: 1) for the production of digital editions; 2) for the production of large searchable text corpora; and 3) for the production of non-comprehensive transcriptions to feed knowledge bases. However, it is difficult to elaborate definitive categories before I am done processing all the collected articles.  &lt;/p&gt;
&lt;p&gt;Due to the large number of articles to be analyzed, I have continued to use the grep command to quickly review the content of articles and speed up my sorting process. For example, I am more interested in firsthand usages of ATR, want to be able to quickly identify non relevant mentions of my keywords as was the case in Liu &amp;amp; Zhu (2023). The command &lt;code&gt;grep -i -w -C 5 -H -f ../pattern.txt *.txt &amp;gt; grep_out&lt;/code&gt; allows me to generate a file, grep_out, in which, for each time a keyword is matched in a document, five lines of context are displayed before and after the match, as well as the name of the file. I still have to read the abstracts and parts of the articles to clearly understand in which contexts the automatic text recognition technologies are used. However, this is an effective method for quickly sorting through the articles.&lt;/p&gt;
&lt;p&gt;I'm looking forward to sharing the results of this analysis in my dissertation! &lt;/p&gt;
&lt;!-- FOOTNOTES ---&gt;

&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:python_server"&gt;
&lt;p&gt;This emulation is necessary to allow the Zotero connector to work properly. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/024/#fnref:python_server" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:proxy_dsh"&gt;
&lt;p&gt;I want to specify here that it was not by lack of reading documentations on proxies and requests. Unable to find a straightforward solution, unsure if it was even something that the UdeM proxy allowed, and because I would have still needed to write additional scripts afterwards, I decided that it would take just as long to do it manually (about 2-3 hours). &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/024/#fnref:proxy_dsh" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:liu_zhu_2023"&gt;
&lt;p&gt;Liu, Lei, and Min Zhu. "Bertalign: Improved Word Embedding-Based Sentence Alignment for Chinese–English Parallel Corpora of Literary Texts." &lt;em&gt;Digital Scholarship in the Humanities&lt;/em&gt; 38, no. 2 (June 1, 2023): 621–34. &lt;a href="https://doi.org/10.1093/llc/fqac089"&gt;https://doi.org/10.1093/llc/fqac089&lt;/a&gt;. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/024/#fnref:liu_zhu_2023" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:tarride_et_al_2023"&gt;
&lt;p&gt;Tarride, Solène, Mélodie Boillet, and Christopher Kermorvant. "Key-Value Information Extraction from Full Handwritten Pages." arXiv, April 26, 2023. &lt;a href="https://doi.org/10.48550/arXiv.2304.13530"&gt;https://doi.org/10.48550/arXiv.2304.13530&lt;/a&gt;. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/024/#fnref:tarride_et_al_2023" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>HTR</category><category>literature review</category><category>OCR</category><category>survey</category><guid>https://alix-tz.github.io/phd/posts/024/</guid><pubDate>Sat, 21 Jun 2025 19:15:27 GMT</pubDate></item><item><title>022 - McCATMuS #5 - Training models</title><link>https://alix-tz.github.io/phd/posts/022/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;Last week, I visited Rimouski in the Bas-Saint-Laurent region of Québec, along the South-eastern bank of the St Laurent river. I was invited to contribute to discussions around the &lt;a href="https://nouvellefrancenumerique.info/"&gt;Nouvelle-France Numérique project&lt;/a&gt;, and I took this opportunity to &lt;a href="https://inria.hal.science/hal-04706828"&gt;present&lt;/a&gt; HTR-United, CATMuS as well as preliminary results on training a McCATMuS model. In preparation for this presentation, I conducted a series of tests on the two first models I trained. Today, this blog post gives me a space to discuss these tests and their results in more details.&lt;/p&gt;
&lt;p&gt;The Kraken McCATMuS models were not directly trained on the HuggingFace dataset I introduced in my &lt;a href="https://alix-tz.github.io/phd/posts/022/021/"&gt;previous post&lt;/a&gt;, but rather on ARROW files created with the same ALTO XML files used to create the HuggingFace dataset. At the beginning of September, I wrote a Python script which reproduces the split of ALTO XML files into the train, validation and test sets, and which applies the same type of filtering of lines and modifications as I previously presented. Instead of generating the PARQUET files for HuggingFace, it simply creates alternative &lt;code&gt;.catmus_arrow.xml&lt;/code&gt; files and three listings of these files, ready to be served to a &lt;a href="https://kraken.re/4.3.0/ketos.html#binary-datasets"&gt;&lt;code&gt;ketos compile&lt;/code&gt;&lt;/a&gt; command&lt;sup id="fnref:compile"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/022/#fn:compile"&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;I used Kraken 4.3.13 to train the models on Inria's computation server because I've had dependency issues with Kraken 5 and haven't fixed them yet. The first model I trained strictly followed the train/validation split thanks to the &lt;a href="https://github.com/mittagessen/kraken/blob/cdfb923eba8d7dba10b6f32fb73bdf1e355aaf74/kraken/ketos/recognition.py#L129C16-L129C30"&gt;&lt;code&gt;--fixed-splits&lt;/code&gt; option&lt;/a&gt;. After 60 epochs, the model plateaued at 79.9% of character accuracy. When applied to the test set, this accuracy remained at 78.06%, a mere two points drop.&lt;/p&gt;
&lt;p&gt;I trained a second model using the same parameters&lt;sup id="fnref:params"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/022/#fn:params"&gt;2&lt;/a&gt;&lt;/sup&gt; but without the &lt;code&gt;--fixed-splits&lt;/code&gt; option, allowing Kraken to shuffle the train set and the validation set into a 90/10 split (the test set was left untouched however). This time, the training lasted 157 epochs before stopping, with the best model scoring with an accuracy of 92.8% on the validation set. When applied to the test set however, the model lost 7 points of accuracy (85.24%).&lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://alix-tz.github.io/phd/images/mccatmus_v1_entra%C3%AEnement_fixedsplits.png" alt="Learning curve for the model trained on the fixed split."&gt;
    &lt;figcaption&gt;Learning curve (Character and Word Accuracies) for the model trained on the fixed "feature"-based split between train and validation.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src="https://alix-tz.github.io/phd/images/mccatmus_v1_entra%C3%AEnement.png" alt="Learning curve for the model trained on the non-fixed split."&gt;
    &lt;figcaption&gt;Learning curve (Character and Word Accuracies) for the model trained on the random split between train and validation.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Although disappointing, this was consistent with the observations made when training the CATMuS Medieval model:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;As anticipated, the "General" split exhibits lower CER, given the absence of out-of-domain documents, whereas the "Feature"-based split surpasses 10%. This higher score presents an intriguing challenge for developing more domain-specific models that consider factors such as script type and language.&lt;/em&gt; (from &lt;a href="https://univ-paris8.hal.science/hal-04453952v1"&gt;Thibault Clérice, Ariane Pinche, Malamatenia Vlachou-Efstathiou, Alix Chagué, Jean-Baptiste Camps, et al.. CATMuS Medieval: A multilingual large-scale cross-century dataset in Latin script for handwritten text recognition and beyond. 2024 International Conference on Document Analysis and Recognition (ICDAR), 2024, Athens, Greece. ⟨hal-04453952⟩&lt;/a&gt; p. 15)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, the drop in accuracy observed on the test set is, as suggested in &lt;em&gt;Clérice et al, 2024&lt;/em&gt;, likely due to the fact that with a fixed-split, the model is both validated and tested against out-of-domain hands and documents (although the documents differ in the two sets). On the other hand, the model trained with a random split is validated against known hands and documents, but tested on out-of-domain examples.&lt;/p&gt;
&lt;p&gt;The test set contains transcriptions of printed, typewritten and handwritten texts, covering all centuries. Limiting ourselves to only one accuracy score obtained on the whole test set would tell us very little about the model's capacity and its limitations. This is why I divided the test set into several smaller test sets based on the century of the documents and/or on the main type of writing present in the documents. For documents spanning over several centuries, I used the most represented century.&lt;/p&gt;
&lt;p&gt;I only used the McCATMuS trained on the random split for these tests, because the accuracy of the other one was too low for the results to be meaningful. Instead of only testing McCATMuS, I also ran the Manu McFrench V3 and McFondue on the McCATMuS test set. They are two generic models trained on similar data (although with no or different normalization approaches).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;Test set..............&lt;/th&gt;
&lt;th style="text-align: left;"&gt;...McCATMuS...&lt;/th&gt;
&lt;th style="text-align: center;"&gt;...Manu McFrench V3...&lt;/th&gt;
&lt;th style="text-align: right;"&gt;...McFondue&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;All...................&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...85.24...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...&lt;strong&gt;91.17&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...76.12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Handwritten...........&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...78.72...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...&lt;strong&gt;89.40&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...75.17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Print.................&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...&lt;strong&gt;96.37&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...94.15...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...78.30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Typewritten...........&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...90.93...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...&lt;strong&gt;92.69&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...58.13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;17th cent.............&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...&lt;strong&gt;87.27&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...86.39...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...72.81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;18th cent.............&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...88.65...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...&lt;strong&gt;94.21&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...81.64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;19th cent.............&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...79.81...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...&lt;strong&gt;93.70&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...75.46&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;20th cent.............&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...74.92...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...&lt;strong&gt;86.52&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...56.74&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;21st cent.............&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...73.86...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...&lt;strong&gt;90.20&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...68.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;(HW) 17th cent........&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...58.69...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...&lt;strong&gt;64.83&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...&lt;em&gt;64.26&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;(HW) 18th cent........&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...85.38...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...&lt;strong&gt;93.35&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...80.47&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;(HW) 19th cent........&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...79.81...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...&lt;strong&gt;93.70&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...75.46&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;(HW) 20th cent........&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...63.02...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...&lt;strong&gt;82.23&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...55.89&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;(HW) 21st cent........&lt;/td&gt;
&lt;td style="text-align: left;"&gt;...73.86...&lt;/td&gt;
&lt;td style="text-align: center;"&gt;...&lt;strong&gt;90.20&lt;/strong&gt;...&lt;/td&gt;
&lt;td style="text-align: right;"&gt;...68.04&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- add plot? --&gt;

&lt;p&gt;I was initially surprised by the consistent margin Manu McFrench had over McCATMuS, considering it was trained on less data (73.9K + 8.8K lines, against the 106K + 5.8K lines) which had not been harmonized to follow the same transcription rules. However, these scores are actually biased in favor of Manu McFrench because several of the documents included in the McCATMuS test set were also used in Manu McFrench's train set. Even though this is not true for all documents, it concerns almost half of the test set. It might also be the case for McFonddue, but this model scores higher than McCATMuS in only one instance (handwritten documents from the 17th century). Creating a new test set, with documents that are not present in any of the train sets but follow the CATMuS guidelines, would be a good way to confirm this bias.&lt;/p&gt;
&lt;p&gt;Additionally, I detected an issue in one of the datasets used in the test set: &lt;code&gt;FoNDUE_Wolfflin_Fotosammlung&lt;/code&gt; contains some lines of faulty transcriptions, resulting from automatic text recognition, which most certainly cause an inaccurate evaluation of all three models.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;A couple of examples of the faulty transcriptions, along with their CER they generate when compared to what would be a correct transcription (the CER is generated with &lt;a href="https://github.com/WHaverals/CERberus"&gt;CERberus&lt;/a&gt;):&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;Line image&lt;/th&gt;
&lt;th style="text-align: right;"&gt;Faulty transcription&lt;/th&gt;
&lt;th style="text-align: right;"&gt;Correct transcription&lt;/th&gt;
&lt;th style="text-align: center;"&gt;Faulty CER would be&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;img alt='text line images reading, in print, "COLLECTION HANFSTAENGL LONDON"' src="https://alix-tz.github.io/phd/images/fotosammlung_error_example1.jpg"&gt;&lt;/td&gt;
&lt;td style="text-align: right;"&gt;"CSTITHER, KIESERMAEAER AogS."&lt;/td&gt;
&lt;td style="text-align: right;"&gt;"COLLECTION HANFSTAENGL LONDON"&lt;/td&gt;
&lt;td style="text-align: center;"&gt;89.29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;img alt='text line image reading, in print, "NATIONAL GALLERY"' src="https://alix-tz.github.io/phd/images/fotosammlung_error_example2.jpg"&gt;&lt;/td&gt;
&lt;td style="text-align: right;"&gt;"PEcLioL."&lt;/td&gt;
&lt;td style="text-align: right;"&gt;"NATIONAL GALLERY"&lt;/td&gt;
&lt;td style="text-align: center;"&gt;175.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/blockquote&gt;
&lt;p&gt;I have planned to manually control this dataset and update the McCATMuS dataset accordingly. I don't know yet how many lines are affected.&lt;/p&gt;
&lt;p&gt;The better accuracy of the Manu McFrench model is not just a product of the biases in the test set. I had the occasion to apply it to two documents, one from the 17th century and one from the 20th century. In both cases, Manu McFrench's transcription seemed more likely to be correct than McCATMuS's. This has led me to compare the training parameters used for both models and to start a third training experiment using Manu McFrench's parameters. In this case, the batch size is reduced to 16 (as opposed to 32) and the Unicode normalization follows &lt;a href="https://unicode.org/reports/tr15/#Compatibility_Composite_Figure"&gt;NFKD instead of NFD&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If the results of this third training are consistent with the previous experiments, it will be interesting to see if adding more data to the training set will improve the results. Also, I have yet to test the model in a situation of finetuning.&lt;/p&gt;
&lt;p&gt;As said at the beginning of this post, these results are preliminary, so I hope to have more to share in the coming weeks.&lt;/p&gt;
&lt;!-- footnotes --&gt;

&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:compile"&gt;
&lt;p&gt;The command looks like this: cat "./list_of_paths.txt" | xargs -d "\n" ketos compile -o "./binary_dataset.arrow" --random-split .0 .0 1.0 -f alto. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/022/#fnref:compile" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:params"&gt;
&lt;p&gt;The configuration of Kraken for training these two model relies on the default network architecture, on a NFD Unicode normalization, a learning rate of 0.0001 (1e&lt;sup&gt;-4&lt;/sup&gt;), batch size of 32, padding of 16 (default value), and applies augmentation (&lt;code&gt;--augment&lt;/code&gt;). The &lt;code&gt;--fixed-splits&lt;/code&gt; option is used for the first model. Following Kraken's default behavior, the training stops when the validation loss does not decrease for 10 epochs (early stops); this prevents the model from overfitting, which is confirmed when looking at the accuracy score of the intermediary models on the test set (orange line on the graphs). The training is done on a GPU. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/022/#fnref:params" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>CATMuS</category><category>datasets</category><category>HTR</category><guid>https://alix-tz.github.io/phd/posts/022/</guid><pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate></item><item><title>021 - McCATMuS #4 - Cleaning data, collection metadata</title><link>https://alix-tz.github.io/phd/posts/021/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;Preparing the data for CATMuS would certainly have taken much more time had I not been able to benefit from Thibault Clérice's experience with CATMuS Medieval. Not only was I able to build on the workflow he set up when he built it, but I also relied heavily on his scripts to parse and build the final dataset into &lt;a href="https://parquet.apache.org/"&gt;PARQUET&lt;/a&gt; files that were pushed to HuggingFace. Most of these steps are described in &lt;a href="https://univ-paris8.hal.science/hal-04453952v1"&gt;Thibault Clérice, Ariane Pinche, Malamatenia Vlachou-Efstathiou, Alix Chagué, Jean-Baptiste Camps, et al.. CATMuS Medieval: A multilingual large-scale cross-century dataset in Latin script for handwritten text recognition and beyond. 2024 International Conference on Document Analysis and Recognition (ICDAR), 2024, Athens, Greece&lt;/a&gt;, presented at the &lt;a href="https://icdar2024.net/"&gt;ICDAR&lt;/a&gt; conference in Athens in a few days.&lt;/p&gt;
&lt;p&gt;For McCATMuS, I started by downloading all the datasets (keeping track of the official releases) then I manually reorganized all the datasets so that the transcription and images were always under &lt;code&gt;{dataset_repo}/data/{sub_folder}&lt;/code&gt;, which made later manipulation easier. Based on the notes I took while filtering the datasets, and after generating a character table for each dataset with &lt;a href="https://github.com/PonteIneptique/choco-mufin"&gt;Chocomufin&lt;/a&gt;, I created several conversion tables to harmonize the transcription. The conversions are a mix of single character or multiple character replacements (&lt;code&gt;[&lt;/code&gt; and  &lt;code&gt;[[?]]&lt;/code&gt;) and more or less sophisticated replacements based on regular expressions (&lt;code&gt;#r#«&lt;/code&gt;).&lt;sup id="fnref:chocomufin"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/021/#fn:chocomufin"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Here is a sample of the Chocomufin conversion table used for the LECTAUREP datasets. If the character is replaced by itself, it remains unchanged in the dataset, while replacing it allows either to remove a character from the dataset (the &lt;code&gt;¥&lt;/code&gt;) or to harmonize its transcription with the CATMuS guidelines (see &lt;code&gt;œ&lt;/code&gt; and &lt;code&gt;°&lt;/code&gt; for example).&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="nc"&gt;char&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;replacement&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;codepoint&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mufidecode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="k"&gt;order&lt;/span&gt;
&lt;span class="n"&gt;#r&lt;/span&gt;&lt;span class="err"&gt;#«&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;Repl&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;extra&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;space&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;before&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;LEFT&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;POINTING&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;DOUBLE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ANGLE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;QUOTATION&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;MARK&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;""""&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;00&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;#r&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;»&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;Repl&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;extra&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;space&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;before&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;RIGHT&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;POINTING&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;DOUBLE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ANGLE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;QUOTATION&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;MARK&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;""""&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;00&lt;/span&gt;&lt;span class="n"&gt;BB&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;[?&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nf"&gt;replace&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;[?&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;⟦⟧&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="err"&gt;⟦⟧&lt;/span&gt;&lt;span class="p"&gt;,,,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;?&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nf"&gt;replace&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;?&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;⟦⟧&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="err"&gt;⟦⟧&lt;/span&gt;&lt;span class="p"&gt;,,,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="nf"&gt;RIGHT&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;PARENTHESIS&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;&lt;span class="mi"&gt;0029&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;LATIN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;SMALL&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LETTER&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;006&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;É&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;LATIN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;CAPITAL&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LETTER&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;WITH&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ACUTE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;É&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;00&lt;/span&gt;&lt;span class="n"&gt;C9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;LATIN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;SMALL&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LETTER&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0061&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="ss"&gt;","&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;COMMA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;","&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;002&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;","&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;LATIN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;SMALL&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LETTER&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0065&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;CIRCUMFLEX&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ACCENT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;005&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;œ&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;LATIN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;SMALL&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LIGATURE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;OE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;oe&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0153&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;oe&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="err"&gt;̂&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;COMBINING&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;CIRCUMFLEX&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ACCENT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="err"&gt;̂&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0302&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;
&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;LATIN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;CAPITAL&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LETTER&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0057&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="err"&gt;°&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;DEGREE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;SIGN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;00&lt;/span&gt;&lt;span class="n"&gt;B0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="err"&gt;¥&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;YEN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;SIGN&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;&lt;span class="mi"&gt;00&lt;/span&gt;&lt;span class="n"&gt;A5&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;
&lt;span class="n"&gt;½&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;VULGAR&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;FRACTION&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ONE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;HALF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;00&lt;/span&gt;&lt;span class="n"&gt;BD&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;LATIN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;SMALL&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LETTER&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0068&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;LATIN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;SMALL&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LETTER&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0072&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;æ&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;LATIN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;SMALL&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LETTER&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;AE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ae&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;00E6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ae&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;ȼ&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;LATIN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;SMALL&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LETTER&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;WITH&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;STROKE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;023&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="err"&gt;∟&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nf"&gt;RIGHT&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ANGLE&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;&lt;span class="mi"&gt;221&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;UNKNOWN&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It wasn't possible to use a single conversion table for all the datasets because some had different transcription approaches. While replacing  &lt;code&gt;¬&lt;/code&gt; with &lt;code&gt;-&lt;/code&gt; could, in principle, be used for each dataset, normalizing the way corrections and uncertainties were transcribed was another story. For example, in some of the CREMMA datasets, &lt;code&gt;&amp;gt;&amp;lt;&lt;/code&gt; is used to signal a crossed word, while in other datasets &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; is used. So replacing &lt;code&gt;&amp;gt;&lt;/code&gt; with &lt;code&gt;⟦&lt;/code&gt; and &lt;code&gt;&amp;lt;&lt;/code&gt; with &lt;code&gt;⟧&lt;/code&gt; in &lt;code&gt;&amp;gt;hello&amp;lt;&lt;/code&gt; meant that in some cases we would successfully get &lt;code&gt;⟦hello⟧&lt;/code&gt;, while in other cases we would end up with &lt;code&gt;⟧hello⟦&lt;/code&gt;. There are a few documents where I had to manually intervene in the XML file to fix the transcription. In such cases, I fork the dataset repository to keep track of the corrected version of the ground truth or I push the correction back into the original dataset to create a new, more consistent version.&lt;/p&gt;
&lt;p&gt;In general, the converted dataset is saved as &lt;code&gt;.catmus.xml&lt;/code&gt; files, which allows us to keep track of the original ground truth and easily adjust the conversion table later if necessary afterwards.&lt;/p&gt;
&lt;p&gt;In the &lt;a href="https://alix-tz.github.io/phd/posts/19/"&gt;second post&lt;/a&gt; of this series, I mentioned that "&lt;em&gt;the CATMuS guidelines can (should?) be used as a reference point&lt;/em&gt;" and that "&lt;em&gt;if a project decides to use a special character to mark the end of each paragraph, then in order to create a CATMuS-compatible version of the dataset, I should only have to replace or remove that character. In such cases, the special character that was chosen should be unambiguous and the rule should be explicitly presented&lt;/em&gt;." Providing a Chocomufin conversion table along with a dataset that uses project-specific guidelines would be an excellent practice to ensure that the dataset is indeed compatible with CATMuS.&lt;/p&gt;
&lt;p&gt;Once all the &lt;code&gt;.catmus.xml&lt;/code&gt; files were ready, I created a new metadata table for McCATMuS listing all the subdirectories under each dataset's "data" folder. This table was used as a basis to start collecting additional metadata at the document level rather than at dataset level, like the language used in the source or the type of writing (printed, handwritten or typewritten). Working at the document level is important because some dataset contain different types of writing and/or are multilingual. In some cases, when a document would mix different languages and/or different types of writing in the source, if the distinction could be made at the image level, I manually sorted them and created two different subfolders. This is what I did in the "Memorials for Jane Lathrop Stanford" dataset, for example: the subfolder "PageX-LettreX" mixed typewritten and handwritten letters, so I sorted them into "PageX-LettreX-handwritten" and "PageX-LettreX-typewritten" in order to have the most accurate metadata possible.&lt;/p&gt;
&lt;p&gt;Other metadata included the assignment of a call number (or shelf mark) for each source represented in the datasets. In some cases a call number may apply to multiple subfolders, but in most cases, each subfolder is de facto a different document. Retrieving the call number is useful for several reasons: it allows for an accurate assessment of the diversity of documents in McCATMuS, it allows for a document to be associated with additional metadata found in its institution's catalog, or the list of call numbers can be used during benchmarking or production to check whether a document is known to the models trained on that dataset, thus explaining potentially higher accuracy scores.&lt;/p&gt;
&lt;p&gt;In the few cases where the source used to build the ground truth did not have a corresponding call number, I simply made one up, keeping "nobs_" as a signal that it was a made-up call number. Thus, if "cph_paris_tissage_1858/" in "timeuscorpus" is now associated with its corresponding call number at the Paris archive center (Paris, AD75, D1U10 386), CREMMAWiki's "batch-04", which is composed of documents we created for the project, is associated with a made-up call number: "nobs_cremma-wikipedia_b04".&lt;/p&gt;
&lt;p&gt;In the end, when the PARQUET files are created, the metadata from the table I just presented is collected, along with information extracted from parsing the contents of the XML file. Each of the metadata is then represented at the text line level. If you compare &lt;a href="https://huggingface.co/datasets/CATMuS/modern"&gt;McCATMuS&lt;/a&gt; with &lt;a href="https://huggingface.co/datasets/CATMuS/medieval"&gt;CATMuS Medieval&lt;/a&gt; using HuggingFace's dataset viewer, you can see that they don't use exactly the same metadata.&lt;/p&gt;
&lt;p&gt;"Language", "region type" and "line type" (which are based on the segmOnto classification), "project" and "gen_split" are common to both datasets, along with "shelfmark" I just described above. They both have a "genre" column with similar values (treatise, epistolary, document of practice, etc.). In the case of CATMuS Medieval, "genre" is complemented by "verse" (prose, verse).&lt;/p&gt;
&lt;p&gt;Following Thibault's advice, I defined the creation date of a text line using two numbers ("not_before" and "not_after") instead of a single "century" value. This allows for a precise dating when it is possible or on the contrary, to spread the dating over several centuries when it cannot be avoided, which is more accurate in both cases.&lt;/p&gt;
&lt;p&gt;McCATMuS mixes printed, handwritten and typewritten documents, so it was important to have a "writing type" column to help filter the dataset based on this information, in cases where one does not want to mix them. This metadata also makes it possible to use McCATMuS to train a classifier capable of distinguishing between the different types of writing. CATMuS Medieval on the other hand presents only handwritten sources, so such a metadata would be useless and is able to rely on paleographic classifications to characterize each text line based on a "script type" metadata, that includes values such as "caroline", "textualis", "hybrida", etc.&lt;/p&gt;
&lt;p&gt;McCATMuS also has a "color" column that helps sort text lines based on whether the source image is colored (true) or in grayscale (false).&lt;/p&gt;
&lt;p&gt;Although I reused the scripts developed by Thibault to build this dataset, I had to make several modifications to include this new metadata in the PARQUET files and to add additional filtering to the text lines. This included updating the mapping to the segmOnto vocabulary to match what existed in my datasets, or filtering some types of lines such as those identified as signatures.&lt;sup id="fnref:signatures"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/021/#fn:signatures"&gt;2&lt;/a&gt;&lt;/sup&gt; I also included an update of "writing_type" at the line level whenever the value in "line_type" allowed it to be controlled. &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="s2"&gt;":handwritten"&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;line_type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;writing_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"handwritten"&lt;/span&gt;
    &lt;span class="n"&gt;line_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line_type&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;":handwritten"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;""&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="s2"&gt;":print"&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;line_type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;writing_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"printed"&lt;/span&gt;
    &lt;span class="n"&gt;line_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line_type&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;":print"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;""&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="s2"&gt;":typewritten"&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;line_type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;writing_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"typewritten"&lt;/span&gt;
    &lt;span class="n"&gt;line_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line_type&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;":typewritten"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;""&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;writing_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;metadata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"writing_type"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the end, having built such a dataset (the first version of McCATMuS contains 117 text lines!) with such a variety of metadata is very satisfying although there is room for improvement. I have already mentioned that it would be interesting to have a greater variety of languages in McCATMuS. I also know that some of the values in "writing_type" are not completely accurate so adding a control based on a classifier might be interesting. Finally, I've noticed that some transcriptions in the "FoNDUE_Wolfflin_Fotosammlung" dataset are not correct at all, probably due to an automatic transcription that wasn't corrected.&lt;/p&gt;
&lt;p&gt;However, before we dive into improving McCATMuS, it's important to first examine the accuracy of the models that can be built on top of it! This will be the topic of the next and last post in this series!&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:chocomufin"&gt;
&lt;p&gt;To learn more about how &lt;a href="https://github.com/PonteIneptique/choco-mufin?tab=readme-ov-file#commands"&gt;&lt;code&gt;chocomufin convert&lt;/code&gt;&lt;/a&gt; works, just read the software's short documentation. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/021/#fnref:chocomufin" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:signatures"&gt;
&lt;p&gt;I don't think it makes sense to include signatures in a dataset to train a generic model, since the transcription of such lines can be very context specific. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/021/#fnref:signatures" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>CATMuS</category><category>datasets</category><category>HTR</category><guid>https://alix-tz.github.io/phd/posts/021/</guid><pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate></item><item><title>020 - McCATMuS #3 - Datasets selection</title><link>https://alix-tz.github.io/phd/posts/020/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;HTR-United made identifying candidate datasets for McCATMuS a piece of cake. Once the rest of the CATMuS community agreed with the period to be covered by a "modern and contemporary" dataset, I created a simple script to parse the content of the HTR-United catalog and make a list of existing datasets covering documents written in Latin alphabet and matching our time criteria. &lt;/p&gt;
&lt;p&gt;Actually, here is the script!&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;url_latest_htrunited&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"https://raw.githubusercontent.com/HTR-United/htr-united/master/htr-united.yml"&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;requests&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;yaml&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pandas&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="c1"&gt;# get latest htr-united.yml from main repository&lt;/span&gt;
&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url_latest_htrunited&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;catalog&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;yaml&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;safe_load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;in_time_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dates&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;century_scope_min&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1600&lt;/span&gt;
    &lt;span class="n"&gt;century_scope_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2100&lt;/span&gt;
    &lt;span class="c1"&gt;# this means that we allow datasets that intersect with the period&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"notBefore"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;century_scope_min&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"notAfter"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;century_scope_min&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"notBefore"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;century_scope_max&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"notAfter"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;century_scope_max&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;

&lt;span class="n"&gt;filtered_by_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;entry&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;catalog&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;in_time_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;entry&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"time"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{})):&lt;/span&gt;
        &lt;span class="n"&gt;filtered_by_date&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Found &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filtered_by_date&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; entries matching the time scope."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;targeted_script&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Latn"&lt;/span&gt;
&lt;span class="n"&gt;filtered_by_script&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;entry&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;filtered_by_date&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;targeted_script&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"iso"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;entry&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"script"&lt;/span&gt;&lt;span class="p"&gt;)]:&lt;/span&gt;
        &lt;span class="n"&gt;filtered_by_script&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Found &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filtered_by_script&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; entries matching the script criteria."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;cols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Script Type"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Time Span"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Languages"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Repository"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Project Name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Dataset Name"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;metadata_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cols&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;selected_entries&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;filtered_by_script&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;entry&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;selected_entries&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;""&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cols&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;languages&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;l&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;entry&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"language"&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;languages&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Languages"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;languages&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;languages&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Languages"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;", "&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;languages&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Couldn't find a field for language in this repository"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Languages"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"no language"&lt;/span&gt;
    &lt;span class="c1"&gt;# get centuries/y&lt;/span&gt;
    &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Time Span"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;entry&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"time"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"notBefore"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;-&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;entry&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"time"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"notAfter"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;
    &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Project Name"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;entry&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"project-name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"no project name"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;repository&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;entry&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"url"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"no url found"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;repository&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;startswith&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"https://github.com/"&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Repository"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;repository&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"https://github.com/"&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;repository&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;startswith&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"https://zenodo.org/"&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Repository"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;repository&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"https://zenodo.org/"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"zenodo:"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Repository"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;repository&lt;/span&gt;
    &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Dataset Name"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;entry&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"title"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"no title found"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;script_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;entry&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"script-type"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;script_type&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;"only-typed"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Script Type"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Print"&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;script_type&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;"only-manuscript"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Script Type"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Handwritten"&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Script Type"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Mixed"&lt;/span&gt;
    &lt;span class="n"&gt;metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metadata_df&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;

&lt;span class="n"&gt;metadata_df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I saved the output as a CSV and proceeded to go through each of the selected datasets and its metadata. I checked several things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I made sure the datasets were available and easy to download. For example, I excluded those requiring manual image retrieval.&lt;/li&gt;
&lt;li&gt;I checked the format of the data because I decided to initially focus only on datasets available in ALTO XML and PAGE XML.&lt;/li&gt;
&lt;li&gt;I controlled the overall compatibility between the transcription guidelines used for the dataset and those designed by CATMuS.&lt;/li&gt;
&lt;li&gt;I also checked the conformity of the dataset when trying to import it into eScriptorium. This import allowed me to detect when there was a discrepancies between the names of the image files and the value for the source image in the XML file which prevented the import from successfully running.&lt;sup id="fnref:images"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/020/#fn:images"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;Loading a sample of the dataset in eScriptorium also allowed me to visually control other incompatibilities with CATMuS that may not have been documented by the producers of the data.&lt;sup id="fnref:segmentation"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/020/#fn:segmentation"&gt;2&lt;/a&gt;&lt;/sup&gt; &lt;/li&gt;
&lt;li&gt;Finally, I considered the structure of the repository and, when necessary, the facility to reorganize it into a single &lt;code&gt;data/&lt;/code&gt; folder containing the images and the XML files, often distributed among sub-folders.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I assigned each dataset a priority number from 1 to 6. The lowest number was for dataset compatible with CATMuS without any modification (no dataset was giving a priority rank of 1...) and 6 for massive datasets that would require a nerve-racking script to be built correctly. My grading system is shown below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1=ready as is&lt;/li&gt;
&lt;li&gt;2=need to be &lt;a href="https://github.com/PonteIneptique/choco-mufin"&gt;chocomufin&lt;/a&gt;-ed&lt;/li&gt;
&lt;li&gt;3=require manual corrections but the dataset is very small, or the dataset is chocomufin/catmus compatible but requires a script to build it&lt;/li&gt;
&lt;li&gt;4=require manual corrections but the dataset is relatively big, or require a script to be fixed&lt;/li&gt;
&lt;li&gt;5=require manual corrections but the dataset is really big&lt;/li&gt;
&lt;li&gt;6=require manual corrections but the dataset is really big and require a personalized script to be built&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, &lt;a href="https://htr-united.github.io/share.html?uri=507bb514d"&gt;"Notaires de Paris - Bronod"&lt;/a&gt; had to be modified to comply with CATMuS requirements. This included replacing &lt;code&gt;[[&lt;/code&gt; and &lt;code&gt;]]&lt;/code&gt; &lt;a href="https://catmus-guidelines.github.io/html/guidelines/en/corrections_and_others.html"&gt;with &lt;code&gt;⟦&lt;/code&gt; and &lt;code&gt;⟧&lt;/code&gt;&lt;/a&gt;, or also to ignore lines containing &lt;code&gt;¥&lt;/code&gt;, a symbol used in LECTAUREP's datasets to transcribe signatures and paraphs. These were straightforward modifications, thanks to Chocomufin. On the complete opposite, &lt;a href="https://htr-united.github.io/share.html?uri=7a99090c5"&gt;"University of Denver Collections as Data - HTR Train and Validation Set JCRS_2020_5_27"&lt;/a&gt; is a massive dataset (2660 XML files), but there are segmentation errors in this dataset, creating erroneous transcriptions given the way the line is drawn, and the annotation of the superscripted text is not compatible with CATMuS. To make it compatible with CATMuS, it would be necessary to control and correct each page one by one.&lt;/p&gt;
&lt;p&gt;I chose to focus on datasets with priority 2 for the &lt;em&gt;first&lt;/em&gt; version of McCATMuS. Indeed, it'll be possible to add more datasets into CATMuS in later versions, so there was no need to spend too much time on manually cleaning datasets. I had 23 with priority 2 to go through.&lt;/p&gt;
&lt;p&gt;Identifying eligible datasets was not as time consuming as cleaning them and collecting additional metadata turned out to be. However, it gave me a good idea of the challenges I would face when trying to aggregate the datasets. I would have liked to be able to find a greater diversity of languages, but this is wasn't possible at this stage, mainly because many non-French datasets require more elaborate corrections than applying Chocomufin and were thus given a priority score higher than 2. &lt;/p&gt;
&lt;p&gt;The next post will be covering the tedious phase of data cleaning and aggregation, along with metadata collection!&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:images"&gt;
&lt;p&gt;It was the case in "&lt;a href="https://htr-united.github.io/share.html?uri=c326a6fee"&gt;Données vérité de terrain HTR+ Annuaire des propriétaires et des propriétés de Paris et du département de la Seine (1898-1923)&lt;/a&gt; where the ALTO XML files are not explicitly linked to their corresponding source images. I believe it can be fixed, but it would require creating a script just for this purpose and the dataset presented other incompatibilities with CATMuS' guidelines. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/020/#fnref:images" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:segmentation"&gt;
&lt;p&gt;For example, "&lt;a href="https://htr-united.github.io/share.html?uri=43d1c93c7"&gt;Argus des Brevets&lt;/a&gt;" contains some segmentation errors that will need to be corrected manually. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/020/#fnref:segmentation" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>CATMuS</category><category>datasets</category><category>HTR</category><guid>https://alix-tz.github.io/phd/posts/020/</guid><pubDate>Thu, 29 Aug 2024 04:00:00 GMT</pubDate></item><item><title>019 - McCATMuS #2 - Defining guidelines</title><link>https://alix-tz.github.io/phd/posts/019/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;&lt;a href="https://x.com/JMFradeRue/status/1730191566508060883"&gt;Previous experiments&lt;/a&gt; have shown that conflicting transcription guidelines in training datasets make it less likely that a model will learn to transcribe correctly. This is particularly relevant when it comes to abbreviations and it's something to keep in mind when merging existing datasets. We didn't really address this when we trained the &lt;a href="https://inria.hal.science/hal-04094241"&gt;Manu McFrench model&lt;/a&gt; because it's difficult to retroactively align datasets to follow the same transcription rules. Unless you can afford to manually check every line, of course. In the case of Manu McFrench however, we only merged datasets that didn't solve abbreviations, so we ensured a minimum of cohesion.&lt;/p&gt;
&lt;p&gt;CATMuS was built on the foundation laid by CREMMALab and the &lt;a href="https://hal.science/hal-03716526"&gt;annotation guidelines&lt;/a&gt; developed by Ariane Pinche at the end of a seminar organized in 2021. These guidelines are intended to be generic, meaning they should be compatible with most transcription situations and are not project-specific. Following these guidelines will help data producers create ground truth that is compatible with data from other projects. It will also help those projects save time by not having to create transcription rules from scratch. From my experience, it is indeed easy for the members of a project discovering HTR to get caught up in the specifics of one project and forget what is and is not relevant (or even complicating) in the transcription phase.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;It's worth mentioning that a project can choose to follow some of the CATMuS guidelines, while maintaining more specific rules for certain cases. If that's the case, the CATMuS guidelines can (should?) be used as a reference point. Ideally, the specific rules defined by a project should be retro-compatible with CATMuS. For example, if a project decides to use a special character to mark the end of each paragraph, then in order to create a CATMuS-compatible version of the dataset, I should only have to replace or remove that character. In such cases, the special character that was chosen should be unambiguous and the rule should be explicitly presented.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As CREMMALab focused on the transcription of medieval manuscripts, so did the first CATMuS dataset and guidelines. As I said in my &lt;a href="https://alix-tz.github.io/phd/posts/018/"&gt;previous post&lt;/a&gt;, I focused on data covering the modern and contemporary periods, for which there was no equivalent to the CREMMALab guidelines. So, when extending CATMuS to these periods, I started with collecting existing guidelines and comparing them. I used the &lt;a href="https://hal.science/hal-03697382"&gt;CREMMA Medieval guidelines&lt;/a&gt;, the &lt;a href="https://gist.github.com/alix-tz/6f89444521bf1cab0522da520f7e4ff4"&gt;CREMMA guidelines for modern and contemporary documents&lt;/a&gt;, &lt;a href="https://hal.science/hal-04281804"&gt;SETAF's guidelines&lt;/a&gt; and &lt;a href="https://hal.science/hal-04557457"&gt;CATMuS Print's guidelines&lt;/a&gt; as a basis to elaborate the transcription rules for McCATMuS.&lt;/p&gt;
&lt;p&gt;For each rubric, I &lt;a href="https://docs.google.com/spreadsheets/d/1bFE-rRk6ZwgIHqXAOgwPo1s1zwQ-UPTLPnzjaRmTMsk/edit?usp=sharing"&gt;compared&lt;/a&gt; what each set of rules suggested, when they covered it. It was rare for all guidelines to align, but some cases were easy to solve. For example, all the guidelines recommended not to differentiate between regular s (&lt;code&gt;⟨s⟩&lt;/code&gt;) and long s (&lt;code&gt;⟨ſ⟩&lt;/code&gt;), except for the rules I had set for the modern and contemporary sources transcribed by CREMMA in 2021, before the CREMMALab seminar. It was thus decided that for McCATMuS there would be no distinction between all types of s's.&lt;/p&gt;
&lt;p&gt;Some rubrics needed to be discussed to figure out why the rule had been chosen in the first place by some of the projects, to decide which one to keep for McCATMuS. In February, I met with Ariane Pinche and Simon Gabay to go over the rubrics that still needed to be set. One example of a rule we discussed is how hyphenations are handled. CATMuS Medieval and the two CREMMA guidelines say to always use the same symbol (&lt;code&gt;⟨-⟩&lt;/code&gt;), whereas for the SETAF and CATMuS Print datasets, inline hyphenations (&lt;code&gt;⟨-⟩&lt;/code&gt;) are differentiated from hyphenations at the end of a line (&lt;code&gt;⟨¬⟩&lt;/code&gt;). Other symbols, like &lt;code&gt;⟨⸗⟩&lt;/code&gt;, were unanimously rejected.&lt;/p&gt;
&lt;p&gt;Two factors were considered when making those decisions: the feasibility of a retro-conversion for the existing datasets and the compatibility of the rule with a maximum of projects. In the case of hyphenations, I eventually decided to follow the same rule as CATMuS Medieval and CREMMA. On top of simplifying the compatibility of McCATMuS with CATMuS Medieval, I found that replacing all &lt;code&gt;⟨¬⟩&lt;/code&gt; with &lt;code&gt;⟨-⟩&lt;/code&gt;, rather than retroactively place &lt;code&gt;⟨¬⟩&lt;/code&gt; where there was indeed an hyphenation at the end of a line&lt;sup id="fnref:hyphen"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/019/#fn:hyphen"&gt;1&lt;/a&gt;&lt;/sup&gt; was much more straightforward.&lt;/p&gt;
&lt;p&gt;Once the set of rules was fixed, I used it to sort between the different datasets I had identified (I'll discuss this in the next post) and to decide which one would be retained for McCATMuS v1. I also defined the transformation scenarios necessary to turn each of these datasets into a CATMuS-compatible version. Then, once McCATMuS v1 was ready, I integrated the modern and contemporary guidelines into the &lt;a href="https://catmus-guidelines.github.io/"&gt;CATMuS website&lt;/a&gt;, where the transcription guidelines for CATMuS medieval were already published.&lt;/p&gt;
&lt;p&gt;Now that I am done integrating the rules set for McCATMuS into the website, I am confident that we have successfully designed rules that are overall compatible across the medieval, modern and contemporary periods, despite some unavoidable exceptions. Two good examples of the impossibility to cover a whole millennium of document production with the same rule are the &lt;a href="https://catmus-guidelines.github.io/html/guidelines/en/abbreviations.html"&gt;abbreviations&lt;/a&gt; and the &lt;a href="https://catmus-guidelines.github.io/html/guidelines/en/punctuation.html"&gt;punctuation signs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I've now explained how the transcription guidelines were established for McCATMuS. Next, I'll cover how they were integrated into existing datasets to create the first version of the McCATMuS dataset.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:hyphen"&gt;
&lt;p&gt;You can't assume that every instance of &lt;code&gt;⟨-⟩&lt;/code&gt; at the end of a line must be replaced with a &lt;code&gt;⟨¬⟩&lt;/code&gt;. In many cases, this can be a simple typographic decoration marking the end of a paragraph or the end of a title. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/019/#fnref:hyphen" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>CATMuS</category><category>guidelines</category><category>HTR</category><guid>https://alix-tz.github.io/phd/posts/019/</guid><pubDate>Tue, 20 Aug 2024 04:00:00 GMT</pubDate></item><item><title>018 - McCATMuS #1 - Overview</title><link>https://alix-tz.github.io/phd/posts/018/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;Last week, I attended &lt;a href="https://dh2024.adho.org/"&gt;ADHO's annual conference&lt;/a&gt; in Washington DC. I presented a short paper, co-authored with Floriane Chiffoleau and Hugo Scheithauer, about the documentation we wrote for eScriptorium (I wrote &lt;a href="https://alix-tz.github.io/phd/posts/018/010"&gt;a post&lt;/a&gt; about it last year and you can also find our presentation &lt;a href="https://inria.hal.science/hal-04594142"&gt;here&lt;/a&gt;). I was also a co-author on a long paper presented by Ariane Pinche on the &lt;a href="https://inria.hal.science/hal-04346939"&gt;CATMuS Medieval dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;CATMuS, which stands for "Consistent Approach to Transcribing ManuScripts", is a collective initiative and a framework to aggregate ground truth datasets using compatible &lt;a href="https://catmus-guidelines.github.io/"&gt;transcription guidelines&lt;/a&gt; for documents from different period written in romance languages. It started with &lt;a href="https://huggingface.co/datasets/CATMuS/medieval"&gt;CATMuS Medieval&lt;/a&gt;, but since January this year, I have been working on a version of CATMuS for the modern and contemporary period. &lt;/p&gt;
&lt;p&gt;While I should (and will) try to publish a data paper on CATMuS Modern &amp;amp; Contemporary (I'll call it McCatmus from now on), I figured I could start with a series of blog posts here. I want to describe the various steps I followed in order to eventually release &lt;a href="https://huggingface.co/datasets/CATMuS/modern"&gt;a dataset on HuggingFace&lt;/a&gt; and hopefully soon the corresponding transcription model.&lt;/p&gt;
&lt;p&gt;I started working on McCatmus in January, but because of a major personal event (I moved to Canada!), it took seven month of stop-and-go before the release of the V1. This was particularly challenging due to the scale of the project and its technicality (it was hard to get back into McCatmus after several weeks of interruption, which I had to do several times).&lt;/p&gt;
&lt;p&gt;To add to this complexity, McCatmus was also a multi-front operation. Indeed, to create McCatmus, it was necessary to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;define transcription guidelines in collaboration with other data producers,&lt;/li&gt;
&lt;li&gt;identify datasets compatible with the guidelines and set priorities,&lt;/li&gt;
&lt;li&gt;actually make all the dataset compatible with each other and clean some of the data,&lt;/li&gt;
&lt;li&gt;model and collect metadata that made sense for this dataset,&lt;/li&gt;
&lt;li&gt;release the dataset and fix the issues that came up.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To this date, two tasks remain on my to-do list for McCatmus: train a transcription model corresponding to this dataset and compare it with other existing ones, and make sure to have a publication describing this dataset and its usefulness.&lt;/p&gt;
&lt;p&gt;My plan is to dedicate one post to the creation of the guidelines for the dataset, then a post about the identification and collection of the datasets used in McCatmus v1, and then I'll wrap up with a post about the process to create the dataset, the metadata and the release. Stay tuned!&lt;/p&gt;</description><category>CATMuS</category><category>HTR</category><guid>https://alix-tz.github.io/phd/posts/018/</guid><pubDate>Wed, 14 Aug 2024 04:00:00 GMT</pubDate></item><item><title>017 - Deploying eScriptorium online: notes on CREMMA's server specifications</title><link>https://alix-tz.github.io/phd/posts/017/</link><dc:creator>Alix Chagué and Thibault Clérice</dc:creator><description>&lt;p&gt;&lt;a href="https://gitlab.com/scripta/escriptorium/"&gt;eScriptorium&lt;/a&gt; is a web application designed to perform automatic text recognition campaigns, by default powered by the OCR/HTR engine &lt;a href="https://kraken.re/"&gt;Kraken&lt;/a&gt;. It comes in a decentralized form, meaning that the application is not distributed by a single organization but can, on the contrary, be deployed by several actors on many different servers. In fact, you can also deploy eScriptorium &lt;a href="https://gitlab.com/scripta/escriptorium/-/wikis/docker-install"&gt;on your personal machine&lt;/a&gt;, simulating a local server.&lt;sup id="fnref:localhost"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:localhost"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;As eScriptorium is gaining attention, more institutions are interested in building their own server to host the application and offer it to their associates. At Inria, we deployed eScriptorium for the first time in 2020, specifically for the project called &lt;a href="https://lectaurep.hypotheses.org/"&gt;LECTAUREP&lt;/a&gt; which we ran with the &lt;a href="https://www.archives-nationales.culture.gouv.fr/"&gt;French national archives&lt;/a&gt; between 2018 and 2021. While the initial server was hosted on a virtual machine, without any GPU, and open to a relatively small amount of users, our current eScriptorium application already counts nearly 500 users and will soon be hosted on a much different server infrastructure, funded by the &lt;a href="https://www.pamir.fr/projets-soutenus/cremma/"&gt;CREMMA project&lt;/a&gt;. Between the original LECTAUREP-eScriptorium server and the CREMMA server, we moved to a dedicated server (&lt;code&gt;Traces-6&lt;/code&gt;) for which we invested about 20K€.&lt;/p&gt;
&lt;p&gt;Since I have been regularly in touch with people from different institutions who were looking into buying the hardware to create their own server for eScriptorium, I thought it was largely time to put all the deets in writing!&lt;/p&gt;
&lt;p&gt;To write today's post, I'm very happy to welcome a second pair of hands: Thibault Clérice's. His expertise and involvement in designing CREMMA server are crucial here!&lt;/p&gt;
&lt;p&gt;Let's first discuss some technical requirements, then we'll describe how the CREMMA server was designed. We finish with some very important remarks on the necessity (or not) to build a server and on useful alternatives for the community!&lt;/p&gt;
&lt;h3&gt;Should you buy GPUs?&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Graphics_processing_unit"&gt;GPUs&lt;/a&gt; (or Graphics Processing Units) are not mandatory at all when you use eScriptorium. This is the reason why it is perfectly acceptable to run eScriptorium locally, on your own computer. Actually GPUs are not even mandatory to train Kraken models: training can be done on CPUs (your computer's processor), they will simply go much much much slower.&lt;/p&gt;
&lt;p&gt;That, however, is true for personal or light use of the training features. If on the contrary you create a server open to dozens of users or more, then connecting eScriptorium to GPUs is very much a good idea: since training a model on a CPU alone can take 2-3 days (or much more), you don't really want 10 users to start a training task at the same time. In the absence of shared GPUs, their training will be queued for days or even weeks and the overload might degrade the experience of other users on the rest of the application. As long as we are building an infrastructure (and hopefully sharing costs), we may as well enhance the experience of everyone, no?&lt;/p&gt;
&lt;p&gt;This being said, you shouldn't rush and go buy a GPU right away. Instead, you should first look at options to &lt;em&gt;optimize&lt;/em&gt; its usage or at infrastructures that are already available to you. For example, the &lt;a href="https://www.unige.ch/lettres/humanites-numeriques/recherche/projets-de-la-chaire/fondue"&gt;FONDuE infrastructure&lt;/a&gt;, at the University of Geneva, doesn't use the GPUs only for eScriptorium: they connect their application to a cluster which is used by researchers for intense computation tasks outside of eScriptorium (it's an &lt;a href="https://en.wikipedia.org/wiki/High-performance_computing"&gt;HPC&lt;/a&gt; with a university-wide queue controlled by &lt;a href="https://en.wikipedia.org/wiki/Slurm_Workload_Manager"&gt;SLURM&lt;/a&gt;). This is a very good solution for optimization, because training Kraken models is not a constant activity: if the GPU is dedicated to eScriptorium only, then it will be used for a few hours here and there, not even at 100% of its capacity. Think of it: users of the application will usually need to train a model at the beginning of their transcription campaign, therefore once they have an &lt;a href="https://alix-tz.github.io/phd/posts/12/"&gt;accurate model&lt;/a&gt;, they will focus on using the model for prediction, which doesn't rely on the GPUs (and Kraken isn't really optimized for GPU usage at prediction time anyway).&lt;/p&gt;
&lt;p&gt;Other possibilities include connecting the server to a completely physically separate cluster where training jobs are submitted. This is a possibility that several people told me they were exploring, but I don't know if anyone has set it already. Why would you opt for a solution with an external cluster? To replace some huge investment costs (original funding) with some smaller (but much more regular) functioning costs: for example, for CREMMA, nearly half of our 40K€ budget was spent, in 2022, on buying two &lt;a href="https://www.nvidia.com/fr-fr/data-center/a100/"&gt;A100 graphic cards from Nvidia&lt;/a&gt;. When using someone else's GPUs, not only you save the money you would spend on the hardware, but on top of that, you contribute to optimizing the use of other GPUs already in place. Another reason is because you might not have the human resources to administer the system and the GPUs. There are multiple calculation clusters created for Academia (of the top of our head: &lt;a href="https://www.cnrs.fr/fr/presse/jean-zay-le-supercalculateur-le-plus-puissant-de-france-pour-la-recherche"&gt;Jean Zay&lt;/a&gt; or &lt;a href="https://www.calculquebec.ca/services-aux-chercheurs/infrastructures-et-services/"&gt;Calcul Québec&lt;/a&gt;), and you could even consider using commercial solutions as well (like &lt;a href="https://aws.amazon.com/nvidia/"&gt;AWS&lt;/a&gt;, &lt;a href="https://cloud.google.com/gpu?hl=fr"&gt;Google Cloud&lt;/a&gt; and the like). Then, your money is spent on the actual computation and not on making the computation possible in the first place.&lt;/p&gt;
&lt;p&gt;Fair enough, plugging eScriptorium's task manager to an external server might not be that simple. However, for smaller groups of users, it is also worth taking into account that it is perfectly possible to train Kraken models using Kraken directly (through an SSH connection to a (super-)cluster, for example) before uploading them into the application. In such a case, eScriptorium is only used for its ergonomics, not as a simplified interface to train models.&lt;/p&gt;
&lt;p&gt;Let's summarize the point here: GPUs are not always a must-have for eScriptorium or Kraken, so you should definitely consider first and foremost your future usage. They currently represent the biggest share in the hardware expenses to build a calculation server. There are options out there where you don't spend 10K€ to buy a GPU but rather connect to an external, ready-to-use service. Or, if you do decide to spend the money, you should consider ways to maximize its usage for other training tasks, possibly outside of eScriptorium.&lt;/p&gt;
&lt;h3&gt;Some considerations on storage&lt;/h3&gt;
&lt;p&gt;Normally, eScriptorium is used as an (assisted) annotation environment to obtain the transcription of documents. You would use eScriptorium:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In a preparatory phase:&lt;ul&gt;
&lt;li&gt;(1a) to produce training data, and&lt;/li&gt;
&lt;li&gt;(1b) to elaborate (aka train) performant segmentation or transcription models;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In a production phase, but only for relatively small corpora, to apply segmentation and transcription models and manually correct the results (in which case the size of the corpora must be compatible with the scale of what an individual or your assembled team can process);&lt;/li&gt;
&lt;li&gt;In a post-production phase, including for samples of a very large corpus, to easily visualize and control the result of the (large-scale) automatic prediction and potentially correct it (cf. n°2).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On the other hand, large scale transcription campaigns should probably be led with Kraken in the command line directly (so only n°1 and n°3 necessitate eScriptorium). Thibault has even produced a small python library to design such campaigns (&lt;a href="https://github.com/ponteIneptique/rtk"&gt;RTK&lt;/a&gt;, for Release the Krakens) which was recently used in &lt;a href="https://enc.hal.science/hal-04250657/"&gt;a paper&lt;/a&gt;&lt;sup id="fnref:lovewarref"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:lovewarref"&gt;2&lt;/a&gt;&lt;/sup&gt; where a 38.5M token corpus was produced. In some cases, n°1b even benefits from being performed outside of eScriptorium, since the application offers a very limited control over &lt;a href="https://kraken.re/main/ketos.html#recognition-training"&gt;Kraken's training parameters&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This has several consequences on the way you should consider storage on a server dedicated to eScriptorium. Duplicates of images are created on the server while they are being processed in the application, but they should always be considered as such: temporary duplicates while phase 1, 2 or 3 are under progress. They shouldn't be considered as if eScriptorium was 1) an archiving solution for transcription projects, 2) a querying interface to explore a corpus or even 3) a publication environment for a minimalistic digital edition. eScriptorium is only one brick --an early one even-- in the corresponding pipelines. Instead, the original image files should be stored somewhere else, in an adapted data warehouse (like &lt;a href="https://zenodo.org/"&gt;Zenodo&lt;/a&gt;, &lt;a href="https://www.nakala.fr/"&gt;Nakala&lt;/a&gt;, etc.), or published in digital libraries under the responsibility of their owner (like &lt;a href="https://archive.org/"&gt;Internet Archive&lt;/a&gt;, &lt;a href="https://gallica.bnf.fr/"&gt;Gallica&lt;/a&gt;, etc.).&lt;/p&gt;
&lt;p&gt;What this means when designing a server to host eScriptorium is that its storage capacity should of course be big enough to store the temporary image files,&lt;sup id="fnref:temporary"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:temporary"&gt;3&lt;/a&gt;&lt;/sup&gt; while users are working on their annotation, aka the active projects. However, this storage doesn't need to be expended all the time and it should also be ok to flush the terminated projects: at that point the images and their annotations should have been archived on more appropriate data warehouses by their creators, and it should be their responsibility.&lt;/p&gt;
&lt;h3&gt;Don't forget the RAM!&lt;/h3&gt;
&lt;p&gt;Not overlooking the &lt;a href="https://en.wikipedia.org/wiki/Random-access_memory"&gt;RAM&lt;/a&gt; is very important when designing your server! But what is it used for? It's used for cache by the web application: it means that frequently accessed data, like web pages and images but also the content of the database, are temporarily loaded in live memory. Cache thus ensures that the requests sent by the users are served quickly. For example, if you don't have enough RAM (or enough cache), pages will load slowly, and if you have used eScriptorium before reading this post, you know how important it is to be able to load images fast enough.&lt;/p&gt;
&lt;p&gt;RAM is also essential for inference and training because images and annotations are loaded in memory before being passed to the CPU or the GPU. If the RAM is not powerful enough, it will be detrimental to computation and will cause a bottleneck situation. Thus having invested in GPUs and/or CPUs but not in enough RAM would be like having a horse to pull a Ferrari: even if prediction and training could go fast on the processing units, it will be restrained by the available live memory.&lt;/p&gt;
&lt;h3&gt;Modularity for the CREMMA infrastructure&lt;/h3&gt;
&lt;p&gt;The CREMMA infrastructure was originally designed by Thibault with a simple but essential principle in mind: modularity. Instead of thinking of an eScriptorium server as a monolithic block of hardware designed for front-end service, storage and intense computation, he suggested to break each of these blocks into individual servers connected together. CREMMA&lt;sup id="fnref:cremma"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:cremma"&gt;4&lt;/a&gt;&lt;/sup&gt; is thus made of at least three servers, as shown in the schema below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CREMMA_FRONTEND&lt;/code&gt;, for the front-end, where the application is deployed and where the database is stored.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CREMMA_STORAGE&lt;/code&gt;, for storage, where all the images and models, as well as the backup of the database are stored on the long term. Currently, &lt;code&gt;CREMMA_STORAGE&lt;/code&gt; has a storage capacity of 38Tb&lt;sup id="fnref:storage"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:storage"&gt;5&lt;/a&gt;&lt;/sup&gt; but we could easily add more disks if we find that it is necessary. &lt;/li&gt;
&lt;li&gt;&lt;code&gt;CREMMA_COMPUTE&lt;/code&gt;, where the two A100 GPUs I mentioned earlier are plugged and where the application task manager "sends" all the jobs, whether they are to be run on CPU (these tasks include segmentation and transcription prediction for example), or on GPU (training for the most part).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/CREMMA_server_specs.png" alt="A model of the CREMMA infrastructure where three blocks (front-end, storage and compute) are connected together through an intranet 10Gb/s connection. For each block, one or two server(s) is presented along with their specification. Credits: Thibault Clérice and Alix Chagué. The full text of the specifications is accessible in a commentary in the source code of this page, just after this image." widht="400px"&gt;&lt;/p&gt;
&lt;!-- 
Full text of the specifications displayed on the image, for accessibility purposes:

- CREMMA_FRONTEND: 2xEPYC 7302 16C/32T; 128 Gb RAM; 1.6 To; RAID with 8Gb Cache.
- CREMMA_STORAGE: Optimized for cache; 256 Gb RAM; 2xRaidZ 19Tb; NVME for ZFS / L2ARC; SSD for OS.
- CREMMA_COMPUTE: 2xAMD EPYC 7302 16C/32T; 128 Gb RAM; 2xRTX A6000 24Gb.
- TRACES-6: 2xEPYC 7452 32C/64T; 512 Gb RAM; 0.8 Tb RAID with 8Gb Cache; 2xA100 40 Gb.
--&gt;

&lt;p&gt;As you can see on the schema, there will actually be a fourth server involved in the infrastructure: &lt;code&gt;Traces-6&lt;/code&gt;, the server we currently use &lt;a href="https://escriptorium.inria.fr/"&gt;to deploy eScriptorium at Inria&lt;/a&gt;. Like &lt;code&gt;CREMMA_COMPUTE&lt;/code&gt;, &lt;code&gt;Traces-6&lt;/code&gt; can be called by &lt;code&gt;CREMMA_FRONTEND&lt;/code&gt; for computation tasks. In fact, this is where the modularity of the system is interesting: with such a set-up, it is possible to add more computation servers to the pool of GPUs reachable by &lt;code&gt;CREMMA_FRONTEND&lt;/code&gt; without having to redesign the whole infrastructure. On their side, &lt;code&gt;CREMMA_FRONTEND&lt;/code&gt; and &lt;code&gt;CREMMA_STORAGE&lt;/code&gt; can be upgraded (to add more RAM or more storage) very easily.&lt;/p&gt;
&lt;p&gt;This modularity also means that the GPUs remain free for other uses: for example if we were to have to run maintenances on &lt;code&gt;CREMMA_COMPUTE&lt;/code&gt;, we can simply cut it from the infrastructure, and let &lt;code&gt;CREMMA_FRONTEND&lt;/code&gt; interact with &lt;code&gt;Traces-6&lt;/code&gt; only while we work on &lt;code&gt;CREMMA_COMPUTE&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;CREMMA_COMPUTE&lt;/code&gt; is equipped with two &lt;a href="https://www.nvidia.com/fr-fr/data-center/a100/"&gt;A100&lt;/a&gt; graphic cards, and &lt;code&gt;Traces-6&lt;/code&gt; with two &lt;a href="https://www.nvidia.com/fr-fr/design-visualization/rtx-6000/"&gt;RTX 6000&lt;/a&gt;. Actually, it doesn't mean that only 4 training can be happening at once. Each of these GPUs offer between 24 and 40 Gb of RAM for intense computation. It's a lot. It's so much actually that training a Kraken model at max speed would rarely use more than 40% of this processing power. &lt;a href="https://www.nvidia.com/en-us/data-center/virtual-solutions/"&gt;Virtualization&lt;/a&gt; is a nice trick to "break" the GPU down into smaller virtual GPUs (or vGPUs). What is broken down is the RAM capacity. We opted for the following virtualization set up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each of the A100 graphic cards and their 40Gb of RAM are turned into 1 10Gb vGPU + 5 5Gb vGPUs (since 10+5x5=35, note that we must leave 5Gb out of the equation for the virtualization).&lt;/li&gt;
&lt;li&gt;No virtualization is applied to Traces-6's RTX6000s.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How did we decide on these numbers? Thibault ran a series of small tests executing either &lt;a href="https://kraken.re/main/ketos.html#segmentation-training"&gt;&lt;code&gt;segtrain&lt;/code&gt;&lt;/a&gt; or &lt;a href="https://kraken.re/main/ketos.html#recognition-training"&gt;&lt;code&gt;train&lt;/code&gt;&lt;/a&gt; and playing with two different parameters: the &lt;a href="https://github.com/mittagessen/kraken/blob/992fb0bc915e689fc76fa6b021e364c3f0f17ca3/kraken/ketos/recognition.py#L38"&gt;batch size&lt;/a&gt;&lt;sup id="fnref:batch_size"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:batch_size"&gt;6&lt;/a&gt;&lt;/sup&gt; and the &lt;a href="https://github.com/mittagessen/kraken/blob/992fb0bc915e689fc76fa6b021e364c3f0f17ca3/kraken/ketos/recognition.py#L79"&gt;single point precision&lt;/a&gt;&lt;sup id="fnref:precision"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:precision"&gt;7&lt;/a&gt;&lt;/sup&gt;. He found that for training a recognition model with a batch size of 8 and either 32 or 16 of precision, less than 5 Gb of RAM on the GPU is enough. With a batch size of 1 and a precision of 32, it's even less than 1 Gb. To train a segmentation model, less than 10Gb is enough, and this type of training is more rare. Since our goal for the infrastructure is not to maximize the speed of the training but to maximize the amount of possible parallel training jobs at decent speed, we decided that 10 vGPUs with 5Gb of RAM and 2 vGPUs with 10Gb of RAM were a good compromise. If we find that more GPU RAM is occasionally needed, we still have two times 24Gb with the RTX6000!&lt;/p&gt;
&lt;h3&gt;Should you build your own server?&lt;/h3&gt;
&lt;p&gt;We have spent all this time writing about how to build, how to spec out your server or your infrastructure, but let's talk about the elephant in the room: should you do it?&lt;/p&gt;
&lt;p&gt;Well, it's all a matter of perspectives. We'd say it probably makes sense if:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You are a very big organization, you have a lot of money available to you, a super-cluster (and possibly a well staffed IT services department), and you have a high demand;&lt;/li&gt;
&lt;li&gt;You are working on very sensitive data that can't be shared with the outside (&lt;em&gt;e.g.&lt;/em&gt; medical reports);&lt;/li&gt;
&lt;li&gt;You are geographically far away from any other existing server, and face latency issues when you connect to potential welcoming servers;&lt;/li&gt;
&lt;li&gt;Servers that exist around you are reluctant to onboard you and the teams behind the request for a server of your own.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These four points are definitely valid. But we'd say that, if you are in another situation, sharing infrastructural costs probably makes way more sense. In our experience, building a server is long, tedious, require special (and rare) skills&lt;sup id="fnref:sysadmin"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:sysadmin"&gt;8&lt;/a&gt;&lt;/sup&gt; and costly (in terms of human resources as well!). Setting up a working server can take a really long time. For CREMMA, we ended up outsourcing part of the installation of the new infrastructure because we realized that we did not have the time nor the skills to set everything up ourselves. The cost of this installation by a third-party? Between 8 and 12K€, and again, a little time and bandwidth on our end.&lt;/p&gt;
&lt;p&gt;Next you have the maintenance fees. You can outsource them, for a little bill from a company which would make sure that everything is installed on time, that updates work well, etc. Or you can do the maintenance yourself. But again, this comes with a cost: human time. A worker on the server goes down? You are in for a few hours. Some people crashed a third-party server by uploading too much IIIF images on your instance of eScriptorium? Well, then you will not only receive emails from these third parties (and this is completely normal), but also have to deal with your user base doing things that eScriptorium allows and that you may not (yet) be able to control/limit.&lt;/p&gt;
&lt;p&gt;In the end, we would definitely recommend that, when this is possible, you first consider joining existing servers, including by offering &lt;em&gt;quid pro quo&lt;/em&gt; by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Participating in covering the salary of people maintaining the server (through some kind of yearly fees for example);&lt;/li&gt;
&lt;li&gt;Providing some money to expand the existing infrastructure (to increase storage or computation, etc);&lt;/li&gt;
&lt;li&gt;In general, helping eScriptorium grow, discussing with the owners of the server you are joining and/or the eScriptorium team about what kind of new functionality should be added, and if you can contribute to fund these updates.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This final point is super important: sure, owning your own server sounds appealing, even if it is costly to put in place. However, developing eScriptorium also comes with expenses. Thus, participating in eScriptorium directly -- we think -- is also very beneficial and welcome by the developing team. Open-source is free to use, free of charge but is not appearing out of thin air: developing costs money. And the more people participate in infrastructural costs (servers or software), the better the experience will be.&lt;/p&gt;
&lt;!-- footnotes --&gt;

&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:localhost"&gt;
&lt;p&gt;If you don't know anything about local servers and are curious to learn more, you can check this page: &lt;a href="https://www.freecodecamp.org/news/what-is-localhost/"&gt;https://www.freecodecamp.org/news/what-is-localhost/&lt;/a&gt;. Or you can also take a look at the corresponding &lt;a href="https://en.wikipedia.org/wiki/Localhost"&gt;entry&lt;/a&gt; in Wikipedia! &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:localhost" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:lovewarref"&gt;
&lt;p&gt;The full reference is: Jean-Baptiste Camps, Nicolas Baumard, Pierre-Carl Langlais, Olivier Morin, Thibault Clérice, et al.. Make Love or War? Monitoring the Thematic Evolution of Medieval French Narratives. Computational Humanities Research (CHR 2023), Dec 2023, Paris, France. ⟨hal-04250657⟩ &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:lovewarref" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:temporary"&gt;
&lt;p&gt;By temporary, we don't mean that the image file are stored for a few hours only, on the contrary, they can stay on the disk for many years. We mean that it should be ok to consider that they can be erased whenever a user is done working on a corpus and has moved away from the transcription phase. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:temporary" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:cremma"&gt;
&lt;p&gt;From now on, "CREMMA" means the server created through the CREMMA project. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:cremma" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:storage"&gt;
&lt;p&gt;Safety first! We have 38 Tb available, but there is actually a little more  physically because we have redundancy and spare. We have 2 series of disks working with redundancy (&lt;a href="https://openzfs.github.io/openzfs-docs/Basic%20Concepts/RAIDZ.html"&gt;RaidZ&lt;/a&gt;). In each series two disks are entirely dedicated to redundancy only, and one more is completely unused until something fails (it is used as a safety spare disk). While &lt;code&gt;CREMMA_STORAGE&lt;/code&gt;, as we said before, is not used as a permanent storage solution, it needs to be a little bit safe for the user base. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:storage" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:batch_size"&gt;
&lt;p&gt;To understand what the batch size corresponds to and why it is important, you can check this entry in the Stack Exchange forum: &lt;a href="https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network"&gt;https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network&lt;/a&gt;. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:batch_size" title="Jump back to footnote 6 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:precision"&gt;
&lt;p&gt;To quote &lt;a href="https://kraken.re/main/ketos.html#recognition-model-training"&gt;Kraken's documentation&lt;/a&gt;: "When using an Nvidia GPU, set the --precision option to 16 to use automatic mixed precision (AMP). This can provide significant speedup without any loss in accuracy." Kraken's default value for precision is 32. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:precision" title="Jump back to footnote 7 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:sysadmin"&gt;
&lt;p&gt;It can be difficult to justify hiring a full-time or even part-time system administrator for a team because it is a very specialized and highly demanded type of profile. For example, public organizations can rarely offer competitive salaries compared to the private sector. In addition, the workload for administrating a web server can be irregular, and it can be difficult to make the skills for system administration meet with other needs faced by a team, complicating even more offering a meaningful full-time job. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:sysadmin" title="Jump back to footnote 8 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>CREMMA</category><category>eScriptorium</category><category>HTR</category><category>kraken</category><guid>https://alix-tz.github.io/phd/posts/017/</guid><pubDate>Fri, 22 Dec 2023 13:43:54 GMT</pubDate></item><item><title>014 - RT(F)M for the Peraire Experiment</title><link>https://alix-tz.github.io/phd/posts/014/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;Turns out, there is more to say on last week's &lt;a href="https://alix-tz.github.io/phd/posts/013/"&gt;experiments on the Peraire dataset&lt;/a&gt;! And I found out while I was working on a completely different dataset. Let me explain!&lt;/p&gt;
&lt;p&gt;This morning, I helped my colleague train a Kraken transcription model for &lt;a href="https://ecrituresnumeriques.ca/fr/Activites/Projets/2016/1/19/Anthologie-grecque"&gt;Greek manuscripts&lt;/a&gt;. They gave me the ground truth and I set and executed the training from the command line. It gave me an opportunity to try fine-tuning a model like &lt;a href="https://zenodo.org/record/7234166"&gt;CREMMA Medieval&lt;/a&gt;, in stead of only training from scratch. &lt;strong&gt;CREMMA Medieval&lt;/strong&gt; was trained on manuscripts written in Latin, whereas the Greek manuscripts were written only, well, in Ancient Greek. I didn't want the resulting model to add Latin letters in the transcription when applied to other Greek documents, so I used Kraken's option to allow the model to forget previously learned characters and to force it to only remember the characters contained in the new training data. This option is called &lt;strong&gt;&lt;code&gt;--resize&lt;/code&gt;&lt;/strong&gt; (check the documentation &lt;a href="https://github.com/mittagessen/kraken/blob/4.3.7/docs/ketos.rst#fine-tuning"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;When I fine-tune a model, I usually follow Kraken's recommendations and keep both the previously learned characters and the new ones coming from the new set of ground truth. When this morning I checked what is the keyword to use to keep only the characters from the new dataset, I realized that I didn't correctly set the training on Peraire last week. I had set it to only keep the new characters!&lt;/p&gt;
&lt;p&gt;Up until Kraken v. 4.3.10, &lt;strong&gt;&lt;code&gt;--resize&lt;/code&gt;&lt;/strong&gt; can take the keywords &lt;strong&gt;&lt;code&gt;both&lt;/code&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;code&gt;add&lt;/code&gt;&lt;/strong&gt;. The ambiguity of these keywords &lt;a href="https://github.com/mittagessen/kraken/issues/478"&gt;has been discussed&lt;/a&gt; in the past, which is the reason why starting from Kraken v. 4.3.10, the keywords respectively become &lt;strong&gt;&lt;code&gt;new&lt;/code&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;code&gt;union&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let's quote the manual:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are two modes dealing with mismatching alphabets, &lt;strong&gt;add&lt;/strong&gt; and &lt;strong&gt;both&lt;/strong&gt;. &lt;strong&gt;add&lt;/strong&gt; resizes the output layer and codec of the loaded model to include all characters in the new training set without removing any characters. &lt;strong&gt;both&lt;/strong&gt; will make the resulting model an exact match with the new training set by both removing unused characters from the model and adding new ones.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I fell for this trap of ambiguity and used &lt;strong&gt;both&lt;/strong&gt; instead of &lt;strong&gt;add&lt;/strong&gt;, thinking &lt;strong&gt;both&lt;/strong&gt; meant I was keep &lt;em&gt;both&lt;/em&gt; character sets. (Again this is the very reason why the keywords were recently changed).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Side note: you should really read &lt;a href="https://alix-tz.github.io/phd/posts/013/"&gt;last week's post&lt;/a&gt; to fully understand the rest of this post!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;At the end of my post last week, I wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;peraire_D&lt;/strong&gt; on the other hand seems to lose it completely on the B series. This is most likely due to the fact that the contrast between the page and the "ink" is too low in the pencil-written series compared to the data used to train &lt;strong&gt;Manu McFrench&lt;/strong&gt; and in the D series. &lt;strong&gt;peraire_D&lt;/strong&gt; even loses 11 points of accuracy to &lt;strong&gt;Manu McFrench&lt;/strong&gt;!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But how could I be sure that it was not actually due to the fact that the model had unlearned some precious characters?&lt;/p&gt;
&lt;p&gt;The only way to know, I thought, was to re-train the models! I used this opportunity to also train the models from scratch because I was curious to see how much noise/improvement was brought by the base model.&lt;/p&gt;
&lt;p&gt;I tried 4 types of models and, like last week, used &lt;a href="https://github.com/WHaverals/CERberus"&gt;CERberus 🐶🐶🐶&lt;/a&gt; to measure the character error rates on the predictions made on the test sets:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Models trained "from scratch"&lt;/li&gt;
&lt;li&gt;A model not trained on any data coming from the Peraire dataset (aka &lt;a href="https://zenodo.org/record/6657809"&gt;&lt;strong&gt;Manu McFrench&lt;/strong&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Models obtained from finetuning &lt;strong&gt;Manu McFrench&lt;/strong&gt; using the &lt;strong&gt;add&lt;/strong&gt; resize mode&lt;/li&gt;
&lt;li&gt;Models obtained from finetuning &lt;strong&gt;Manu McFrench&lt;/strong&gt; using the &lt;strong&gt;both&lt;/strong&gt; resize mode&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For each model trained on the Peraire dataset, I used 3 compositions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the full dataset ("ALL")&lt;/li&gt;
&lt;li&gt;only data coming from the B series ("B")&lt;/li&gt;
&lt;li&gt;only data coming from the D series ("D")&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I used the same composition system for the test sets.&lt;/p&gt;
&lt;p&gt;Here are my results in the form of a table:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/peraire_scores.png" alt="a table of the scored obtained on the different train set, test set and resize configurations" widht="400px"&gt;&lt;/p&gt;
&lt;p&gt;Fortunately, it seems that my previous interpretation is not fully contradicted by the results I obtain with this second series of training. Let's focus on two observations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Whenever a model is trained only on the D series, and tested only on the B series, it appears to be completely incapable of predicting anything but gibberish, losing between 32 and 35 points of accuracy. It confirms that the aspect of the documents from the two series are too different. On the other hand, when the model is fine-tuned on the B series only, it maintains a fairly good accuracy when applied to the D series, whichever resize mode is used. I think it confirms that the B series is enough for the model to learn some sort of formal features from Peraire's handwriting, which the models can transfer to documents written with a different writing instrument.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is very interesting is the difference between the models trained on the whole datasets and tested on the B series: when we use the &lt;strong&gt;both&lt;/strong&gt; resize mode (meaning we only keep the characters from the new dataset), the model is very good. On the contrary, the performance of the model trained with the &lt;strong&gt;add&lt;/strong&gt; resize mode (meaning we keep the output layer and the codec from the base model and add the new characters) is as bad as with a model trained only on the D series.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my previous post, I wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;peraire_both&lt;/strong&gt; is able to generalize from seeing both datasets and even benefits from seeing more data thanks to the D series, since it performs better on the B series compared to &lt;strong&gt;peraire_B&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, in the light of my experiment with the &lt;strong&gt;resize&lt;/strong&gt; option, I think this is not correct. Instead, it appears that resetting the output layer by using &lt;strong&gt;both&lt;/strong&gt; (or &lt;strong&gt;new&lt;/strong&gt;) on accident, allowed the model to better take into account the data from the B series (pencil). Contrary to what I observed last week, the model trained on the whole dataset but this time with the &lt;strong&gt;add&lt;/strong&gt; resize mode (or &lt;strong&gt;union&lt;/strong&gt;) doesn't benefit from seeing more data compared to the model trained only on the B series.&lt;/p&gt;
&lt;p&gt;My understanding is that keeping the output layer from the base model with &lt;strong&gt;add&lt;/strong&gt; (or &lt;strong&gt;union&lt;/strong&gt;) probably drowns the specificity of the pencil-written documents into a base knowledge tailored to handle documents with a high contrast (like the ones in the D series &lt;em&gt;and&lt;/em&gt; in &lt;strong&gt;Manu McFrench&lt;/strong&gt;'s training set). Or, to put it differently, when we use &lt;strong&gt;both&lt;/strong&gt; (or &lt;strong&gt;new&lt;/strong&gt;), more attention is given to the pencil written documents, meaning that the model actually gets better at handling this category of data.&lt;/p&gt;
&lt;p&gt;I am extremely curious to see how I can investigate this further, or if any of you, readers, would understand these results differently!&lt;/p&gt;</description><category>experiment</category><category>HTR</category><category>kraken</category><guid>https://alix-tz.github.io/phd/posts/014/</guid><pubDate>Fri, 04 Aug 2023 17:51:14 GMT</pubDate></item><item><title>013 - The Peraire experiment</title><link>https://alix-tz.github.io/phd/posts/013/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;&lt;em&gt;WARNING: in my &lt;a href="https://alix-tz.github.io/phd/posts/014/"&gt;next post&lt;/a&gt;, I nuance the conclusions drawn in this post, because of a parameter I didn't correctly set during the training of the models described below. You should really read it after reading this post, to get the full picture!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As a small side project during my phD, I have been sharing my expertise (and a bit of my workforce) with the members of the &lt;a href="https://www.pamir.fr/projets-soutenus/spe-vlp/"&gt;DIM SPE-VLP&lt;/a&gt; project. The acronym stands for "Sauver le patrimoine espérantiste : le voyage de Lucien Péraire (1928-1932)." The project revolves around the digitization, transcription and edition/valorization of &lt;a href="https://fr.wikipedia.org/wiki/Lucien_P%C3%A9raire"&gt;Lucien Peraire&lt;/a&gt;'s archives. He was a French citizen who, in the late 1920s, travelled across the European and the Asian continents, mostly by bike and using &lt;a href="https://en.wikipedia.org/wiki/Esperanto"&gt;Esperanto&lt;/a&gt; to communicate. He kept a diary during his journey (and later published a book about his adventures). His notes are written both in French and in Esperanto and in some documents, he also used &lt;a href="https://en.wikipedia.org/wiki/Shorthand"&gt;stenography&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My contribution to the project has mostly consisted in helping developing transcription models for the French diaries (although I'm also interested in the shorthand and the esperanto). This meant both helping with the production of ground truth and training &lt;a href="https://kraken.re/"&gt;Kraken&lt;/a&gt; models. This post will briefly explain how the ground truth was created and published, as well as present the models that were trained with it.&lt;/p&gt;
&lt;p&gt;Peraire's notebooks are organized in different series, and each series is divided in ensembles regrouping the pages of a notebook. Each ensemble is named after the countries visited while the notebook was used. For example, notebook 11 in the B series forms one ensemble and covers a part of Peraire's travels in Japan. There are 31 notebooks in the B series. The notebooks of this series are written with a blue pencil on (low quality) school papers. On some pages, the pencil is very faded which makes it hard to read the text, let alone to run a successful segmentation task on the image. On the other hand, the D series gathers notes and comments on the diaries, written at the end of the 1960s. This time the handwriting is much easier to read because Peraire mostly used a blue or black ball-point pen. There are 9 ensembles in this series.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/peraire_handwriting.png" alt="two extracts of Peraire's notebooks side by side, on the left the image is taken from the B series, on the right the image is taken from the D series." widht="600px"&gt;&lt;/p&gt;
&lt;p&gt;One aspect that I find particularly interesting with this dataset is that we have a case where the handwriting is similar but the writing tool is different. It means that it is possible to explore how the writing tools and/or writing supports affect the efficiency of a transcription model. On top of that, all the documents were digitized under the same (good) conditions and by the same people.&lt;/p&gt;
&lt;h3&gt;Segmenting, transcribing, aligning and publishing&lt;/h3&gt;
&lt;p&gt;The first version of the dataset was solely focused on the B series. I selected 1 random page from each ensemble (avoiding to take the first page each time) to compose a train set of 33 files&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/013/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;. On top of that, I selected 4 additional pages from B3, B5, B12 and B18 to compose a fixed test set which would never be used as training data.&lt;/p&gt;
&lt;p&gt;I pre-segmented the images with Kraken's default model before correcting the result manually. At this point, I also applied the &lt;a href="https://segmonto.github.io/"&gt;segmOnto&lt;/a&gt; ontology for the lines and regions&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/013/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;. Because of the &lt;a href="https://raw.githubusercontent.com/alix-tz/peraire-ground-truth/master/data/train/B.2.europe-orientale_0007.jpg"&gt;fading ink&lt;/a&gt;, some words could not be transcribed. In order to avoid complicating the transcription rules, I decided to simply segment out the passages that couldn't be read. On the one hand it simplifies the transcription, but on the other hand, it means that a small portion of my segmented documents cannot be re-used by others to train a segmentation model. Since we were not training a segmentation model, it was an easy decision.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/peraire_faded.png" alt="screenshot showing the segmentation and the transcription panels from eScriptorium where we can see that some lines are broken down into several segments and that some segments were left blank" widht="400px"&gt;&lt;/p&gt;
&lt;p&gt;More recently, it was decided to augment the dataset with examples from the D series because the model trained on the B series was not good enough. This time, Gilles Pérez, a member of the project, took charge of the transcription. I recommended to create a new sample of 30 to 40 images, so he randomly selected series of 4 continuous pages from each ensemble. The transcription of the corresponding 36 pages was sent to me as a Word document. Therefore, on top of taking care of the segmentation of the images, I also went through an alignment phase during which I verified the order of the lines and copy-pasted the transcription. It took longer than I expected but it allowed me to align the transcription with the rules I had followed when creating the first set. I also picked 4 of the 36 pages to add to the test set.&lt;/p&gt;
&lt;p&gt;The dataset is versioned and published applying the principles and tools we developed withing the frame of &lt;a href="https://htr-united.github.io/"&gt;HTR-United&lt;/a&gt;. I also added illustrated segmentation and transcription guidelines.&lt;/p&gt;
&lt;h3&gt;Testing different dataset configurations to train transcription models&lt;/h3&gt;
&lt;p&gt;As I mentioned before, the goal of these datasets was to create transcription models. Taking the opportunity of the recent update of the dataset, I tried different scenarios.&lt;/p&gt;
&lt;p&gt;I never trained the model from scratch because the dataset is too small to get any sort of usable model. Instead, I used &lt;a href="https://zenodo.org/record/6657809"&gt;&lt;strong&gt;Manu McFrench&lt;/strong&gt;&lt;/a&gt; as a base model, fine-tuned with the Peraire dataset. (We were actually able to use Peraire as an example during the &lt;a href="https://www.conftool.pro/dh2023/index.php?page=browseSessions&amp;amp;form_session=76#paperID690"&gt;DH2023&lt;/a&gt; conference&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/013/#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt; earlier this month to show the usefulness of having this kind of base model). I tested fine-tuning only on the B series, only on the D series or on both the B and the D series. Then I used a B-series-only test set, a D-series-only test set and the full test set to see how the models performed.&lt;/p&gt;
&lt;p&gt;Since I wanted to try it after discovering it during DH2023, I used &lt;a href="https://github.com/WHaverals/CERberus"&gt;CERberus 🐶🐶🐶&lt;/a&gt; (I talked about it in my &lt;a href="https://alix-tz.github.io/phd/posts/012/"&gt;last post&lt;/a&gt;) to measure the accuracy of the models on the test sets listed above.&lt;/p&gt;
&lt;p&gt;Like &lt;a href="https://huggingface.co/spaces/lterriel/kami-app"&gt;KaMI&lt;/a&gt;, CERberus takes 2 categories of text input: the reference (aka the ground truth) and the prediction (or the hypothesis made by the model). In order to get the prediction, I loaded my models on eScriptorium, as well as the images and transcription of the test set before applying each model to the documents. This way, all the transcription are predicted with the same segmentation, which comes from the ground truth.&lt;/p&gt;
&lt;p&gt;Here are the results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Manu McFrench&lt;/strong&gt;, before fine-tuning, gets a CER of 26.16% when tested on the whole test set, and a score of 27.19% on the documents from the B series, 25.29% on the D series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_both&lt;/strong&gt;, trained on the B and the D series, gets a CER of 4.63% when tested on the whole test set, but a score of 6.41% on the documents from the B series and 3.54% on the D series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_B&lt;/strong&gt;, trained only on the B series, gets a CER of 8.72% on the whole test set, but a score of 7.12% on test-B and 9.67% on test-D.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_D&lt;/strong&gt;, trained only on the D series, gets an CER of 16.38% on the whole test set, but this is because of the enormous descripancy between its score on each sub test set. It skyrockets to a CER of 38,53% on test-B while going as low as 3.65% on test-D.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of this makes sense, though.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ManuMcFrench&lt;/strong&gt; could not be used without fine-tuning, its error rate on both documents is too high.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_both&lt;/strong&gt; is able to generalize from seeing both datasets and even benefits from seeing more data thanks to the D series, since it performs better on the B series compared to &lt;strong&gt;peraire_B&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_B&lt;/strong&gt; which was trained on the more difficult dataset seems to use the knowledge inherited from &lt;strong&gt;Manu McFrench&lt;/strong&gt; and to have learned some formal features from Peraire's handwriting since it is able to maintain a fairly low CER on the D series (it gains 16 points of accuracy compared to &lt;strong&gt;Manu McFrench&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_D&lt;/strong&gt; on the other hand seems to lose it completely on the B series. This is most likely due to the fact that the contrast between the page and the "ink" is too low in the pencil-written series compared to the data used to train &lt;strong&gt;Manu McFrench&lt;/strong&gt; and in the D series. &lt;strong&gt;peraire_D&lt;/strong&gt; even loses 11 points of accuracy to &lt;strong&gt;Manu McFrench&lt;/strong&gt;!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What happens with &lt;strong&gt;peraire_D&lt;/strong&gt; is very interesting because it confirms that it is useful to compose a train set with examples of more difficult documents instead of only showing the ones that are easy to read! Now, the nice thing is that I will soon be working on a little experiment with my colleague Hugo Scheithauer where we will be able to measure the impact of the contrast between the ink and the paper. Stay tuned!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT #1: I added the scores obtained by Manu McFrench alone.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT #2: I added a disclaimer at the beginning of the post.&lt;/em&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;I used 2 images from B2 because one of them was extremely faded and I wanted to include some of these extreme cases in the dataset, and 2 images from B30 because it consisted of shorter lines (table of contents) which I found was interesting to include. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/013/#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;As described in the documents, I only used the "InterlinearLine" and "DefaultLine" for the lines, and the "MainZone" and "NumberingZone" for the regions. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/013/#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;See the submission and the slides on HAL: &lt;a href="https://inria.hal.science/hal-04094241"&gt;https://inria.hal.science/hal-04094241&lt;/a&gt;. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/013/#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>experiment</category><category>HTR</category><category>kraken</category><guid>https://alix-tz.github.io/phd/posts/013/</guid><pubDate>Fri, 28 Jul 2023 15:39:18 GMT</pubDate></item><item><title>012 - "It did a very good job"</title><link>https://alix-tz.github.io/phd/posts/012/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;A few weeks ago, I attended the presentation of an automatic transcription software. The majority of the audience was unfamiliar with the concept of handwritten text recognition (HTR) or had little experience using it. The presentation lasted only an hour, so it couldn't delve into much detail. Its main objective was to demonstrate the software's results. The presenter showed several slides, displaying on one side  images of manuscripts (often in a language unknown to the audience) and on the other side the transcriptions generated by the software. Throughout the presentation, the presenter repeatedly commented on the HTR software saying that "it did a very good job."&lt;/p&gt;
&lt;p&gt;But what does it even mean?&lt;/p&gt;
&lt;p&gt;The very first aspect to explore is what distinguishes a good job from a bad one. Normally, such an evaluation relies on the measurement of the accuracy of the result compared to the ideal transcription. The accuracy can be expressed positively or negatively using the error rates (a 0% error rate is the same as a 100% accuracy).&lt;/p&gt;
&lt;p&gt;Measuring the accuracy of a prediction (another way to call the result of HTR) is commonly done at character level. The character accuracy of a model is equal to the number of matches between the prediction and the ideal transcription. The character error rate (CER) is a very common measure to express a model's theoretical efficiency.&lt;/p&gt;
&lt;p&gt;Sometimes softwares also consider the word error rate (WER), which is the proportion of words in the prediction containing errors. A high score at WER doesn't actually mean that the transcription is bad. It only means that the errors are distributed on all the words. I never use WER alone because it is hard to get an exact impression of the quality of the prediction based on that metric alone.&lt;/p&gt;
&lt;p&gt;There is a paper from &lt;a href="https://dl.acm.org/doi/10.1145/3476887.3476888"&gt;Neudecker et al. (2021)&lt;/a&gt; where they test 5 different software used for evaluating the prediction. They also develop an interesting reflection on alternative metrics such as the "non-stopword accuracy", the "phrase accuracy", the "flexible character accuracy" (which is useful when the line order isn't always the same), the "figure of merit" (which "aims to quantify the effort required for manual post-correction" (p. 15)) or else the "unordered WER".&lt;/p&gt;
&lt;p&gt;When your score is a rate, there is an implicit idea that 100% is both the maximum score and the targeted score (for accuracy of course). But in the case of HTR, 100% accuracy is extremely rare because there are also edge cases where the way a letter was drawn is ambiguous: in such cases the error is not particularly caused by the inaccuracy of the HTR engine but rather by the imperfection of the handwriting in the first place.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.46"&gt;Hodel et al., (2021)&lt;/a&gt;, the authors provided a grid to interpret accuracy scores. They suggest the following three thresholds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CER &amp;lt; 10% == good (it allows efficient post-processing)&lt;/li&gt;
&lt;li&gt;CER &amp;lt; 5% == very good (errors are usually focused on rare or unknown words)&lt;/li&gt;
&lt;li&gt;CER &amp;lt; 2.5% == excellent (but it is usually only reached when the handwriting is very regular)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Personally, I think this grid should also include 20% and 0%. 20% as a threshold, because at 80% of accuracy, the transcription is supposedly good enough for fuzzy search and keyword spotting (I should add a reference here, but I can't find it anymore...); and 0% because it should be reminded that an accuracy of 100% is virtually impossible.&lt;/p&gt;
&lt;p&gt;To complement this, I would like to mention another possible approach to get an interpretable score: during the DH2023 conference, Thibault Clérice and I &lt;a href="https://inria.hal.science/hal-04094241"&gt;presented an experiment&lt;/a&gt; where we trained a model using the same data in the train set and the test set. Our model reached an accuracy close to 90%, which we were able to use as a baseline to define the highest accuracy score possible for the data we had. Thus we were able to consider that a model approaching 90% of accuracy would be an excellent model, as far as that dataset was concerned.&lt;/p&gt;
&lt;p&gt;Still during &lt;a href="https://www.conftool.pro/dh2023/index.php?page=browseSessions&amp;amp;form_session=76#paperID395"&gt;the DH2023 conference&lt;/a&gt;, Wouter Haverals introduced &lt;a href="https://github.com/WHaverals/CERberus"&gt;CERberus 🐶🐶🐶&lt;/a&gt;, a web interface which addresses the same type of issues as &lt;a href="https://huggingface.co/spaces/lterriel/kami-app"&gt;KaMI&lt;/a&gt;: the lack of nuance in a plain CER computation. Indeed, in a CER score, every type of error has the same weight. This means that mistaking an "e" for a "é" costs the same as mistaking a "e" for a "0": in the first case the text is likely still readable or understandable, whereas in the latter, it might not be the case.&lt;/p&gt;
&lt;p&gt;The CER metric is still very useful, but when applied to transcription projects, it is even more valuable when we can filter the types of errors we want to include in the evaluation.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT: I should have noted here that my reflection was focused on the evaluation of an automatic transcription in cases where you already have the expected transcription. When we apply an HTR model to a whole new set of documents, we usually don't have the correct transcription at hand (otherwise we wouldn't use HTR in the first place). This is the reason why many researchers try to find ways to evaluate the quality of the transcription without ground truth. One example can be found in &lt;a href="https://enc.hal.science/hal-03828529"&gt;Clérice (2022)&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So, to go back to our initial problem, we can see that there are many ways to draw the line between a good job and a bad one. The threshold will depend on the metric used to express the accuracy of the prediction and also (and actually mostly) on the way the generated text will be used down the line. Even though the software presentation I attended was short, I think we should always remind future users of HTR that 100% of accuracy is not always what they are seeking.&lt;/p&gt;
&lt;p&gt;A short reflection to finish this post: I was bothered by the expression used to qualify the transcription. I am still trying to figure out a way to put it into words. On top of lacking accuracy, the expression "it did a good job" was also calling for a vision of HTR as a magic tool at the service of the searchers and students. But, in which other cases do you say that someone did "a good job?" Likely when you delegate a task to a &lt;a href="https://africanarguments.org/2023/03/the-invisible-labour-of-africa-in-the-digital-revolution/"&gt;subaltern&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I see a problem here: in their current state, HTR engines are efficient but not to the point that people can use them without thinking clearly about what they want the engine to produce. It is easy to sell a software pretending that it is a magic servant that will do all the transcription in your place, a tool so smart that you can even consider delegating a part of your responsibility to it. But I think when new users of HTR fail to first reflect on the outcome they can reasonably expect from these engines, it creates disappointment and crappy data and workflows.&lt;/p&gt;</description><category>accuracy</category><category>evaluation</category><category>HTR</category><category>metrics</category><guid>https://alix-tz.github.io/phd/posts/012/</guid><pubDate>Sat, 15 Jul 2023 12:06:20 GMT</pubDate></item></channel></rss>