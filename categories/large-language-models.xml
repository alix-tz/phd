<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="https://alix-tz.github.io/phd/assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>A research (b)log (Posts about Large Language Models)</title><link>https://alix-tz.github.io/phd/</link><description></description><atom:link href="https://alix-tz.github.io/phd/categories/large-language-models.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="https://alix-tz.github.io/phd/"&gt;Alix Chagué&lt;/a&gt; CC-BY</copyright><lastBuildDate>Mon, 01 Dec 2025 17:02:20 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>025 - A Perfect Job is the New Very Good Job</title><link>https://alix-tz.github.io/phd/posts/025/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;blockquote&gt;
&lt;p&gt;A little disclaimer for once, because I usually prefer to praise if I name people. I do not know Dan Cohen nor his work, my criticism of his article is not directed against him personally, but rather it takes his text as one example among many of the kind, that develop the same type of discourse and contain the same type of flaws.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Plus tôt cette semaine, mon collègue Louis-Olivier Brassard m'a demandé mon avis sur le &lt;a href="https://newsletter.dancohen.org/archive/the-writing-is-on-the-wall-for-handwriting-recognition/"&gt;dernier billet&lt;/a&gt; posté par Dan Cohen, qu'il a intitulé "&lt;em&gt;The Writing Is on the Wall for Handwriting Recognition&lt;/em&gt;", ajoutant un sous-titre annonçant la couleur: "&lt;em&gt;One of the hardest problems in digital humanities has finally been solved&lt;/em&gt;". J'avais envie de rendre un peu plus public ma lecture critique, donc j'en tire un billet de blog, en français pour une fois.&lt;/p&gt;
&lt;p&gt;J'ai lu avec attention cet article car le sujet m'intéresse (forcément), mais je ne cache pas que je débute en général ce genre de lecture avec un a priori négatif. C'est le traitement que je réserve à tous ces postes, de blog ou sur les réseaux sociaux, qui annoncent à tour de bras que l'IA générative a révolutionné ceci ou cela -- ceci et cela étant généralement des problèmes qui ont occupé des chercheur-ses et ingénieur-es depuis des années, et qui donnent lieu à des débats parfois houleux voire insolvables. Tous ces billets contribuent à alimenter l'esbroufe de l'IA générative et à saper notre capacité collective déjà pas mal usée à développer une pensée critique à son endroit. &lt;!--J'essaie quand même d'être honnête et de faire attention à mes propres biais dans ce que j'en tire ci-dessous.--&gt;&lt;/p&gt;
&lt;p&gt;Le billet de Dan Cohen fait suite à la sortie de la version 3 de Gemini, le modèle d'IA générative de Google, publicisé comme le modèle de Google "le plus intelligent à date" ("&lt;em&gt;our most intelligent model yet&lt;/em&gt;" dit Google). Comme à chaque fois qu'un nouveau modèle de ce type sort, plusieurs utilisateurs partagent les résultats de leurs "expérimentations" avec ces modèles. Dan Cohen n'est pas le seul, par exemple Mark Humphries a aussi posté le même jour &lt;a href="https://generativehistory.substack.com/p/gemini-3-solves-handwriting-recognition"&gt;un billet sur le sujet&lt;/a&gt; intitulé sobrement "&lt;em&gt;Gemini 3 Solves Handwriting Recognition and it’s a Bitter Lesson&lt;/em&gt;". J'ai beaucoup vu ces deux billets relayés sur BlueSky, salués par des chercheurs que j'estime occuper des place d'autorité dans le domaine de la transcription automatique. Après avoir lu le billet de Dan Cohen, je me suis retrouvée assez agacée de ces relais: je ne suis pas convaincue que le texte ait été bien lu par ceux qui l'ont relayé sur BlueSky.&lt;/p&gt;
&lt;p&gt;A mon avis, le problème du billet que Dan Cohen est double: 1) il développe un discours universel sur un outil qu'il n'a testé que sur sélection minime d'exemples qui ne disent presque rien des problèmes que rencontrent les utilisateurs de la transcription automatique sur les documents anciens, 2) sa démonstration tient sur des arguments fallacieux.  &lt;/p&gt;
&lt;h3&gt;Un problème de rigueur scientifique&lt;/h3&gt;
&lt;p&gt;Sur le premier point tout d'abord. Dan Cohen utilise trois exemples qui ne sont pas du tout représentatifs des défis de la transcription automatique. D'emblée, cela justifierait une note de bas de page à son sous-titre: il dit "l'un des problèmes les plus difficiles des humanités numériques a enfin été résolu", j'ajoute "en ce qui concerne les documents épistollaires rédigés en anglais durant la première moitié du XIXe siècle par des personnalités dont des biographies ont été écrites, voire dont la correspondance à déjà été éditée"&lt;sup id="fnref:precision_inedit"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025/#fn:precision_inedit"&gt;1&lt;/a&gt;&lt;/sup&gt; car c'est ce qu'il a testé. Ca réduit déjà pas mal la portée de ses résultats, non? D'ailleurs, étant donné que le modèle ne parvient pas à transcrire le troisème exemple, on pourrait même ajouter que cela ne concerne en plus que les documents dont la mise en page est simple.&lt;sup id="fnref:standard_layout"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025/#fn:standard_layout"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Ce premier point est vraiment problématique parce qu'il s'agit d'un texte publié par une personne qui a une autorité scientifique et qui devrait donc faire preuve de rigueur scientifique, même si ce texte n'est qu'une newsletter et pas un article ou un ouvrage édité. J'attendrais de cette rigueur scientifique qu'on se limite à tirer des conclusions sur ce que l'on a réussi à démontrer au lieu de jouer les Cassandre avec des (sous-)titres tape-à-l'oeil. On peut avoir la conviction que Gemini est capable de traiter avec succès bien d'autres cas que ceux présentés par Dan Cohen, mais cela relève de la croyance, pas de la démonstration scientifique. Je pense que c'est un sujet qui doit être discuté plus largement, dans un contexte où l'IA nous est messianiquement servie à toutes les sauces, mais Marcello Vitali-Rosati en parle bien dans &lt;a href="https://blog.sens-public.org/marcellovitalirosati/2025-11-htr.html"&gt;son dernier billet&lt;/a&gt; ou encore, sous un autre angle et qui sort des usages par le monde académique, il y a le récent travail d'&lt;a href="https://www.polytechnique-insights.com/tribunes/digital/comment-se-proteger-du-syndrome-de-stockholm-technologique-face-a-lia/"&gt;Hamilton Mann&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Il se trouve que le jour où Louis-Olivier m'a demandé de lire le texte de Dan Cohen, j'avais aussi lu celui de &lt;a href="https://digitalorientalist.com/2025/11/25/teaching-bengali-digital-texts-to-anglophone-undergraduates-what-voyant-reveals-about-the-infrastructural-bias-of-dh-tools/"&gt;Sunayani Bhattacharya&lt;/a&gt; qui a formé ses élèves du Saint Mary’s College en Californie à l'analyse de texte avec &lt;a href="https://voyant-tools.org/"&gt;Voyant Tools&lt;/a&gt; et qui traite aussi de transcription automatique au détour de son billet. Elle explique que, dans l'optique de proposer une ouverture vers le Sud Global à ses étudiant-es, elle les a fait travailler sur des textes en Bengali (même si aucun ne sait parler ou lire le Bengali). Je trouve l'exercice intéressant et prometteur tel qu'elle le présente. Après avoir développé chez ses élèves une familiarité avec ce à quoi ressemble les textes de presse correctement édités dans Voyant Tools, elle leur a montré ce qu'on obtient quand on tente de faire tourner Voyant Tools sur des textes directement sortis d'un logiciel d'OCR. Ces textes contiennent énormément de bruit et parfois n'utilisent même pas les bons jeux de caractères. Cela lui permet de donner un exemple très concret à ses étudiant-es des limites des infrastructures logicielles dès qu'il s'agit de traiter de textes en langues indiennes. Elle conclut en redisant l'utlité de donner aux étudiant-es une meilleure idée de ce à quoi ressemblent les biais anglophones dans la technologie quand on est sur le terrain. Dans un texte comme celui dont je discute dans ce billet, ce biais anglophone (et j'ajouterai même moderniste) saute aux yeux.&lt;/p&gt;
&lt;h3&gt;Une démonstration bancale&lt;/h3&gt;
&lt;p&gt;Maintenant, concernant le deuxième point, il suppose de regarder d'un peu plus près ce que Dan Cohen nous dit et les exemples qu'il donne. Il y a des imprécisions qui doivent être relevées, mais aussi des extraits qui ne correspondent pas aux affirmations qui sont faites dans le billet.  &lt;/p&gt;
&lt;p&gt;Une imprécision qui commence justement par la question de la précision des modèles. J'en ai déjà parlé dans &lt;a href="https://alix-tz.github.io/phd/posts/012/"&gt;un précédent billet&lt;/a&gt; car il me semble que c'est l'un des sujets où les chercheurs font le plus preuve de paresse: de quelle précision on parle, et quelles sont les limites de ces mesures de précision ? Dan Cohen affirme que "&lt;em&gt;les meilleurs logiciel d'HTR ont du mal à atteindre 80% de précision&lt;/em&gt;". Comme il clarifie que cela signifie 2 mots faux tous les 10 mots, déjà on s'aperçoit qu'il nous parle de taux d'erreur au mot et non au caractère. Un tel taux d'erreur ne dit rien de la lisibilité du texte puisqu'une seule erreur suffit pour que le mot soit compté comme faux. Dans une phrase comme "&lt;em&gt;the hardest problem in digtial humaities has finolly beeen sol ved&lt;/em&gt;", un mot sur deux contient une faute, pourtant il me semble que la phrase est parfaitement lisible.&lt;sup id="fnref:lisible"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025/#fn:lisible"&gt;3&lt;/a&gt;&lt;/sup&gt; Pour mettre les choses en perspective, le taux de précision au caractère dans cette phrase, lui, est de 90.77% (d'après un logiciel comme &lt;a href="https://huggingface.co/spaces/lterriel/kami-app"&gt;KaMI&lt;/a&gt;). En plus de cette imprécision de départ, l'affirmation de Dan Cohen sur les difficultés des logiciels traditionnels me semble fause. Je ne vois pas sur quelle source il se base. Pour des documents comme ceux qu'il teste, on est bien au-delà des 80% de précision, y compris au mot, et ce avec plusieurs modèles et plusieurs logiciels.&lt;/p&gt;
&lt;p&gt;Comme cette affirmation m'a surprise, j'ai voulu regarder si vraiment le modèle de Transkribus avait fait autant de fautes que ça. Bien sûr, il a fait des erreurs. Quand on regarde le document source, on voit que certaînes sont compréhensibles dans un contexte zero-shot: lorsque Boole trace deux "l" à la suite, son deuxième "l" ressemble à un "e" avec une boucle très très petite. C'est ce qui explique que la prédiction de Transkribus contient des erreurs sur "&lt;em&gt;tell&lt;/em&gt;" (lu "&lt;em&gt;tele&lt;/em&gt;") sur la page de gauche, et "&lt;em&gt;All&lt;/em&gt;" (lu "&lt;em&gt;Ale&lt;/em&gt;") sur la page de droite. Pour savoir quelle était vraiment l'ampleur des erreurs de Transkribus, j'ai fait ma propre transcription de la double page, ligne par ligne (en suivant l'ordre des lignes tiré de la segmentation dans Transkribus, et en m'aidant un peu de la lecture proposé par Gemini&lt;sup id="fnref:ordre_lignes"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025/#fn:ordre_lignes"&gt;4&lt;/a&gt;&lt;/sup&gt;). Quand je calcule le taux de précision sur cet extrait, j'obtiens une précision au caractère d'environ 95% et une précision au mot de 88%.&lt;sup id="fnref:precision_error_tkb"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025/#fn:precision_error_tkb"&gt;5&lt;/a&gt;&lt;/sup&gt; Largement perfectible donc, mais on n'est pas dans une situation catastrophique comme le laisse supposer le préambule.&lt;/p&gt;
&lt;p&gt;Maintenant, si on regarde la transcription de Gemini, on s'aperçoit qu'il y a en fait aussi des erreurs, alors que Dan Cohen nous dit "&lt;em&gt;Gemini transcribed the letter perfectly&lt;/em&gt;". Par exemple, Gemini transcrit, sur la page de droite, "&lt;em&gt;occasionally by&lt;/em&gt;",&lt;sup id="fnref:occasion_by"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025/#fn:occasion_by"&gt;6&lt;/a&gt;&lt;/sup&gt; en générant comme précision complémentaire dans une section de notes que "&lt;em&gt;On the right page (line 8), the handwriting becomes very scribbled. It appears to say 'take a long walk occasionally try &amp;amp; once or twice...' or possibly 'occasionally by &amp;amp; once or twice...'.&lt;/em&gt;" Donc Gemini, échoue ici à proposer de lire une césure qui fait pourtant sens et préfère ajouter un mot dans sa transcription. Le problème ce n'est pas que Gemini n'ai pas fait une transcription parfaite bien sûr, mais plutôt que Dan Cohen l'affirme sans relever cette erreur.&lt;/p&gt;
&lt;p&gt;On a le même problème dans le deuxième exemple, où Gemini met en forme le mot "transmitted" pour signaler qu'il est barré dans la source alors que ce n'est pas le cas. Le texte généré par Gemini ne laisse pas de doute vis-à-vis de l'aspect du texte dans la source, et invente une intention de la part de l'auteur: "&lt;em&gt;In the second line of the body, the word 'transmitted' is crossed out in the original text, but the sentence is grammatically incomplete without it (or a similar verb). It is likely the author meant to replace it to avoid repetition with the word 'transmitting' appearing a few lines later but forgot to insert the new word.&lt;/em&gt;" Alors que cette erreur était plus facile à repérer, Dan Cohen nous dit pourtant encore une fois: "&lt;em&gt;Another perfect job.&lt;/em&gt;"&lt;/p&gt;
&lt;p&gt;Le coup de grâce à mon avis vient avec le troisième exemple. Gemini n'en propose pas de transcription complète, et génère, après quelques lignes, un message indiquant que le texte est illisible au-delà d'un certain point. Cela permet à Dan Cohen d'en conclure: "&lt;em&gt;Gemini does the right thing here: rather than venture a guess like a sycophantic chatbot, it is candid when it can’t interpret a section of the letter.&lt;/em&gt;" Personnellement, je m'étouffe en lisant ça, vu les erreurs déjà notées dans les deux exemples précédents. Au contraire de ce qu'affirme Dan Cohen, il n'y a pas de candeur ici, mais plutôt une effet pervers de ce que j'imagine être un calibrage du modèle en fonction de son taux de perplexité. Dans les deux premiers exemples, on peut imaginer que la perplexité du modèle face à certains passages difficiles conduit à la génération d'une note et/ou d'un insert entre crochets, mais n'empêche pas la génération d'une transcription fausse. Elle passe d'autant plus inaperçue que les explications générées en notes sonnent bien, même si elles sont fausses. On n'a donc pas affaire à un robot candide, mais à un chatbot arnaqueur, un presti-générateur, qui trouve une porte de sortie lorsque la situation est trop grosse pour une feinte subtile. Et à mon avis il serait vraiment temps que les utilisateurs de ces logiciel intègrent cette réalité, en ayant la main d'autant moins légère quand ils contrôlent ce que génèrent ces outils.&lt;/p&gt;
&lt;p&gt;Je n'ai pas encore lu le &lt;a href="https://generativehistory.substack.com/p/gemini-3-solves-handwriting-recognition"&gt;billet&lt;/a&gt; de Mark Humphries que je mentionnais tout au début, mais j'aurais peut-être l'occasion de revenir encore sur le sujet. A vrai dire, ce que je trouve vraiment vraiment dommage avec ces publications, issues du monde académique, qui contribuent à alimenter l'hystérie autour de l'IA générative, c'est qu'elle me donne l'impression que décidément ce n'est même pas de la part de la communauté scientifique que viendra le Salut. En tant que citoyenne et jeune chercheuse, cela m'inquiète beaucoup.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT: 2025-12-01: Petites corrections et ajout de notes d'une note supplémentaire en bas de page.&lt;/em&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:precision_inedit"&gt;
&lt;p&gt;Je donne cette précision sur l'édition des biographies et des correspondances car elle me semble importante: Dan Cohen n'a pas pris des documents dont on est sûr qu'ils soient inédits. Etant donné que les modèles d'IA générative sont entraînés à partir de tout ce qui peut être trouvé sur le Web, cela veut dire que ces lettres ont peut-être d'une manière ou d'une autre, fait partie des lots utilisés pour l'entraînement. Par exemple, &lt;a href="https://foinse.ucc.ie/en/records/IE/BL/PP/BP/1/A/1/1/51?utm_source=dancohen&amp;amp;utm_medium=email&amp;amp;utm_campaign=the-writing-is-on-the-wall-for-handwriting-recognition"&gt;sur le site&lt;/a&gt; des Archives du University College of Cork, d'où est tirée la numérisation de la lettre de Boole, on trouve le texte suivant dans le champ description: "&lt;em&gt;Boole in Cork to Maryann. He is in a very depressed mood, life has become monotonous with only his work adding interest to the day. He enjoys playing the piano but 'it would be better with someone else to listen and to be listened to'. He is also very annoyed by [Cropers] dedicating his book to him without first asking for permission - 'I cannot help feeling that he has taken a great liberty' - and speaks in strong terms of [Cropers] 'pretensions to high morality'. He invites and urges Maryann to visit him as soon as their mother's health would allow. He feels the climate would do her good.&lt;/em&gt;" Ce sont des éléments de contexte qui peuvent aider, y compris un modèle, au moment de transcrire. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025/#fnref:precision_inedit" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:standard_layout"&gt;
&lt;p&gt;Je ne dis pas "mise en page standard", parce que le phénomène qui est illustré par le troisième exemple, le fait de réécrire sur la même feuille après l'avoir tournée à 90°, correspond à une pratique qu'on retrouve au moins jusqu'au milieu du XXe siècle. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025/#fnref:standard_layout" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:lisible"&gt;
&lt;p&gt;Par lisible, je veux dire qu'on n'a pas besoin de savoir quelle était la phrase de départ pour comprendre ce qu'on aurait du lire dans les erreurs. J'admets par contre qu'en fonction de la familiarité avec le texte ou de la langue ou de la nature des erreurs, cette lisibilité peut varier. Si jamais vous trouvez quand même cette phrase illisible, il faut la lire comme ceci: "the hardest problem in digital humanities has finally been solved". Il y avait 1 inversion de lettres dans "&lt;em&gt;digital&lt;/em&gt;", une lettre manquante dans "&lt;em&gt;humanities&lt;/em&gt;", une lettre substituée par une autre dans "&lt;em&gt;finally&lt;/em&gt;", un lettre en trop dans "&lt;em&gt;been&lt;/em&gt;" et une séparation inappropriée dans "&lt;em&gt;solved&lt;/em&gt;". &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025/#fnref:lisible" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:ordre_lignes"&gt;
&lt;p&gt;Je développe très rapidement sur le point de la mise en page. Dans la transcription de Gemini, il y a des compléments d'informations qui suggèrent que le modèle a bien identifié à quelle page correspond telle ou telle partie du texte. Dans la transcription de Transkribus, ce n'est pas le cas, mais je pense que c'est parce que Dan Cohen a seulement utilisé la page de test de modèles de transcription de Transkribus. S'il avait utilisé la version complète de Transkribus, je suis sûre que le modèle aurait aussi parfaitement identifié la mise en page en double page. Pour ce qui concerne la transcription ligne par ligne, on n'a plus cette information dans la transcription de Gemini, qui génère le texte en continu. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025/#fnref:ordre_lignes" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:precision_error_tkb"&gt;
&lt;p&gt;Parmi les erreurs de Transkribus, on peut aussi noter l'utilisation d'un "&lt;a href="https://www.compart.com/en/unicode/U+0432"&gt;в&lt;/a&gt;" (le v cyrillique) pour transcrire le "B" de la côte du document, et d'un "&lt;a href="https://www.compart.com/en/unicode/U+0440"&gt;р&lt;/a&gt;" (le r cyrillique) pour transcrire le "P" qui suit. Ce sont des erreurs qui nous échappent quand on fait un contrôle visuel rapide, qui ne gêne pas la lecture par les humains, mais qui font baisser la précision calculée automatiquement puisque qu'un в n'est pas un B et un р n'est pas un P, ni d'ailleurs un p (see what I did here?). &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025/#fnref:precision_error_tkb" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:occasion_by"&gt;
&lt;p&gt;Transkribus l'avait transcrit "&lt;em&gt;occasion by&lt;/em&gt;". &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025/#fnref:occasion_by" title="Jump back to footnote 6 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>evaluation</category><category>French blog posts</category><category>Generative AI</category><category>HTR</category><category>Large Language Models</category><category>literature review</category><guid>https://alix-tz.github.io/phd/posts/025/</guid><pubDate>Fri, 28 Nov 2025 21:50:54 GMT</pubDate></item><item><title>016 - Text Recognition, Large Models and Expectations</title><link>https://alix-tz.github.io/phd/posts/016/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;Since the boom around ChatGPT almost a year ago, I've heard several people wondering if "tools like ChatGPT" were more efficient than HTR models trained with &lt;a href="https://kraken.re"&gt;Kraken&lt;/a&gt; and the like. The glimmer of hope in their eyes was most likely lit by their own struggle to set successful and/or efficient HTR campaigns with more traditional tools. The capacity of Large Language Models (LLMs) to reformulate a text&lt;sup id="fnref:spina"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/016/#fn:spina"&gt;1&lt;/a&gt;&lt;/sup&gt; or, more specifically, of Large Multimodal Models (LMMs) to generate text based on a visual input may indeed lead people to believe that HTR technologies built on &lt;a href="https://poloclub.github.io/cnn-explainer/"&gt;CNNs&lt;/a&gt; are on the verge of being flipped upside-down.&lt;sup id="fnref:multimodal_turn"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/016/#fn:multimodal_turn"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Annika Rockenberger recently conducted a series of small experiments on the matter and wrote &lt;a href="https://greflinger.hypotheses.org/739"&gt;an interesting blog post&lt;/a&gt; about it. Let's summarize it!&lt;/p&gt;
&lt;p&gt;She signed up for a premium subscription (25$/mo) to be able to chat with &lt;a href="https://openai.com/gpt-4"&gt;GPT4&lt;/a&gt;, which allows users to upload images. Then she submitted printed or handwritten documents she would normally transcribe with &lt;a href="https://readcoop.eu/transkribus"&gt;Transkribus&lt;/a&gt; and assessed the results. She found that GPT4 was fairly good on ancient print (German Fraktur) and that it was even able to follow transcription guidelines if provided with an example. However on a letter bearing handwritten cursive, the model completely hallucinated the content and attempted a transcription in the wrong language. This didn't change when she provided more context on the document. Rockenberger concludes that there is a potential for using ChatGPT for HTR but that the capacity of scaling it up is completely unsure and that learning how to provide good prompts to get the appropriate results is a challenge. I would also add that in the end, Rockenberger paid 25$ to get 10 lines of raw text, whereas with software like Transkribus or eScriptorium, she would also get a standard structured output.&lt;/p&gt;
&lt;p&gt;So, in other words, after reading Rockenberger's post, one can conclude that GPT4 (or, better, similar free and open source models) does have a potential for "quick and dirty-ish" OCR. However, I would argue that users tempted by this strategy might still miss an important point: even LMM-based tools will requires a little bit of organization and precision from the users. This, I find, often lacks in unsuccessful HTR campaigns. LMMs could generate a good output, but you will likely have to pay a counterpart one way or the other(s): with lower text recognition quality, with hallucinated text content, with impoverished non-structured output, with premium fees, etc.&lt;/p&gt;
&lt;p&gt;Earlier this year, an article proposed by &lt;a href="https://arxiv.org/abs/2305.07895"&gt;Liu et al. (2023)&lt;/a&gt;, "On the Hidden Mystery of OCR in Large Multimodal Models", explored almost exactly the same topic but in a more comprehensive way. Their article presents an extensive survey of how well several Large &lt;a href="https://en.wikipedia.org/wiki/Multimodal_learning"&gt;Multimodal&lt;/a&gt; Models (LMMs) performed on "zero-shot" tasks.&lt;/p&gt;
&lt;p&gt;Zero-shot refers to the act of requesting an output from an LLM or a LMM without training it for this task in particular. It is very similar to Rockenberger's first attempt with GPT4, when she uploaded the image of a printed document and asked for its transcription. In such a case, she relied on the capacity of the model to transfer its knowledge to the specific tasks of Text Recognition, on a specific type of documents (historical printed text).&lt;/p&gt;
&lt;p&gt;Other terms are often associated with "zero-shot:" "one-shot" and "few-shot". One-shot is equivalent to Rockenberger's second attempt: when she showed GPT4 an example of the output she expected on the 10 first lines of the documents, and requested that the model copied her strategy to generate the transcription of the 10 next lines. Few-shot would mean showing several pages and several expected output to the model before asking for the transcription of a new document.&lt;sup id="fnref:shot-definition"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/016/#fn:shot-definition"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The paper focused on currently available LMMs representing five different approaches for training LMMs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://huggingface.co/docs/transformers/model_doc/blip-2"&gt;BLIP-2&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2204.14198"&gt;Flamingo&lt;/a&gt;/&lt;a href="https://laion.ai/blog/open-flamingo/"&gt;Open-Flamingo&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://llava-vl.github.io/"&gt;LLaVa&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://minigpt-4.github.io/"&gt;miniGPT4&lt;/a&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;a href="https://huggingface.co/spaces/MAGAer13/mPLUG-Owl"&gt;mPLUG-Owl&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They evaluated the models on 4 tasks: text recognition, text-based visual question answering, key information extraction and handwritten mathematical expression recognition. Here are a few examples of what these tasks entail, as illustrated in the original article (on the images, P stands for Prediction and GT for Ground Truth):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;Task&lt;/th&gt;
&lt;th style="text-align: center;"&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Text Recognition&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;img alt="Examples of failed Text Recognition" src="https://alix-tz.github.io/phd/images/LLM_text_recogntion.png" title="Four images contained printed of handwritten words along with the ground truth (expected transcription) and the prediction generated by the models. For example, the model predicted 'chocolate' when the expected transcription was 'choco'"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Visual Question Answering&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;img alt="Examples of failed Visual Question Answering" src="https://alix-tz.github.io/phd/images/LLM_textVQA.png" title="Two images of real-life views along with a question used as a prompt, the expected answer and the predicted answer. For example, when asked 'What is the yellow number?' on the image of an airport luggage retrieval conveyor belt showing a clear '7' in yellow in the background, the model provided the following answer: 'The yellow number on the luggage trolley is 32"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;*Key Information Extraction&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;img alt="Examples of failed Key Information Extraction" src="https://alix-tz.github.io/phd/images/LLM_keyinfoextraction.png" title="Three images of real-life documents or textual information, along side with a question used as a prompt for the model, the expected answer and the predicted answer. For example, when asked 'what is the Sample No information in the input?', the model is expected to answer '1194-90' but answers 'The sample number is 33340'"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Handwritten Mathematical Expression Recognition&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;img alt="Examples of failed Handwritten Mathematical Expression Recognition" src="https://alix-tz.github.io/phd/images/LLM_HMExpr.png" title="Four example of failed attempts from the LMM to predict a LaTeX representation of handwritten mathematical expression: the numbers are wrong and/or the mathematical structure of the equations is made up"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- "Four images contained printed of handwritten words along with the ground truth (expected transcription) and the prediction generated by the models. For example, the model predicted 'chocolate' when the expected transcription was 'choco'.") | --&gt;
&lt;!--  "Two images of real-life views along with a question used as a prompt, the expected answer and the predicted answer. For example, when asked 'What is the yellow number?' on the image of an airport luggage retrieval conveyor belt showing a clear '7' in yellow in the background, the model provided the following answer: 'The yellow number on the luggage trolley is 32") | --&gt;
&lt;!--  "Three images of real-life documents or textual information, along side with a question used as a prompt for the model, the expected answer and the predicted answer. For example, when asked 'what is the Sample No information in the input?', the model is expected to answer '1194-90' but answers 'The sample number is 33340'") | --&gt;
&lt;!--  "Four example of failed attempts from the LMM to predict a LaTeX representation of handwritten mathematical expression: the numbers are wrong and/or the mathematical structure of the equations is made up") | --&gt;

&lt;p&gt;For each task, they used several datasets presenting different challenges. For each of these datasets and tasks, they retrieved the scores of the state-of-the-art (sota) for supervised methods and used them as a baseline. For example, for text recognition on the &lt;a href="https://fki.tic.heia-fr.ch/databases/iam-handwriting-database"&gt;IAM dataset&lt;/a&gt;, the sota method of AttentionHTR&lt;sup id="fnref:attentionhtr_ref"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/016/#fn:attentionhtr_ref"&gt;4&lt;/a&gt;&lt;/sup&gt; reaches a word accuracy of 91.24%.&lt;sup id="fnref:remark_wer"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/016/#fn:remark_wer"&gt;5&lt;/a&gt;&lt;/sup&gt; In comparison, Liu et al provide the following scores for the tested LMM on this dataset:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;test LMM&lt;/th&gt;
&lt;th style="text-align: center;"&gt;Score on IAM&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;BLIP-2 OPT&lt;sub&gt;6.7b&lt;/sub&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;38.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;BLIP-2 FlanT5&lt;sub&gt;XXL&lt;/sub&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;40.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;OpenFlamingo&lt;/td&gt;
&lt;td style="text-align: center;"&gt;45.53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;LLaVa&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;em&gt;50.40&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;MiniGPT4&lt;/td&gt;
&lt;td style="text-align: center;"&gt;28.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;mPLUG-Owl&lt;/td&gt;
&lt;td style="text-align: center;"&gt;42.53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;---------------&lt;/td&gt;
&lt;td style="text-align: center;"&gt;-----&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Supervised SOTA&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;strong&gt;91.24&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The illustrations provided by the article are all of failed attempts, but it corresponds to the overall impression conveyed by the results of the experiments. Indeed, compared to the state-of-the-art supervised methods, zero-shot tasks prompted to LMMs yield results largely outperformed, similar to what is visible in the case of text recognition on the IAM dataset. The only exception is BLIP-2 on a Text Recognition task on a dataset of artistic text (&lt;a href="https://github.com/xdxie/WordArt#wordart-dataset"&gt;WordArt&lt;/a&gt;) which is more challenging. The authors consider that this is a sign that LMMs have a promising potential for visually complex texts.&lt;/p&gt;
&lt;p&gt;A very important section of their paper is their remarks on the relationship between LMMs and semantics. Submitting non-word images to the LMMs, they find that the LMMs systematically over-correct the prediction and suggest real-words as an answer. Traditional text recognition approaches, on the other hand, are much less sensitive to the notion of likelihood for the words to recognize. Similarly, the need for semantics interferes with the LMMs' output, and they tend to more easily recognize common words and make up additional letters ("choco" is read as "chocolate"). Lastly, LMMs are insensitive to word length: they are unable to count how many letters are in the image of a word. These results are similar to what Rockenberger experienced with the handwritten letter: the model hallucinated words to compose a semantically plausible letter. But using the wrong date, the wrong names, and the wrong language.&lt;/p&gt;
&lt;p&gt;Liu et al conclude their paper reminding us that they experimented with the capacities of the models in the context of zero-shot prompts, whereas there are already successful attempts at fine-tuning LLMs and LMMs on specialized tasks, such as medical prediction. In fact, I think there already exist such attempts in the context of HTR as well: it seems to be the ambition of a model like Transkribus' Text Titan, released at the beginning of the Summer. It is based on a &lt;a href="https://youtu.be/zxQyTK8quyY?feature=shared"&gt;Transformer&lt;/a&gt; coupled with an LLM. Unfortunately, I wasn't able to find more information on this model, aside from the community-oriented communications released by Transkribus on their website (&lt;a href="https://readcoop.eu/introducing-transkribus-super-models-get-access-to-the-text-titan-i/"&gt;here&lt;/a&gt; and &lt;a href="https://help.transkribus.org/super-models"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:spina"&gt;
&lt;p&gt;In stead of a multimodal approach, Salvatore Spina explored the possibility to use a LLM-based tool like ChatGPT3 to post-process the result of HTR and correct the text. See: Spina, S. (2023). &lt;em&gt;Artificial Intelligence in archival and historical scholarship workflow: HTS and ChatGPT&lt;/em&gt; (arXiv:2308.02044). arXiv. &lt;a href="https://doi.org/10.48550/arXiv.2308.02044"&gt;arXiv.2308.02044&lt;/a&gt;. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/016/#fnref:spina" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:multimodal_turn"&gt;
&lt;p&gt;Multimodality is presented by some researchers of the Digital Humanities community as a real epistemological turn for the field. See for example: Smits, T., &amp;amp; Wevers, M. (2023). &lt;em&gt;A multimodal turn in Digital Humanities. Using contrastive machine learning models to explore, enrich, and analyze digital visual historical collections&lt;/em&gt;. Digital Scholarship in the Humanities, fqad008. &lt;a href="https://doi.org/10.1093/llc/fqad008"&gt;doi: 10.1093/llc/fqad008&lt;/a&gt; ; or Impett, L., &amp;amp; Offert, F. (2023). &lt;em&gt;There Is a Digital Art History&lt;/em&gt; (arXiv:2308.07464). arXiv. &lt;a href="https://doi.org/10.48550/arXiv.2308.07464"&gt;arXiv.2308.07464&lt;/a&gt;. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/016/#fnref:multimodal_turn" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:shot-definition"&gt;
&lt;p&gt;There are a few videos offering more or less detailed explanations on these expressions &lt;a href="https://www.youtube.com/watch?v=E6X1Ufhxtf0"&gt;in the context of prompting an LLM&lt;/a&gt;. However, this is not specific to LLM, it is often used in the context of &lt;a href="https://huggingface.co/tasks/zero-shot-classification"&gt;classification&lt;/a&gt; or &lt;a href="https://joeddav.github.io/blog/2020/05/29/ZSL.html"&gt;NLP&lt;/a&gt; tasks for example. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/016/#fnref:shot-definition" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:attentionhtr_ref"&gt;
&lt;p&gt;Kass, D., &amp;amp; Vats, E. (2022). &lt;em&gt;AttentionHTR: Handwritten Text Recognition Based on Attention Encoder-Decoder Networks&lt;/em&gt; (arXiv:2201.09390). arXiv. &lt;a href="https://doi.org/10.48550/arXiv.2201.09390"&gt;arXiv.2201.09390&lt;/a&gt;. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/016/#fnref:attentionhtr_ref" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:remark_wer"&gt;
&lt;p&gt;In this case, the WER is used as a baseline to compare different approaches. However, in general, it is not a good idea to only take into account Word accuracy to understand a model's performance in real life. This is something I discussed in &lt;a href="https://alix-tz.github.io/phd/posts/012/"&gt;this&lt;/a&gt; post. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/016/#fnref:remark_wer" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>Large Language Models</category><category>OCR</category><guid>https://alix-tz.github.io/phd/posts/016/</guid><pubDate>Tue, 28 Nov 2023 10:28:15 GMT</pubDate></item></channel></rss>