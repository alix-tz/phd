<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="https://alix-tz.github.io/phd/assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>A research (b)log (Posts about literature review)</title><link>https://alix-tz.github.io/phd/</link><description></description><atom:link href="https://alix-tz.github.io/phd/categories/literature-review.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="https://alix-tz.github.io/phd/"&gt;Alix Chagué&lt;/a&gt; CC-BY</copyright><lastBuildDate>Fri, 19 Dec 2025 02:13:46 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>025 - A Perfect Job is the New Very Good Job</title><link>https://alix-tz.github.io/phd/posts/025/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;blockquote&gt;
&lt;p&gt;A little disclaimer for once, because I usually prefer to praise if I name people. I do not know Dan Cohen nor his work, my criticism of his article is not directed against him personally, but rather it takes his text as one example among many of the kind, that develop the same type of discourse and contain the same type of flaws.&lt;/p&gt;
&lt;p&gt;A second disclaimer: I moved the original French version of this post here: &lt;a href="https://alix-tz.github.io/phd/posts/025-fr"&gt;posts/025-fr.md&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Earlier this week, my colleague Louis-Olivier Brassard asked me for my opinion on the &lt;a href="https://newsletter.dancohen.org/archive/the-writing-is-on-the-wall-for-handwriting-recognition/"&gt;latest post&lt;/a&gt; by Dan Cohen, which he titled "&lt;em&gt;The Writing Is on the Wall for Handwriting Recognition&lt;/em&gt;", adding a subtitle that sets the tone: "&lt;em&gt;One of the hardest problems in digital humanities has finally been solved&lt;/em&gt;". I wanted to make my critical reading a bit more public, so I'm turning it into a blog post&lt;!--, in French for once--&gt;.&lt;/p&gt;
&lt;p&gt;I carefully read this article because the subject is of interest to me (obviously), but I must admit that I usually start this kind of reading with a negative &lt;em&gt;a priori&lt;/em&gt;. This is the treatment I save for all those posts, whether on blogs or social media, that announce left and right that generative AI has revolutionized this or that -- this and that generally being problems that have occupied researchers and engineers for years, and which gave rise to sometimes heated or even unsolvable debates. All these posts contribute to fueling the hype around generative AI and undermining our already quite worn collective ability to develop critical thinking about it.&lt;/p&gt;
&lt;p&gt;Dan Cohen's post follows the release of version 3 of Gemini, Google's generative AI model, publicized as Google's "most intelligent model yet". Like every time a new model of this type is released, several users share the results of their "experiments" with these models. Dan Cohen is not the only one; for example, Mark Humphries also posted &lt;a href="https://generativehistory.substack.com/p/gemini-3-solves-handwriting-recognition"&gt;a post on the subject&lt;/a&gt; on the same day, soberly titled "&lt;em&gt;Gemini 3 Solves Handwriting Recognition and it’s a Bitter Lesson&lt;/em&gt;". I saw these two posts widely shared on BlueSky, praised by researchers whom I consider to hold positions of authority in the field of automatic transcription. After reading Dan Cohen's post, I found myself quite annoyed by these shares: I'm not convinced that the text was well read by those who shared it on BlueSky.&lt;/p&gt;
&lt;p&gt;In my opinion, the problem with Dan Cohen's post is twofold: 1) he develops a universal discourse on a tool that he has only tested on a minimal selection of examples that say almost nothing about the problems encountered by users of automatic transcription on old documents, 2) his demonstration relies on fallacious arguments.&lt;/p&gt;
&lt;h3&gt;A matter of scientific rigor&lt;/h3&gt;
&lt;p&gt;About the first point: Dan Cohen uses three examples that are not at all representative of the challenges of automatic transcription. Right from the start, this would justify a footnote to his subtitle: he says "&lt;em&gt;one of the hardest problems in digital humanities has finally been solved&lt;/em&gt;", I add "&lt;em&gt;as far as it concerns epistolary documents written in English during the first half of the 19th century by personalities whose biographies have been written, or whose correspondence has already been edited&lt;/em&gt;"&lt;sup id="fnref:precision_inedit"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025/#fn:precision_inedit"&gt;1&lt;/a&gt;&lt;/sup&gt; because that's what he tested. That already reduces the scope of his results quite a bit, doesn't it? Moreover, given that the model fails to transcribe the third example, we could even add that this only concerns documents with a simple layout.&lt;sup id="fnref:standard_layout"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025/#fn:standard_layout"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;This first point is really problematic because this post is a text published by a person who has scientific authority and should therefore demonstrate scientific rigor, even if we are only talking about a newsletter and not an edited article or book. Following this scientific rigor, I would expect us to limit ourselves to drawing conclusions about what has been successfully demonstrated instead of Doom propheting with flashy (sub)titles. One can be convinced that Gemini is capable of successfully handling many other cases than those presented by Dan Cohen, but that is a matter of belief, not scientific demonstration. I think this is a topic that needs to be discussed more broadly, in a context where AI is messianically served to us in all forms of dishes, but Marcello Vitali-Rosati talks about it well in &lt;a href="https://blog.sens-public.org/marcellovitalirosati/2025-11-htr.html"&gt;his latest post&lt;/a&gt; or, from another angle and outside the uses by the academic world, there is the recent work of &lt;a href="https://www.polytechnique-insights.com/tribunes/digital/comment-se-proteger-du-syndrome-de-stockholm-technologique-face-a-lia/"&gt;Hamilton Mann&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It happens that the day Louis-Olivier asked me to read Dan Cohen's text, I had also read that of &lt;a href="https://digitalorientalist.com/2025/11/25/teaching-bengali-digital-texts-to-anglophone-undergraduates-what-voyant-reveals-about-the-infrastructural-bias-of-dh-tools/"&gt;Sunayani Bhattacharya&lt;/a&gt; who trained her students at Saint Mary's College of California in text analysis with &lt;a href="https://voyant-tools.org/"&gt;Voyant Tools&lt;/a&gt; and who also evoked automatic transcription in passing in her post. She explains that, with the objective of offering an opening to the Global South to her students, she had them work on texts in Bengali (even though none of them can speak or read Bengali). I find the exercise interesting and promising as she presents it. After developing in her students a familiarity with what Bengali in properly edited press texts look like in Voyant Tools, she showed them what you get when you try to run Voyant Tools on texts directly taken from OCR software. These texts contain a lot of noise and sometimes do not even use the correct character sets. This allows her to give her students a very concrete example of the limitations of software infrastructures when it comes to processing texts in Indic languages. She concludes by reiterating the usefulness of giving students a better idea of what on-the-ground anglophone biases look like in technology. In a text like the one I discuss in this post, this anglophone bias (and I would even add modernist) is blatant.&lt;/p&gt;
&lt;h3&gt;A shaky demonstration&lt;/h3&gt;
&lt;p&gt;Now, regarding the second point, it requires taking a closer look at what Dan Cohen tells us and the examples he gives. There are inaccuracies that need to be pointed out, but also excerpts that do not correspond to the statements made in the post.&lt;/p&gt;
&lt;p&gt;Let's start with an inacuracy that actually regards the question of model accuracy. I have already discussed this in &lt;a href="https://alix-tz.github.io/phd/posts/012/"&gt;a previous post&lt;/a&gt; because it seems to me that this is one of the topics where researchers are most lazy: what accuracy are we talking about, and what are the limits of these accuracy measures? Dan Cohen states that "&lt;em&gt;the best HTR software struggles to reach 80% accuracy&lt;/em&gt;". As he clarifies that this means 2 wrong words out of 10 words, we already see that he is talking about word error rate and not character error rate. Such an error rate, on its own, says nothing about the readability of the text since a single error is enough for a to be counted as wrong. In a sentence like "&lt;em&gt;the hardest problem in digtial humaities has finolly beeen sol ved&lt;/em&gt;", one word out of two contains a mistake, yet it seems to me that the sentence is perfectly readable.&lt;sup id="fnref:lisible"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025/#fn:lisible"&gt;3&lt;/a&gt;&lt;/sup&gt; To put things into perspective, the character accuracy rate in this same sentence is 90.77% (according to software like &lt;a href="https://huggingface.co/spaces/lterriel/kami-app"&gt;KaMI&lt;/a&gt;). In addition to this initial inaccuracy, Dan Cohen's statement about the difficulties of traditional software seems false to me. I do not see on what source he bases himself. For documents like those he tests, we are well above 80% accuracy, even at the word level, and this with several models and several software using RCNNs or Transformers.&lt;/p&gt;
&lt;p&gt;Since this initial statement surprised me, I wanted to look closer at Transkribus' output to see if it really did this many errors. Of course, there are errors in Transkribus' transcriptions. Yet, when we look at the source document, we see that some of these errors are understandable in a zero-shot context. When Boole draws two "l"s in a row, his second "l" looks like an "e" with a very very small loop. This explains why Transkribus' prediction contains errors on "&lt;em&gt;tell&lt;/em&gt;" (read as "&lt;em&gt;tele&lt;/em&gt;") on the left page, and "&lt;em&gt;All&lt;/em&gt;" (read as "&lt;em&gt;Ale&lt;/em&gt;") on the right page. To find out the real extent of Transkribus' errors, I made my own transcription of the double page tested by Dan Cohen, line by line (following the line order taken from the segmentation in Transkribus, and helping myself a bit with the reading proposed by Gemini&lt;sup id="fnref:ordre_lignes"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025/#fn:ordre_lignes"&gt;4&lt;/a&gt;&lt;/sup&gt;). When I calculate the accuracy rate on this excerpt, I get a character accuracy of about 95% and a word accuracy of 88%.&lt;sup id="fnref:precision_error_tkb"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025/#fn:precision_error_tkb"&gt;5&lt;/a&gt;&lt;/sup&gt; So there is plenty of room for improvement, but we are not in a catastrophic situation as the preamble suggests.&lt;/p&gt;
&lt;p&gt;If we now turn to the transcription generated by Gemini, we can see that there are actually some errors as well, whereas Dan Cohen is telling us that "&lt;em&gt;Gemini transcribed the letter perfectly&lt;/em&gt;". For example, Gemini transcribes, on the right page, "&lt;em&gt;occasionally by&lt;/em&gt;",&lt;sup id="fnref:occasion_by"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025/#fn:occasion_by"&gt;6&lt;/a&gt;&lt;/sup&gt; generating as additional precision in a notes section that "&lt;em&gt;On the right page (line 8), the handwriting becomes very scribbled. It appears to say 'take a long walk occasionally try &amp;amp; once or twice...' or possibly 'occasionally by &amp;amp; once or twice...'.&lt;/em&gt;" Gemini fails here to propose reading a hyphenation that makes sense and prefers to add a word in its transcription. The problem is not that Gemini did not make a perfect transcription of course, but rather that Dan Cohen states it without noting this error.&lt;/p&gt;
&lt;p&gt;We have the same issue in the second example, where Gemini formats the word "transmitted" to indicate that it is crossed out in the source when it is not. The text generated by Gemini leaves no doubt about the look of the text in the source, and invents an intention on the part of the author: "&lt;em&gt;In the second line of the body, the word 'transmitted' is crossed out in the original text, but the sentence is grammatically incomplete without it (or a similar verb). It is likely the author meant to replace it to avoid repetition with the word 'transmitting' appearing a few lines later but forgot to insert the new word.&lt;/em&gt;" Whereas this error was easier to spot, Dan Cohen once again tells us: "&lt;em&gt;Another perfect job.&lt;/em&gt;"&lt;/p&gt;
&lt;p&gt;Then comes the third example. Gemini does not offer a complete transcription of this one, and after a few lines, generates a message indicating that the text is illegible beyond a certain point. This allows Dan Cohen to conclude: "&lt;em&gt;Gemini does the right thing here: rather than venture a guess like a sycophantic chatbot, it is candid when it can’t interpret a section of the letter.&lt;/em&gt;" I personally choke reading that, given the errors already noted in the two previous examples. Contrary to what Dan Cohen claims, there is no candor here, but rather a perverse effect of what I imagine is a calibration of the model based on its perplexity rate. In the first two examples, we can imagine that the model's perplexity regarding certain difficult passages leads to the generation of a note and/or an insert in brackets, but does not prevent the generation of a false transcription. It goes unnoticed all the more because the explanations generated in notes sound good, even if they are false. We are not dealing with a candid robot, but rather with a scammer chatbot, a presti-generator, who finds an escape route when the situation is too big for a subtle feint. And in my opinion, it would really be time for users of these software to integrate this reality, taking an even closer look when they control what these tools generate.&lt;/p&gt;
&lt;p&gt;I haven't yet read &lt;a href="https://generativehistory.substack.com/p/gemini-3-solves-handwriting-recognition"&gt;Mark Humphries' post&lt;/a&gt; that I mentioned at the beginning, but I might come back to the subject in the future. To be honest, what I find really really unfortunate about these publications, coming from the academic world, which help to fuel the hysteria around generative AI, is that it gives me the impression that decisively it will not be from the scientific community that Salvation will come. As a citizen and a young researcher, this worries me a lot.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT: 2025-12-01: Minor corrections and addition of another footnote.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT: 2025-12-04: Translated the post to English (with the help of Copilot) and moved the French version to another path: &lt;a href="https://alix-tz.github.io/phd/posts/025-fr"&gt;posts/025-fr.md&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:precision_inedit"&gt;
&lt;p&gt;I give this precision about the edition of biographies and correspondences because it is important: Dan Cohen did not take documents that we are sure are unpublished. Given that generative AI models are trained from everything that can be found on the Web, this means that these letters may have, in one way or another, been part of the batches used for training. For example, &lt;a href="https://foinse.ucc.ie/en/records/IE/BL/PP/BP/1/A/1/1/51?utm_source=dancohen&amp;amp;utm_medium=email&amp;amp;utm_campaign=the-writing-is-on-the-wall-for-handwriting-recognition"&gt;on the website&lt;/a&gt; of the Archives of University College Cork, from which the digitization of Boole's letter is taken, we find the following text in the description field: "&lt;em&gt;Boole in Cork to Maryann. He is in a very depressed mood, life has become monotonous with only his work adding interest to the day. He enjoys playing the piano but 'it would be better with someone else to listen and to be listened to'. He is also very annoyed by [Cropers] dedicating his book to him without first asking for permission - 'I cannot help feeling that he has taken a great liberty' - and speaks in strong terms of [Cropers] 'pretensions to high morality'. He invites and urges Maryann to visit him as soon as their mother's health would allow. He feels the climate would do her good.&lt;/em&gt;" These are contextual elements that can help a model when transcribing. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025/#fnref:precision_inedit" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:standard_layout"&gt;
&lt;p&gt;I purposefully use the term "simple layout" rather than "standard layout" because the phenomenon illustrated by the third example, the rewriting on the same sheet after having turned it 90°, corresponds to a practice that can be found at least until the mid-20th century. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025/#fnref:standard_layout" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:lisible"&gt;
&lt;p&gt;By readable, I mean that one does not need to know what the original sentence was to understand what we should have read in place of the errors. I admit however that depending on familiarity with the text or the language or the nature of the errors, this readability may vary. If you still find this sentence unreadable, it should be read as follows: "the hardest problem in digital humanities has finally been solved". There was 1 letter inversion in "&lt;em&gt;digital&lt;/em&gt;", one missing letter in "&lt;em&gt;humanities&lt;/em&gt;", one letter substituted by another in "&lt;em&gt;finally&lt;/em&gt;", one extra letter in "&lt;em&gt;been&lt;/em&gt;" and an inappropriate separation in "&lt;em&gt;solved&lt;/em&gt;". &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025/#fnref:lisible" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:ordre_lignes"&gt;
&lt;p&gt;I rapidly develop in the question of the layout. In Gemini's transcription, there are additional pieces of information that suggest that the model correctly identified which part of the text corresponds to which page. In Transkribus' transcription, this is not the case, but I think it's because Dan Cohen only used Transkribus' basic web page from testing models. If he had used the full version of Transkribus, I'm sure the software would have also perfectly identified the double-page layout. As for the line-by-line transcription, we no longer have this information in Gemini's transcription, which generates the text continuously. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025/#fnref:ordre_lignes" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:precision_error_tkb"&gt;
&lt;p&gt;Among the errors made by Transkribus, we can also note the use of a "&lt;a href="https://www.compart.com/en/unicode/U+0432"&gt;в&lt;/a&gt;" (the Cyrillic v) to transcribe the "B" in the margin of the document, and a "&lt;a href="https://www.compart.com/en/unicode/U+0440"&gt;р&lt;/a&gt;" (the Cyrillic r) to transcribe the "P" that follows. These are errors that escape us when we do a quick visual check, which do not hinder reading by humans, but which lower the accuracy calculated automatically since a в is not a B and a р is not a P, nor indeed a p (see what I did here?). &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025/#fnref:precision_error_tkb" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:occasion_by"&gt;
&lt;p&gt;Transkribus transcribed it as "&lt;em&gt;occasion by&lt;/em&gt;". &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025/#fnref:occasion_by" title="Jump back to footnote 6 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>evaluation</category><category>French blog posts</category><category>Generative AI</category><category>HTR</category><category>Large Language Models</category><category>literature review</category><guid>https://alix-tz.github.io/phd/posts/025/</guid><pubDate>Fri, 28 Nov 2025 21:50:54 GMT</pubDate></item><item><title>025 - A Perfect Job is the New Very Good Job (FR)</title><link>https://alix-tz.github.io/phd/posts/025-fr/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;blockquote&gt;
&lt;p&gt;A little disclaimer for once, because I usually prefer to praise if I name people. I do not know Dan Cohen nor his work, my criticism of his article is not directed against him personally, but rather it takes his text as one example among many of the kind, that develop the same type of discourse and contain the same type of flaws.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Plus tôt cette semaine, mon collègue Louis-Olivier Brassard m'a demandé mon avis sur le &lt;a href="https://newsletter.dancohen.org/archive/the-writing-is-on-the-wall-for-handwriting-recognition/"&gt;dernier billet&lt;/a&gt; posté par Dan Cohen, qu'il a intitulé "&lt;em&gt;The Writing Is on the Wall for Handwriting Recognition&lt;/em&gt;", ajoutant un sous-titre annonçant la couleur: "&lt;em&gt;One of the hardest problems in digital humanities has finally been solved&lt;/em&gt;". J'avais envie de rendre un peu plus public ma lecture critique, donc j'en tire un billet de blog, en français pour une fois.&lt;/p&gt;
&lt;p&gt;J'ai lu avec attention cet article car le sujet m'intéresse (forcément), mais je ne cache pas que je débute en général ce genre de lecture avec un a priori négatif. C'est le traitement que je réserve à tous ces postes, de blog ou sur les réseaux sociaux, qui annoncent à tour de bras que l'IA générative a révolutionné ceci ou cela -- ceci et cela étant généralement des problèmes qui ont occupé des chercheur-ses et ingénieur-es depuis des années, et qui donnent lieu à des débats parfois houleux voire insolvables. Tous ces billets contribuent à alimenter l'esbroufe de l'IA générative et à saper notre capacité collective déjà pas mal usée à développer une pensée critique à son endroit. &lt;!--J'essaie quand même d'être honnête et de faire attention à mes propres biais dans ce que j'en tire ci-dessous.--&gt;&lt;/p&gt;
&lt;p&gt;Le billet de Dan Cohen fait suite à la sortie de la version 3 de Gemini, le modèle d'IA générative de Google, publicisé comme le modèle de Google "le plus intelligent à date" ("&lt;em&gt;our most intelligent model yet&lt;/em&gt;" dit Google). Comme à chaque fois qu'un nouveau modèle de ce type sort, plusieurs utilisateurs partagent les résultats de leurs "expérimentations" avec ces modèles. Dan Cohen n'est pas le seul, par exemple Mark Humphries a aussi posté le même jour &lt;a href="https://generativehistory.substack.com/p/gemini-3-solves-handwriting-recognition"&gt;un billet sur le sujet&lt;/a&gt; intitulé sobrement "&lt;em&gt;Gemini 3 Solves Handwriting Recognition and it’s a Bitter Lesson&lt;/em&gt;". J'ai beaucoup vu ces deux billets relayés sur BlueSky, salués par des chercheurs que j'estime occuper des place d'autorité dans le domaine de la transcription automatique. Après avoir lu le billet de Dan Cohen, je me suis retrouvée assez agacée de ces relais: je ne suis pas convaincue que le texte ait été bien lu par ceux qui l'ont relayé sur BlueSky.&lt;/p&gt;
&lt;p&gt;A mon avis, le problème du billet que Dan Cohen est double: 1) il développe un discours universel sur un outil qu'il n'a testé que sur sélection minime d'exemples qui ne disent presque rien des problèmes que rencontrent les utilisateurs de la transcription automatique sur les documents anciens, 2) sa démonstration tient sur des arguments fallacieux.  &lt;/p&gt;
&lt;h3&gt;Un problème de rigueur scientifique&lt;/h3&gt;
&lt;p&gt;Sur le premier point tout d'abord. Dan Cohen utilise trois exemples qui ne sont pas du tout représentatifs des défis de la transcription automatique. D'emblée, cela justifierait une note de bas de page à son sous-titre: il dit "l'un des problèmes les plus difficiles des humanités numériques a enfin été résolu", j'ajoute "en ce qui concerne les documents épistollaires rédigés en anglais durant la première moitié du XIXe siècle par des personnalités dont des biographies ont été écrites, voire dont la correspondance à déjà été éditée"&lt;sup id="fnref:precision_inedit"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025-fr/#fn:precision_inedit"&gt;1&lt;/a&gt;&lt;/sup&gt; car c'est ce qu'il a testé. Ca réduit déjà pas mal la portée de ses résultats, non? D'ailleurs, étant donné que le modèle ne parvient pas à transcrire le troisème exemple, on pourrait même ajouter que cela ne concerne en plus que les documents dont la mise en page est simple.&lt;sup id="fnref:standard_layout"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025-fr/#fn:standard_layout"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Ce premier point est vraiment problématique parce qu'il s'agit d'un texte publié par une personne qui a une autorité scientifique et qui devrait donc faire preuve de rigueur scientifique, même si ce texte n'est qu'une newsletter et pas un article ou un ouvrage édité. J'attendrais de cette rigueur scientifique qu'on se limite à tirer des conclusions sur ce que l'on a réussi à démontrer au lieu de jouer les Cassandre avec des (sous-)titres tape-à-l'oeil. On peut avoir la conviction que Gemini est capable de traiter avec succès bien d'autres cas que ceux présentés par Dan Cohen, mais cela relève de la croyance, pas de la démonstration scientifique. Je pense que c'est un sujet qui doit être discuté plus largement, dans un contexte où l'IA nous est messianiquement servie à toutes les sauces, mais Marcello Vitali-Rosati en parle bien dans &lt;a href="https://blog.sens-public.org/marcellovitalirosati/2025-11-htr.html"&gt;son dernier billet&lt;/a&gt; ou encore, sous un autre angle et qui sort des usages par le monde académique, il y a le récent travail d'&lt;a href="https://www.polytechnique-insights.com/tribunes/digital/comment-se-proteger-du-syndrome-de-stockholm-technologique-face-a-lia/"&gt;Hamilton Mann&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Il se trouve que le jour où Louis-Olivier m'a demandé de lire le texte de Dan Cohen, j'avais aussi lu celui de &lt;a href="https://digitalorientalist.com/2025/11/25/teaching-bengali-digital-texts-to-anglophone-undergraduates-what-voyant-reveals-about-the-infrastructural-bias-of-dh-tools/"&gt;Sunayani Bhattacharya&lt;/a&gt; qui a formé ses élèves du Saint Mary’s College en Californie à l'analyse de texte avec &lt;a href="https://voyant-tools.org/"&gt;Voyant Tools&lt;/a&gt; et qui traite aussi de transcription automatique au détour de son billet. Elle explique que, dans l'optique de proposer une ouverture vers le Sud Global à ses étudiant-es, elle les a fait travailler sur des textes en Bengali (même si aucun ne sait parler ou lire le Bengali). Je trouve l'exercice intéressant et prometteur tel qu'elle le présente. Après avoir développé chez ses élèves une familiarité avec ce à quoi ressemble les textes de presse correctement édités dans Voyant Tools, elle leur a montré ce qu'on obtient quand on tente de faire tourner Voyant Tools sur des textes directement sortis d'un logiciel d'OCR. Ces textes contiennent énormément de bruit et parfois n'utilisent même pas les bons jeux de caractères. Cela lui permet de donner un exemple très concret à ses étudiant-es des limites des infrastructures logicielles dès qu'il s'agit de traiter de textes en langues indiennes. Elle conclut en redisant l'utlité de donner aux étudiant-es une meilleure idée de ce à quoi ressemblent les biais anglophones dans la technologie quand on est sur le terrain. Dans un texte comme celui dont je discute dans ce billet, ce biais anglophone (et j'ajouterai même moderniste) saute aux yeux.&lt;/p&gt;
&lt;h3&gt;Une démonstration bancale&lt;/h3&gt;
&lt;p&gt;Maintenant, concernant le deuxième point, il suppose de regarder d'un peu plus près ce que Dan Cohen nous dit et les exemples qu'il donne. Il y a des imprécisions qui doivent être relevées, mais aussi des extraits qui ne correspondent pas aux affirmations qui sont faites dans le billet.  &lt;/p&gt;
&lt;p&gt;Une imprécision qui commence justement par la question de la précision des modèles. J'en ai déjà parlé dans &lt;a href="https://alix-tz.github.io/phd/posts/012/"&gt;un précédent billet&lt;/a&gt; car il me semble que c'est l'un des sujets où les chercheurs font le plus preuve de paresse: de quelle précision on parle, et quelles sont les limites de ces mesures de précision ? Dan Cohen affirme que "&lt;em&gt;les meilleurs logiciel d'HTR ont du mal à atteindre 80% de précision&lt;/em&gt;". Comme il clarifie que cela signifie 2 mots faux tous les 10 mots, déjà on s'aperçoit qu'il nous parle de taux d'erreur au mot et non au caractère. Un tel taux d'erreur ne dit rien de la lisibilité du texte puisqu'une seule erreur suffit pour que le mot soit compté comme faux. Dans une phrase comme "&lt;em&gt;the hardest problem in digtial humaities has finolly beeen sol ved&lt;/em&gt;", un mot sur deux contient une faute, pourtant il me semble que la phrase est parfaitement lisible.&lt;sup id="fnref:lisible"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025-fr/#fn:lisible"&gt;3&lt;/a&gt;&lt;/sup&gt; Pour mettre les choses en perspective, le taux de précision au caractère dans cette phrase, lui, est de 90.77% (d'après un logiciel comme &lt;a href="https://huggingface.co/spaces/lterriel/kami-app"&gt;KaMI&lt;/a&gt;). En plus de cette imprécision de départ, l'affirmation de Dan Cohen sur les difficultés des logiciels traditionnels me semble fause. Je ne vois pas sur quelle source il se base. Pour des documents comme ceux qu'il teste, on est bien au-delà des 80% de précision, y compris au mot, et ce avec plusieurs modèles et plusieurs logiciels.&lt;/p&gt;
&lt;p&gt;Comme cette affirmation m'a surprise, j'ai voulu regarder si vraiment le modèle de Transkribus avait fait autant de fautes que ça. Bien sûr, il a fait des erreurs. Quand on regarde le document source, on voit que certaînes sont compréhensibles dans un contexte zero-shot: lorsque Boole trace deux "l" à la suite, son deuxième "l" ressemble à un "e" avec une boucle très très petite. C'est ce qui explique que la prédiction de Transkribus contient des erreurs sur "&lt;em&gt;tell&lt;/em&gt;" (lu "&lt;em&gt;tele&lt;/em&gt;") sur la page de gauche, et "&lt;em&gt;All&lt;/em&gt;" (lu "&lt;em&gt;Ale&lt;/em&gt;") sur la page de droite. Pour savoir quelle était vraiment l'ampleur des erreurs de Transkribus, j'ai fait ma propre transcription de la double page, ligne par ligne (en suivant l'ordre des lignes tiré de la segmentation dans Transkribus, et en m'aidant un peu de la lecture proposé par Gemini&lt;sup id="fnref:ordre_lignes"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025-fr/#fn:ordre_lignes"&gt;4&lt;/a&gt;&lt;/sup&gt;). Quand je calcule le taux de précision sur cet extrait, j'obtiens une précision au caractère d'environ 95% et une précision au mot de 88%.&lt;sup id="fnref:precision_error_tkb"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025-fr/#fn:precision_error_tkb"&gt;5&lt;/a&gt;&lt;/sup&gt; Largement perfectible donc, mais on n'est pas dans une situation catastrophique comme le laisse supposer le préambule.&lt;/p&gt;
&lt;p&gt;Maintenant, si on regarde la transcription de Gemini, on s'aperçoit qu'il y a en fait aussi des erreurs, alors que Dan Cohen nous dit "&lt;em&gt;Gemini transcribed the letter perfectly&lt;/em&gt;". Par exemple, Gemini transcrit, sur la page de droite, "&lt;em&gt;occasionally by&lt;/em&gt;",&lt;sup id="fnref:occasion_by"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/025-fr/#fn:occasion_by"&gt;6&lt;/a&gt;&lt;/sup&gt; en générant comme précision complémentaire dans une section de notes que "&lt;em&gt;On the right page (line 8), the handwriting becomes very scribbled. It appears to say 'take a long walk occasionally try &amp;amp; once or twice...' or possibly 'occasionally by &amp;amp; once or twice...'.&lt;/em&gt;" Donc Gemini, échoue ici à proposer de lire une césure qui fait pourtant sens et préfère ajouter un mot dans sa transcription. Le problème ce n'est pas que Gemini n'ai pas fait une transcription parfaite bien sûr, mais plutôt que Dan Cohen l'affirme sans relever cette erreur.&lt;/p&gt;
&lt;p&gt;On a le même problème dans le deuxième exemple, où Gemini met en forme le mot "transmitted" pour signaler qu'il est barré dans la source alors que ce n'est pas le cas. Le texte généré par Gemini ne laisse pas de doute vis-à-vis de l'aspect du texte dans la source, et invente une intention de la part de l'auteur: "&lt;em&gt;In the second line of the body, the word 'transmitted' is crossed out in the original text, but the sentence is grammatically incomplete without it (or a similar verb). It is likely the author meant to replace it to avoid repetition with the word 'transmitting' appearing a few lines later but forgot to insert the new word.&lt;/em&gt;" Alors que cette erreur était plus facile à repérer, Dan Cohen nous dit pourtant encore une fois: "&lt;em&gt;Another perfect job.&lt;/em&gt;"&lt;/p&gt;
&lt;p&gt;Le coup de grâce à mon avis vient avec le troisième exemple. Gemini n'en propose pas de transcription complète, et génère, après quelques lignes, un message indiquant que le texte est illisible au-delà d'un certain point. Cela permet à Dan Cohen d'en conclure: "&lt;em&gt;Gemini does the right thing here: rather than venture a guess like a sycophantic chatbot, it is candid when it can’t interpret a section of the letter.&lt;/em&gt;" Personnellement, je m'étouffe en lisant ça, vu les erreurs déjà notées dans les deux exemples précédents. Au contraire de ce qu'affirme Dan Cohen, il n'y a pas de candeur ici, mais plutôt une effet pervers de ce que j'imagine être un calibrage du modèle en fonction de son taux de perplexité. Dans les deux premiers exemples, on peut imaginer que la perplexité du modèle face à certains passages difficiles conduit à la génération d'une note et/ou d'un insert entre crochets, mais n'empêche pas la génération d'une transcription fausse. Elle passe d'autant plus inaperçue que les explications générées en notes sonnent bien, même si elles sont fausses. On n'a donc pas affaire à un robot candide, mais à un chatbot arnaqueur, un presti-générateur, qui trouve une porte de sortie lorsque la situation est trop grosse pour une feinte subtile. Et à mon avis il serait vraiment temps que les utilisateurs de ces logiciel intègrent cette réalité, en ayant la main d'autant moins légère quand ils contrôlent ce que génèrent ces outils.&lt;/p&gt;
&lt;p&gt;Je n'ai pas encore lu le &lt;a href="https://generativehistory.substack.com/p/gemini-3-solves-handwriting-recognition"&gt;billet&lt;/a&gt; de Mark Humphries que je mentionnais tout au début, mais j'aurais peut-être l'occasion de revenir encore sur le sujet. A vrai dire, ce que je trouve vraiment vraiment dommage avec ces publications, issues du monde académique, qui contribuent à alimenter l'hystérie autour de l'IA générative, c'est qu'elle me donne l'impression que décidément ce n'est même pas de la part de la communauté scientifique que viendra le Salut. En tant que citoyenne et jeune chercheuse, cela m'inquiète beaucoup.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT: 2025-12-01: Petites corrections et ajout de notes d'une note supplémentaire en bas de page.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT: 2025-12-04: Traduction du post en anglais, et déplacement de la version française vers un autre chemin: &lt;a href="https://alix-tz.github.io/phd/posts/025-fr"&gt;posts/025-fr.md&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:precision_inedit"&gt;
&lt;p&gt;Je donne cette précision sur l'édition des biographies et des correspondances car elle me semble importante: Dan Cohen n'a pas pris des documents dont on est sûr qu'ils soient inédits. Etant donné que les modèles d'IA générative sont entraînés à partir de tout ce qui peut être trouvé sur le Web, cela veut dire que ces lettres ont peut-être d'une manière ou d'une autre, fait partie des lots utilisés pour l'entraînement. Par exemple, &lt;a href="https://foinse.ucc.ie/en/records/IE/BL/PP/BP/1/A/1/1/51?utm_source=dancohen&amp;amp;utm_medium=email&amp;amp;utm_campaign=the-writing-is-on-the-wall-for-handwriting-recognition"&gt;sur le site&lt;/a&gt; des Archives du University College of Cork, d'où est tirée la numérisation de la lettre de Boole, on trouve le texte suivant dans le champ description: "&lt;em&gt;Boole in Cork to Maryann. He is in a very depressed mood, life has become monotonous with only his work adding interest to the day. He enjoys playing the piano but 'it would be better with someone else to listen and to be listened to'. He is also very annoyed by [Cropers] dedicating his book to him without first asking for permission - 'I cannot help feeling that he has taken a great liberty' - and speaks in strong terms of [Cropers] 'pretensions to high morality'. He invites and urges Maryann to visit him as soon as their mother's health would allow. He feels the climate would do her good.&lt;/em&gt;" Ce sont des éléments de contexte qui peuvent aider, y compris un modèle, au moment de transcrire. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025-fr/#fnref:precision_inedit" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:standard_layout"&gt;
&lt;p&gt;Je ne dis pas "mise en page standard", parce que le phénomène qui est illustré par le troisième exemple, le fait de réécrire sur la même feuille après l'avoir tournée à 90°, correspond à une pratique qu'on retrouve au moins jusqu'au milieu du XXe siècle. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025-fr/#fnref:standard_layout" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:lisible"&gt;
&lt;p&gt;Par lisible, je veux dire qu'on n'a pas besoin de savoir quelle était la phrase de départ pour comprendre ce qu'on aurait du lire dans les erreurs. J'admets par contre qu'en fonction de la familiarité avec le texte ou de la langue ou de la nature des erreurs, cette lisibilité peut varier. Si jamais vous trouvez quand même cette phrase illisible, il faut la lire comme ceci: "the hardest problem in digital humanities has finally been solved". Il y avait 1 inversion de lettres dans "&lt;em&gt;digital&lt;/em&gt;", une lettre manquante dans "&lt;em&gt;humanities&lt;/em&gt;", une lettre substituée par une autre dans "&lt;em&gt;finally&lt;/em&gt;", un lettre en trop dans "&lt;em&gt;been&lt;/em&gt;" et une séparation inappropriée dans "&lt;em&gt;solved&lt;/em&gt;". &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025-fr/#fnref:lisible" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:ordre_lignes"&gt;
&lt;p&gt;Je développe très rapidement sur le point de la mise en page. Dans la transcription de Gemini, il y a des compléments d'informations qui suggèrent que le modèle a bien identifié à quelle page correspond telle ou telle partie du texte. Dans la transcription de Transkribus, ce n'est pas le cas, mais je pense que c'est parce que Dan Cohen a seulement utilisé la page de test de modèles de transcription de Transkribus. S'il avait utilisé la version complète de Transkribus, je suis sûre que le modèle aurait aussi parfaitement identifié la mise en page en double page. Pour ce qui concerne la transcription ligne par ligne, on n'a plus cette information dans la transcription de Gemini, qui génère le texte en continu. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025-fr/#fnref:ordre_lignes" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:precision_error_tkb"&gt;
&lt;p&gt;Parmi les erreurs de Transkribus, on peut aussi noter l'utilisation d'un "&lt;a href="https://www.compart.com/en/unicode/U+0432"&gt;в&lt;/a&gt;" (le v cyrillique) pour transcrire le "B" de la côte du document, et d'un "&lt;a href="https://www.compart.com/en/unicode/U+0440"&gt;р&lt;/a&gt;" (le r cyrillique) pour transcrire le "P" qui suit. Ce sont des erreurs qui nous échappent quand on fait un contrôle visuel rapide, qui ne gêne pas la lecture par les humains, mais qui font baisser la précision calculée automatiquement puisque qu'un в n'est pas un B et un р n'est pas un P, ni d'ailleurs un p (see what I did here?). &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025-fr/#fnref:precision_error_tkb" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:occasion_by"&gt;
&lt;p&gt;Transkribus l'avait transcrit "&lt;em&gt;occasion by&lt;/em&gt;". &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/025-fr/#fnref:occasion_by" title="Jump back to footnote 6 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>evaluation</category><category>French blog posts</category><category>Generative AI</category><category>HTR</category><category>Large Language Models</category><category>literature review</category><guid>https://alix-tz.github.io/phd/posts/025-fr/</guid><pubDate>Fri, 28 Nov 2025 21:50:54 GMT</pubDate></item><item><title>024 - The messy backstage of a literature review</title><link>https://alix-tz.github.io/phd/posts/024/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;A few weeks ago, I began a thorough review of articles published in four digital humanities venues to track mentions of automatic text recognition and understand how, where, and why scholars use it. Although I wish I had started sooner in my doctoral journey, I stay positive holding on to the idea that "it's never too late." I learn a lot about Digital Humanities as a field of research and gain a better understanding of ATR's presence in the field.&lt;/p&gt;
&lt;p&gt;While catching up on our dissertation progress, I was telling Roch Delanney about the survey I'm conducting, my goals for it, and how I selected and sorted the articles. Roch suggested that I share my method more widely. It seems a little clumsy at times, but I am also able to use many different skills I have learned and sharpened over the years so I think it is indeed interesting to share a bit of my &lt;em&gt;cuisine&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Perimeter of the literature review&lt;/h3&gt;
&lt;p&gt;My literature review focuses on four publication venues. I think they are, collectively, representative of research in the Digital Humanities: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://academic.oup.com/dsh"&gt;&lt;em&gt;Digital Scholarship in the Humanities&lt;/em&gt;&lt;/a&gt; (DSH), which is presented by the Alliance of Digital Humanities Organizations (ADHO) as an international, peer-reviewed journal published by Oxford University Press on behalf of ADHO and the European Association for Digital Humanities (EADH). It was published under the title &lt;em&gt;Literary and Linguistic Computing: The Journal of Digital Scholarship in the Humanities&lt;/em&gt; until 2014. I counted a total of 174 volumes for a total of 1741 articles (excluding retracted articles, book reviews, editorials and committee reports) published since 1985 until the first half of 2025.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://dhq.digitalhumanities.org/"&gt;&lt;em&gt;Digital Humanities Quarterly&lt;/em&gt;&lt;/a&gt; (DHQ) is an open-access peer-reviewed journal, probably more representative of research in North America. It is published by the Association for Computers and the Humanities (ACH). I counted a total of 790 articles published since its first issue in 2007. Most articles are in English.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;a href="https://jdmdh.episciences.org/"&gt;&lt;em&gt;Journal of Data Mining and Digital Humanities&lt;/em&gt;&lt;/a&gt; (JDMDH), is published by Episciences since 2017. Contrary to DHQ, its focus is more European-centric, and it has a special volume dedicated specifically to automatic text recognition (directed by Ariane Pinche and Peter Stokes). I found a total of 162 articles published in JDMDH, including the special volume on ATR.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lastly, the proceedings from the more recent &lt;em&gt;Computational Humanities Research&lt;/em&gt; (CHR) conferences (see the &lt;a href="https://2024.computational-humanities-research.org/"&gt;2024 conference proceedings&lt;/a&gt; for example) offer a perspective on research focused on more intensively computational methods in the Humanities. The conference is held annually since 2021. I found a total of 214 articles in the proceedings.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Aside from DSH, that I can access thanks to the library of the University of Montréal, all the other journals are in open access. &lt;/p&gt;
&lt;h3&gt;Collecting the articles and their metadata&lt;/h3&gt;
&lt;p&gt;For JDMDH, articles are not centralized on the journal website but rather published on platforms like &lt;a href="https://hal.archives-ouvertes.fr/"&gt;HAL&lt;/a&gt; or &lt;a href="https://arxiv.org/"&gt;arXiv&lt;/a&gt; and sometimes &lt;a href="https://zenodo.org/"&gt;Zenodo&lt;/a&gt;. Getting an overview of the articles published in JDMDH is not straightforward, but it is possible to browse the articles per &lt;a href="https://jdmdh.episciences.org/browse/volumes"&gt;volumes&lt;/a&gt;. I opened and downloaded each article in each volume, as well as collected the article entries in Zotero using the Zotero connector. The process was cumbersome and required many clicks, but the variety of publishing platforms deterred me from writing a script to automate the downloading process.  &lt;/p&gt;
&lt;p&gt;CHR, on the other hand, was very easy to scrape, partly because there are only four volumes of proceedings so far. For each series of proceeding, the index of all articles is compatible with the batch import scenario of the Zotero connector. To collect the PDFs, I used a section of the HTML page and regular expressions to identify the links to the PDF files, creating a list of URLs. Finally, I used a Python script to download the PDFs to my computer.  &lt;/p&gt;
&lt;p&gt;For example, in &lt;a href="https://ceur-ws.org/Vol-2989/"&gt;https://ceur-ws.org/Vol-2989/&lt;/a&gt;, the &lt;code&gt;ul&lt;/code&gt; contains simple HTML elements pointing to the PDF files, such as: &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;h3&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"CEURSESSION"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Presented papers&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;h3&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;

&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;ul&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;li&lt;/span&gt; &lt;span class="na"&gt;id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"long_paper5"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="na"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"long_paper5.pdf"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"CEURTITLE"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Entity Matching in Digital Humanities Knowledge
      Graphs&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"CEURPAGES"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;1-15&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;br&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"CEURAUTHOR"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Juriaan Baas&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;,
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"CEURAUTHOR"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Mehdi M. Dastani&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;,
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"CEURAUTHOR"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Ad J. Feelders&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;li&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
...
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;All I had to do was copy and paste this entire list into a text editor (I like to use &lt;a href="https://www.sublimetext.com/"&gt;Sublime Text&lt;/a&gt; in such a situation). Then, I used a simple regular expression like &lt;code&gt;href=".+?"&lt;/code&gt; to select the value in the &lt;code&gt;a&lt;/code&gt; element, which contains the links to the PDF files. I kept only the selected text and then rebuilt the complete URL with a couple of replacements such as &lt;code&gt;href="&lt;/code&gt; -&amp;gt; &lt;code&gt;"https://ceur-ws.org/Vol-2989/&lt;/code&gt; and &lt;code&gt;"\n&lt;/code&gt; -&amp;gt; &lt;code&gt;",\n&lt;/code&gt;. At this point I just added square brackets around the selection, et voilà! I had a Python list ready to be passed to a script like the one below to download the files:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;list_of_urls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"https://ceur-ws.org/Vol-2723/short8.pdf"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s2"&gt;"https://ceur-ws.org/Vol-2723/long35.pdf"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s2"&gt;"https://ceur-ws.org/Vol-2723/long44.pdf"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="c1"&gt;#...&lt;/span&gt;
                &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;requests&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;tqdm&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;tqdm&lt;/span&gt; &lt;span class="c1"&gt;# it makes  progress bar so I know how long I can take to make a tea while the script runs&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tqdm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_of_urls&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;status_code&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"/"&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;-&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"/"&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
        &lt;span class="c1"&gt;#print(filename)&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"wb"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Failed to download: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# This cool down is to be polite to the server&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I used a similar approach for downloading the articles from DHQ because the &lt;a href="https://dhq.digitalhumanities.org/index/title.html"&gt;Index of Titles&lt;/a&gt; lists all of the published articles on a single page. I first downloaded the HTML pages of the articles (DHQ publishes articles in HTML format as well as PDF). I also used regular expressions to extract the list of links and used a Python script to download the files.  &lt;/p&gt;
&lt;p&gt;Unfortunately, the Zotero connector only works on each article page individually, but not for batch-import on the index page. I investigated a bit to understand why it was so, and found that in the source code of each article page, there is a &lt;code&gt;span&lt;/code&gt; element identified with the class &lt;code&gt;Z3988&lt;/code&gt; that the Zotero connector uses to extract the metadata and create an entry in Zotero. In DHQ, these spans look like this:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"Z3988"&lt;/span&gt; &lt;span class="na"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"url_ver=Z39.88-2004&amp;amp;amp;ctx_ver=Z39.88-2004&amp;amp;amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;amp;amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;amp;amp;rft.genre=article&amp;amp;amp;rft.atitle=Academics%20Retire%20and%20Servers%20Die%3A%20Adventures%20in%20the%20Hosting%20and%20Storage%20of%20Digital%20Humanities%20Projects&amp;amp;amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;amp;amp;rft.stitle=DHQ&amp;amp;amp;rft.issn=1938-4122&amp;amp;amp;rft.date=2023-05-26&amp;amp;amp;rft.volume=017&amp;amp;amp;rft.issue=1&amp;amp;amp;rft.aulast=Cummings&amp;amp;amp;rft.aufirst=James&amp;amp;amp;rft.au=James%20Cummings"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;span&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I understood recently, while discussing with Margot Mellet, that Z3988 is a reference to the &lt;a href="https://groups.niso.org/higherlogic/ws/public/download/14833/z39_88_2004_r2010.pdf"&gt;OpenURL Framework Standard (ISO Z 39.88-2004)&lt;/a&gt;, which is used by the Zotero connector. Also, I should note that such spans are not systematically used in online journals. JDMDH for example doesn't use them, and serves the metadata in a different way.  &lt;/p&gt;
&lt;p&gt;Since I had already downloaded all the articles from DHQ as HTML files, I wrote a simple Python script that found all of such spans for each downloaded article and aggregated them in a single, very simple HTML file. Then, I simply opened this page in my browser after emulating a local server&lt;sup id="fnref:python_server"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/024/#fn:python_server"&gt;1&lt;/a&gt;&lt;/sup&gt; (with a command like &lt;code&gt;python -m http.server&lt;/code&gt;), and I was able to use the Zotero connector to import all the articles in a single click. It was very satisfying! The only downside is that I couldn't collect the articles' abstracts because there weren't included in the spans.  &lt;/p&gt;
&lt;p&gt;DSH was different from the rest of the journals. Because of the longevity of the journal and the amount of articles it published, it was quite overwhelming. Unfortunately, it is a paywalled journal and I couldn't figure out how to make the proxy of the University of Montreal library work with my Python scripts and the command line. As a result, I had to manually download the articles,&lt;sup id="fnref:proxy_dsh"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/024/#fn:proxy_dsh"&gt;2&lt;/a&gt;&lt;/sup&gt; but only when they were relevant! Since DSH has a fairly good search engine that allows to do multi-keyword searches, I only downloaded articles matching my search criteria (143 in total).&lt;/p&gt;
&lt;p&gt;Additionally, I went through each of the 174 issues of DSH to batch-import the article references in Zotero. It was tedious but I figured I might be able to use these metadata for other projects in the future.  &lt;/p&gt;
&lt;h3&gt;Filtering the articles&lt;/h3&gt;
&lt;p&gt;For DHQ, JDMDH and CHR, I ran a keyword surch using the command &lt;a href="https://www.man7.org/linux/man-pages/man1/grep.1.html"&gt;&lt;code&gt;grep&lt;/code&gt;&lt;/a&gt; on the content of the articles. I didn't want to limit my search to the titles, abstract or keywords because I really wanted to include anecdotal mentions of automatic text recognition in my results.  &lt;/p&gt;
&lt;p&gt;To use grep, I created a file (pattern.txt) with the keywords I was looking for:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;HTR
OCR
text recognition
ATR
Transkribus
eScriptorium
automatic transcription
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I converted the PDFs into text files using the command &lt;a href="https://man.archlinux.org/man/pdftotext.1.en"&gt;pdftotext&lt;/a&gt;. This was necessary because grep cannot search inside a PDF directly. I didn't need to do this conversion for DHQ, since I had download HTML files from that journal. &lt;/p&gt;
&lt;p&gt;The commands to search inside the PDFs of one of the journals would look like this:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;ls&lt;span class="w"&gt; &lt;/span&gt;*.pdf&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;xargs&lt;span class="w"&gt; &lt;/span&gt;-n1&lt;span class="w"&gt; &lt;/span&gt;pdftotext&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# to convert PDFs to text files&lt;/span&gt;
grep&lt;span class="w"&gt; &lt;/span&gt;-i&lt;span class="w"&gt; &lt;/span&gt;-w&lt;span class="w"&gt; &lt;/span&gt;-m5&lt;span class="w"&gt; &lt;/span&gt;-H&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;../pattern.txt&lt;span class="w"&gt; &lt;/span&gt;*.txt&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# to search for the keywords in the text files and display the first 5 matches&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After controlling how grep matched the keywords, I used &lt;code&gt;grep -l -f ../pattern.txt *.txt&lt;/code&gt; to list the files that matched the keywords. This list was used to sort the documents into two folders, according to whether or not they matched my research.&lt;/p&gt;
&lt;p&gt;In the case of DSH, I directly used the search engine to combine the keywords, using the "OR" operator. I set the full text of the articles as the scope of my research: &lt;a href="https://academic.oup.com/dsh/search-results?allJournals=1&amp;amp;f_ContentType=Journal+Article&amp;amp;fl_SiteID=5447&amp;amp;cqb=[{%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22automatic%20transcription%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22transkribus%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22text%20recognition%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22escriptorium%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22OCR%22}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22HTR%22}]}]&amp;amp;qb={%22_text_1-exact%22:%22automatic%20transcription%22,%22qOp2%22:%22OR%22,%22_text_2-exact%22:%22transkribus%22,%22qOp3%22:%22OR%22,%22_text_3-exact%22:%22text%20recognition%22,%22qOp4%22:%22OR%22,%22_text_4-exact%22:%22escriptorium%22,%22qOp5%22:%22OR%22,%22_text_5%22:%22OCR%22,%22qOp6%22:%22OR%22,%22_text_6%22:%22HTR%22}&amp;amp;page=1"&gt;https://academic.oup.com/dsh/search-results?allJournals=1&amp;amp;f_ContentType=Journal+Article&amp;amp;fl_SiteID=5447&amp;amp;cqb=[{%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22automatic%20transcription%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22transkribus%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22text%20recognition%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22escriptorium%22,%22exactMatch%22:true}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22OCR%22}]},{%22condition%22:%22OR%22,%22terms%22:[{%22filter%22:%22_text_%22,%22input%22:%22HTR%22}]}]&amp;amp;qb={%22_text_1-exact%22:%22automatic%20transcription%22,%22qOp2%22:%22OR%22,%22_text_2-exact%22:%22transkribus%22,%22qOp3%22:%22OR%22,%22_text_3-exact%22:%22text%20recognition%22,%22qOp4%22:%22OR%22,%22_text_4-exact%22:%22escriptorium%22,%22qOp5%22:%22OR%22,%22_text_5%22:%22OCR%22,%22qOp6%22:%22OR%22,%22_text_6%22:%22HTR%22}&amp;amp;page=1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In both cases, the search was not case sensitive, in order to catch a maximum of occurrences of keywords like "automatic text recognition" or "Text Recognition" or "text recognition", etc. However, it meant that sometimes I found false positives: "democracy" often matches with "ocr", so does "theatre" with "atr". Since DSH's search engine returns the match in context, I was able to ignore these false positives. For the other journals, I had to manually check where the matches were. Usually, I combined this control with the next step of my investigation.  &lt;/p&gt;
&lt;h4&gt;Hits per journal&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;JDMDH: 47 hits (out of 162 articles)&lt;/li&gt;
&lt;li&gt;DHQ: 93 hits (out of 790 articles)&lt;/li&gt;
&lt;li&gt;DSH: 143 relevant hits (out of 1741 articles)&lt;/li&gt;
&lt;li&gt;CHR: 65 hits (out of 214 articles)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;em&gt;Dépouillement&lt;/em&gt; and analysis&lt;/h3&gt;
&lt;p&gt;To this date, I am still in the process of reading the articles and taking notes on the occurrences of my keywords. &lt;/p&gt;
&lt;p&gt;I use Zotero to keep track of the articles I read and to confirm whether they are false positives. Sometimes, I leave out articles that are irrelevant, even if they mention a keyword I was looking for. For example, &lt;a href="https://doi.org/10.1093/llc/fqac089"&gt;Liu &amp;amp; Zhu (2023)&lt;/a&gt;&lt;sup id="fnref:liu_zhu_2023"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/024/#fn:liu_zhu_2023"&gt;3&lt;/a&gt;&lt;/sup&gt; contains the string "OCR" but it only appears in a title in their bibliography, for work they refer to in a context where OCR is not relevant to their argument. With tags in Zotero, I clearly identify such articles as "to be left out" from my analysis, but I don't remove them from the collection.  &lt;/p&gt;
&lt;p&gt;I use different tags to identify the various occurrences of the technology in the articles. For example, I distinguish between firsthand applications of ATR and the reuse of data produced by ATR before the experimentation presented by the authors. Typically, there are many mentions of documents that were OCRed by libraries and used by scholars to conduct their research. Overall, with this analysis, I am trying to add more depth to the observations made by &lt;a href="https://doi.org/10.48550/arXiv.2304.13530"&gt;Tarride et al (2023)&lt;/a&gt;&lt;sup id="fnref:tarride_et_al_2023"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/024/#fn:tarride_et_al_2023"&gt;4&lt;/a&gt;&lt;/sup&gt; in which they pragmatically considered three situations leading to the use of ATR: 1) for the production of digital editions; 2) for the production of large searchable text corpora; and 3) for the production of non-comprehensive transcriptions to feed knowledge bases. However, it is difficult to elaborate definitive categories before I am done processing all the collected articles.  &lt;/p&gt;
&lt;p&gt;Due to the large number of articles to be analyzed, I have continued to use the grep command to quickly review the content of articles and speed up my sorting process. For example, I am more interested in firsthand usages of ATR, want to be able to quickly identify non relevant mentions of my keywords as was the case in Liu &amp;amp; Zhu (2023). The command &lt;code&gt;grep -i -w -C 5 -H -f ../pattern.txt *.txt &amp;gt; grep_out&lt;/code&gt; allows me to generate a file, grep_out, in which, for each time a keyword is matched in a document, five lines of context are displayed before and after the match, as well as the name of the file. I still have to read the abstracts and parts of the articles to clearly understand in which contexts the automatic text recognition technologies are used. However, this is an effective method for quickly sorting through the articles.&lt;/p&gt;
&lt;p&gt;I'm looking forward to sharing the results of this analysis in my dissertation! &lt;/p&gt;
&lt;!-- FOOTNOTES ---&gt;

&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:python_server"&gt;
&lt;p&gt;This emulation is necessary to allow the Zotero connector to work properly. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/024/#fnref:python_server" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:proxy_dsh"&gt;
&lt;p&gt;I want to specify here that it was not by lack of reading documentations on proxies and requests. Unable to find a straightforward solution, unsure if it was even something that the UdeM proxy allowed, and because I would have still needed to write additional scripts afterwards, I decided that it would take just as long to do it manually (about 2-3 hours). &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/024/#fnref:proxy_dsh" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:liu_zhu_2023"&gt;
&lt;p&gt;Liu, Lei, and Min Zhu. "Bertalign: Improved Word Embedding-Based Sentence Alignment for Chinese–English Parallel Corpora of Literary Texts." &lt;em&gt;Digital Scholarship in the Humanities&lt;/em&gt; 38, no. 2 (June 1, 2023): 621–34. &lt;a href="https://doi.org/10.1093/llc/fqac089"&gt;https://doi.org/10.1093/llc/fqac089&lt;/a&gt;. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/024/#fnref:liu_zhu_2023" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:tarride_et_al_2023"&gt;
&lt;p&gt;Tarride, Solène, Mélodie Boillet, and Christopher Kermorvant. "Key-Value Information Extraction from Full Handwritten Pages." arXiv, April 26, 2023. &lt;a href="https://doi.org/10.48550/arXiv.2304.13530"&gt;https://doi.org/10.48550/arXiv.2304.13530&lt;/a&gt;. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/024/#fnref:tarride_et_al_2023" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>HTR</category><category>literature review</category><category>OCR</category><category>survey</category><guid>https://alix-tz.github.io/phd/posts/024/</guid><pubDate>Sat, 21 Jun 2025 19:15:27 GMT</pubDate></item></channel></rss>