<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="https://alix-tz.github.io/phd/assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>A research (b)log (Posts about kraken)</title><link>https://alix-tz.github.io/phd/</link><description></description><atom:link href="https://alix-tz.github.io/phd/categories/kraken.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents ¬© 2024 &lt;a href="https://alix-tz.github.io/phd/"&gt;Alix Chagu√©&lt;/a&gt; CC-BY</copyright><lastBuildDate>Wed, 14 Aug 2024 19:54:19 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>017 - Deploying eScriptorium online: notes on CREMMA's server specifications</title><link>https://alix-tz.github.io/phd/posts/017/</link><dc:creator>Alix Chagu√© and Thibault Cl√©rice</dc:creator><description>&lt;p&gt;&lt;a href="https://gitlab.com/scripta/escriptorium/"&gt;eScriptorium&lt;/a&gt; is a web application designed to perform automatic text recognition campaigns, by default powered by the OCR/HTR engine &lt;a href="https://kraken.re/"&gt;Kraken&lt;/a&gt;. It comes in a decentralized form, meaning that the application is not distributed by a single organization but can, on the contrary, be deployed by several actors on many different servers. In fact, you can also deploy eScriptorium &lt;a href="https://gitlab.com/scripta/escriptorium/-/wikis/docker-install"&gt;on your personal machine&lt;/a&gt;, simulating a local server.&lt;sup id="fnref:localhost"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:localhost"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;As eScriptorium is gaining attention, more institutions are interested in building their own server to host the application and offer it to their associates. At Inria, we deployed eScriptorium for the first time in 2020, specifically for the project called &lt;a href="https://lectaurep.hypotheses.org/"&gt;LECTAUREP&lt;/a&gt; which we ran with the &lt;a href="https://www.archives-nationales.culture.gouv.fr/"&gt;French national archives&lt;/a&gt; between 2018 and 2021. While the initial server was hosted on a virtual machine, without any GPU, and open to a relatively small amount of users, our current eScriptorium application already counts nearly 500 users and will soon be hosted on a much different server infrastructure, funded by the &lt;a href="https://www.pamir.fr/projets-soutenus/cremma/"&gt;CREMMA project&lt;/a&gt;. Between the original LECTAUREP-eScriptorium server and the CREMMA server, we moved to a dedicated server (&lt;code&gt;Traces-6&lt;/code&gt;) for which we invested about 20K‚Ç¨.&lt;/p&gt;
&lt;p&gt;Since I have been regularly in touch with people from different institutions who were looking into buying the hardware to create their own server for eScriptorium, I thought it was largely time to put all the deets in writing!&lt;/p&gt;
&lt;p&gt;To write today's post, I'm very happy to welcome a second pair of hands: Thibault Cl√©rice's. His expertise and involvement in designing CREMMA server are crucial here!&lt;/p&gt;
&lt;p&gt;Let's first discuss some technical requirements, then we'll describe how the CREMMA server was designed. We finish with some very important remarks on the necessity (or not) to build a server and on useful alternatives for the community!&lt;/p&gt;
&lt;h3&gt;Should you buy GPUs?&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Graphics_processing_unit"&gt;GPUs&lt;/a&gt; (or Graphics Processing Units) are not mandatory at all when you use eScriptorium. This is the reason why it is perfectly acceptable to run eScriptorium locally, on your own computer. Actually GPUs are not even mandatory to train Kraken models: training can be done on CPUs (your computer's processor), they will simply go much much much slower.&lt;/p&gt;
&lt;p&gt;That, however, is true for personal or light use of the training features. If on the contrary you create a server open to dozens of users or more, then connecting eScriptorium to GPUs is very much a good idea: since training a model on a CPU alone can take 2-3 days (or much more), you don't really want 10 users to start a training task at the same time. In the absence of shared GPUs, their training will be queued for days or even weeks and the overload might degrade the experience of other users on the rest of the application. As long as we are building an infrastructure (and hopefully sharing costs), we may as well enhance the experience of everyone, no?&lt;/p&gt;
&lt;p&gt;This being said, you shouldn't rush and go buy a GPU right away. Instead, you should first look at options to &lt;em&gt;optimize&lt;/em&gt; its usage or at infrastructures that are already available to you. For example, the &lt;a href="https://www.unige.ch/lettres/humanites-numeriques/recherche/projets-de-la-chaire/fondue"&gt;FONDuE infrastructure&lt;/a&gt;, at the University of Geneva, doesn't use the GPUs only for eScriptorium: they connect their application to a cluster which is used by researchers for intense computation tasks outside of eScriptorium (it's an &lt;a href="https://en.wikipedia.org/wiki/High-performance_computing"&gt;HPC&lt;/a&gt; with a university-wide queue controlled by &lt;a href="https://en.wikipedia.org/wiki/Slurm_Workload_Manager"&gt;SLURM&lt;/a&gt;). This is a very good solution for optimization, because training Kraken models is not a constant activity: if the GPU is dedicated to eScriptorium only, then it will be used for a few hours here and there, not even at 100% of its capacity. Think of it: users of the application will usually need to train a model at the beginning of their transcription campaign, therefore once they have an &lt;a href="https://alix-tz.github.io/phd/posts/12/"&gt;accurate model&lt;/a&gt;, they will focus on using the model for prediction, which doesn't rely on the GPUs (and Kraken isn't really optimized for GPU usage at prediction time anyway).&lt;/p&gt;
&lt;p&gt;Other possibilities include connecting the server to a completely physically separate cluster where training jobs are submitted. This is a possibility that several people told me they were exploring, but I don't know if anyone has set it already. Why would you opt for a solution with an external cluster? To replace some huge investment costs (original funding) with some smaller (but much more regular) functioning costs: for example, for CREMMA, nearly half of our 40K‚Ç¨ budget was spent, in 2022, on buying two &lt;a href="https://www.nvidia.com/fr-fr/data-center/a100/"&gt;A100 graphic cards from Nvidia&lt;/a&gt;. When using someone else's GPUs, not only you save the money you would spend on the hardware, but on top of that, you contribute to optimizing the use of other GPUs already in place. Another reason is because you might not have the human resources to administer the system and the GPUs. There are multiple calculation clusters created for Academia (of the top of our head: &lt;a href="https://www.cnrs.fr/fr/presse/jean-zay-le-supercalculateur-le-plus-puissant-de-france-pour-la-recherche"&gt;Jean Zay&lt;/a&gt; or &lt;a href="https://www.calculquebec.ca/services-aux-chercheurs/infrastructures-et-services/"&gt;Calcul Qu√©bec&lt;/a&gt;), and you could even consider using commercial solutions as well (like &lt;a href="https://aws.amazon.com/nvidia/"&gt;AWS&lt;/a&gt;, &lt;a href="https://cloud.google.com/gpu?hl=fr"&gt;Google Cloud&lt;/a&gt; and the like). Then, your money is spent on the actual computation and not on making the computation possible in the first place.&lt;/p&gt;
&lt;p&gt;Fair enough, plugging eScriptorium's task manager to an external server might not be that simple. However, for smaller groups of users, it is also worth taking into account that it is perfectly possible to train Kraken models using Kraken directly (through an SSH connection to a (super-)cluster, for example) before uploading them into the application. In such a case, eScriptorium is only used for its ergonomics, not as a simplified interface to train models.&lt;/p&gt;
&lt;p&gt;Let's summarize the point here: GPUs are not always a must-have for eScriptorium or Kraken, so you should definitely consider first and foremost your future usage. They currently represent the biggest share in the hardware expenses to build a calculation server. There are options out there where you don't spend 10K‚Ç¨ to buy a GPU but rather connect to an external, ready-to-use service. Or, if you do decide to spend the money, you should consider ways to maximize its usage for other training tasks, possibly outside of eScriptorium.&lt;/p&gt;
&lt;h3&gt;Some considerations on storage&lt;/h3&gt;
&lt;p&gt;Normally, eScriptorium is used as an (assisted) annotation environment to obtain the transcription of documents. You would use eScriptorium:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In a preparatory phase:&lt;ul&gt;
&lt;li&gt;(1a) to produce training data, and&lt;/li&gt;
&lt;li&gt;(1b) to elaborate (aka train) performant segmentation or transcription models;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In a production phase, but only for relatively small corpora, to apply segmentation and transcription models and manually correct the results (in which case the size of the corpora must be compatible with the scale of what an individual or your assembled team can process);&lt;/li&gt;
&lt;li&gt;In a post-production phase, including for samples of a very large corpus, to easily visualize and control the result of the (large-scale) automatic prediction and potentially correct it (cf. n¬∞2).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On the other hand, large scale transcription campaigns should probably be led with Kraken in the command line directly (so only n¬∞1 and n¬∞3 necessitate eScriptorium). Thibault has even produced a small python library to design such campaigns (&lt;a href="https://github.com/ponteIneptique/rtk"&gt;RTK&lt;/a&gt;, for Release the Krakens) which was recently used in &lt;a href="https://enc.hal.science/hal-04250657/"&gt;a paper&lt;/a&gt;&lt;sup id="fnref:lovewarref"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:lovewarref"&gt;2&lt;/a&gt;&lt;/sup&gt; where a 38.5M token corpus was produced. In some cases, n¬∞1b even benefits from being performed outside of eScriptorium, since the application offers a very limited control over &lt;a href="https://kraken.re/main/ketos.html#recognition-training"&gt;Kraken's training parameters&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This has several consequences on the way you should consider storage on a server dedicated to eScriptorium. Duplicates of images are created on the server while they are being processed in the application, but they should always be considered as such: temporary duplicates while phase 1, 2 or 3 are under progress. They shouldn't be considered as if eScriptorium was 1) an archiving solution for transcription projects, 2) a querying interface to explore a corpus or even 3) a publication environment for a minimalistic digital edition. eScriptorium is only one brick --an early one even-- in the corresponding pipelines. Instead, the original image files should be stored somewhere else, in an adapted data warehouse (like &lt;a href="https://zenodo.org/"&gt;Zenodo&lt;/a&gt;, &lt;a href="https://www.nakala.fr/"&gt;Nakala&lt;/a&gt;, etc.), or published in digital libraries under the responsibility of their owner (like &lt;a href="https://archive.org/"&gt;Internet Archive&lt;/a&gt;, &lt;a href="https://gallica.bnf.fr/"&gt;Gallica&lt;/a&gt;, etc.).&lt;/p&gt;
&lt;p&gt;What this means when designing a server to host eScriptorium is that its storage capacity should of course be big enough to store the temporary image files,&lt;sup id="fnref:temporary"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:temporary"&gt;3&lt;/a&gt;&lt;/sup&gt; while users are working on their annotation, aka the active projects. However, this storage doesn't need to be expended all the time and it should also be ok to flush the terminated projects: at that point the images and their annotations should have been archived on more appropriate data warehouses by their creators, and it should be their responsibility.&lt;/p&gt;
&lt;h3&gt;Don't forget the RAM!&lt;/h3&gt;
&lt;p&gt;Not overlooking the &lt;a href="https://en.wikipedia.org/wiki/Random-access_memory"&gt;RAM&lt;/a&gt; is very important when designing your server! But what is it used for? It's used for cache by the web application: it means that frequently accessed data, like web pages and images but also the content of the database, are temporarily loaded in live memory. Cache thus ensures that the requests sent by the users are served quickly. For example, if you don't have enough RAM (or enough cache), pages will load slowly, and if you have used eScriptorium before reading this post, you know how important it is to be able to load images fast enough.&lt;/p&gt;
&lt;p&gt;RAM is also essential for inference and training because images and annotations are loaded in memory before being passed to the CPU or the GPU. If the RAM is not powerful enough, it will be detrimental to computation and will cause a bottleneck situation. Thus having invested in GPUs and/or CPUs but not in enough RAM would be like having a horse to pull a Ferrari: even if prediction and training could go fast on the processing units, it will be restrained by the available live memory.&lt;/p&gt;
&lt;h3&gt;Modularity for the CREMMA infrastructure&lt;/h3&gt;
&lt;p&gt;The CREMMA infrastructure was originally designed by Thibault with a simple but essential principle in mind: modularity. Instead of thinking of an eScriptorium server as a monolithic block of hardware designed for front-end service, storage and intense computation, he suggested to break each of these blocks into individual servers connected together. CREMMA&lt;sup id="fnref:cremma"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:cremma"&gt;4&lt;/a&gt;&lt;/sup&gt; is thus made of at least three servers, as shown in the schema below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CREMMA_FRONTEND&lt;/code&gt;, for the front-end, where the application is deployed and where the database is stored.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CREMMA_STORAGE&lt;/code&gt;, for storage, where all the images and models, as well as the backup of the database are stored on the long term. Currently, &lt;code&gt;CREMMA_STORAGE&lt;/code&gt; has a storage capacity of 38Tb&lt;sup id="fnref:storage"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:storage"&gt;5&lt;/a&gt;&lt;/sup&gt; but we could easily add more disks if we find that it is necessary. &lt;/li&gt;
&lt;li&gt;&lt;code&gt;CREMMA_COMPUTE&lt;/code&gt;, where the two A100 GPUs I mentioned earlier are plugged and where the application task manager "sends" all the jobs, whether they are to be run on CPU (these tasks include segmentation and transcription prediction for example), or on GPU (training for the most part).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/CREMMA_server_specs.png" alt="A model of the CREMMA infrastructure where three blocks (front-end, storage and compute) are connected together through an intranet 10Gb/s connection. For each block, one or two server(s) is presented along with their specification. Credits: Thibault Cl√©rice and Alix Chagu√©. The full text of the specifications is accessible in a commentary in the source code of this page, just after this image." widht="400px"&gt;&lt;/p&gt;
&lt;!-- 
Full text of the specifications displayed on the image, for accessibility purposes:

- CREMMA_FRONTEND: 2xEPYC 7302 16C/32T; 128 Gb RAM; 1.6 To; RAID with 8Gb Cache.
- CREMMA_STORAGE: Optimized for cache; 256 Gb RAM; 2xRaidZ 19Tb; NVME for ZFS / L2ARC; SSD for OS.
- CREMMA_COMPUTE: 2xAMD EPYC 7302 16C/32T; 128 Gb RAM; 2xRTX A6000 24Gb.
- TRACES-6: 2xEPYC 7452 32C/64T; 512 Gb RAM; 0.8 Tb RAID with 8Gb Cache; 2xA100 40 Gb.
--&gt;

&lt;p&gt;As you can see on the schema, there will actually be a fourth server involved in the infrastructure: &lt;code&gt;Traces-6&lt;/code&gt;, the server we currently use &lt;a href="https://escriptorium.inria.fr/"&gt;to deploy eScriptorium at Inria&lt;/a&gt;. Like &lt;code&gt;CREMMA_COMPUTE&lt;/code&gt;, &lt;code&gt;Traces-6&lt;/code&gt; can be called by &lt;code&gt;CREMMA_FRONTEND&lt;/code&gt; for computation tasks. In fact, this is where the modularity of the system is interesting: with such a set-up, it is possible to add more computation servers to the pool of GPUs reachable by &lt;code&gt;CREMMA_FRONTEND&lt;/code&gt; without having to redesign the whole infrastructure. On their side, &lt;code&gt;CREMMA_FRONTEND&lt;/code&gt; and &lt;code&gt;CREMMA_STORAGE&lt;/code&gt; can be upgraded (to add more RAM or more storage) very easily.&lt;/p&gt;
&lt;p&gt;This modularity also means that the GPUs remain free for other uses: for example if we were to have to run maintenances on &lt;code&gt;CREMMA_COMPUTE&lt;/code&gt;, we can simply cut it from the infrastructure, and let &lt;code&gt;CREMMA_FRONTEND&lt;/code&gt; interact with &lt;code&gt;Traces-6&lt;/code&gt; only while we work on &lt;code&gt;CREMMA_COMPUTE&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;CREMMA_COMPUTE&lt;/code&gt; is equipped with two &lt;a href="https://www.nvidia.com/fr-fr/data-center/a100/"&gt;A100&lt;/a&gt; graphic cards, and &lt;code&gt;Traces-6&lt;/code&gt; with two &lt;a href="https://www.nvidia.com/fr-fr/design-visualization/rtx-6000/"&gt;RTX 6000&lt;/a&gt;. Actually, it doesn't mean that only 4 training can be happening at once. Each of these GPUs offer between 24 and 40 Gb of RAM for intense computation. It's a lot. It's so much actually that training a Kraken model at max speed would rarely use more than 40% of this processing power. &lt;a href="https://www.nvidia.com/en-us/data-center/virtual-solutions/"&gt;Virtualization&lt;/a&gt; is a nice trick to "break" the GPU down into smaller virtual GPUs (or vGPUs). What is broken down is the RAM capacity. We opted for the following virtualization set up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each of the A100 graphic cards and their 40Gb of RAM are turned into 1 10Gb vGPU + 5 5Gb vGPUs (since 10+5x5=35, note that we must leave 5Gb out of the equation for the virtualization).&lt;/li&gt;
&lt;li&gt;No virtualization is applied to Traces-6's RTX6000s.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How did we decide on these numbers? Thibault ran a series of small tests executing either &lt;a href="https://kraken.re/main/ketos.html#segmentation-training"&gt;&lt;code&gt;segtrain&lt;/code&gt;&lt;/a&gt; or &lt;a href="https://kraken.re/main/ketos.html#recognition-training"&gt;&lt;code&gt;train&lt;/code&gt;&lt;/a&gt; and playing with two different parameters: the &lt;a href="https://github.com/mittagessen/kraken/blob/992fb0bc915e689fc76fa6b021e364c3f0f17ca3/kraken/ketos/recognition.py#L38"&gt;batch size&lt;/a&gt;&lt;sup id="fnref:batch_size"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:batch_size"&gt;6&lt;/a&gt;&lt;/sup&gt; and the &lt;a href="https://github.com/mittagessen/kraken/blob/992fb0bc915e689fc76fa6b021e364c3f0f17ca3/kraken/ketos/recognition.py#L79"&gt;single point precision&lt;/a&gt;&lt;sup id="fnref:precision"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:precision"&gt;7&lt;/a&gt;&lt;/sup&gt;. He found that for training a recognition model with a batch size of 8 and either 32 or 16 of precision, less than 5 Gb of RAM on the GPU is enough. With a batch size of 1 and a precision of 32, it's even less than 1 Gb. To train a segmentation model, less than 10Gb is enough, and this type of training is more rare. Since our goal for the infrastructure is not to maximize the speed of the training but to maximize the amount of possible parallel training jobs at decent speed, we decided that 10 vGPUs with 5Gb of RAM and 2 vGPUs with 10Gb of RAM were a good compromise. If we find that more GPU RAM is occasionally needed, we still have two times 24Gb with the RTX6000!&lt;/p&gt;
&lt;h3&gt;Should you build your own server?&lt;/h3&gt;
&lt;p&gt;We have spent all this time writing about how to build, how to spec out your server or your infrastructure, but let's talk about the elephant in the room: should you do it?&lt;/p&gt;
&lt;p&gt;Well, it's all a matter of perspectives. We'd say it probably makes sense if:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You are a very big organization, you have a lot of money available to you, a super-cluster (and possibly a well staffed IT services department), and you have a high demand;&lt;/li&gt;
&lt;li&gt;You are working on very sensitive data that can't be shared with the outside (&lt;em&gt;e.g.&lt;/em&gt; medical reports);&lt;/li&gt;
&lt;li&gt;You are geographically far away from any other existing server, and face latency issues when you connect to potential welcoming servers;&lt;/li&gt;
&lt;li&gt;Servers that exist around you are reluctant to onboard you and the teams behind the request for a server of your own.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These four points are definitely valid. But we'd say that, if you are in another situation, sharing infrastructural costs probably makes way more sense. In our experience, building a server is long, tedious, require special (and rare) skills&lt;sup id="fnref:sysadmin"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/017/#fn:sysadmin"&gt;8&lt;/a&gt;&lt;/sup&gt; and costly (in terms of human resources as well!). Setting up a working server can take a really long time. For CREMMA, we ended up outsourcing part of the installation of the new infrastructure because we realized that we did not have the time nor the skills to set everything up ourselves. The cost of this installation by a third-party? Between 8 and 12K‚Ç¨, and again, a little time and bandwidth on our end.&lt;/p&gt;
&lt;p&gt;Next you have the maintenance fees. You can outsource them, for a little bill from a company which would make sure that everything is installed on time, that updates work well, etc. Or you can do the maintenance yourself. But again, this comes with a cost: human time. A worker on the server goes down? You are in for a few hours. Some people crashed a third-party server by uploading too much IIIF images on your instance of eScriptorium? Well, then you will not only receive emails from these third parties (and this is completely normal), but also have to deal with your user base doing things that eScriptorium allows and that you may not (yet) be able to control/limit.&lt;/p&gt;
&lt;p&gt;In the end, we would definitely recommend that, when this is possible, you first consider joining existing servers, including by offering &lt;em&gt;quid pro quo&lt;/em&gt; by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Participating in covering the salary of people maintaining the server (through some kind of yearly fees for example);&lt;/li&gt;
&lt;li&gt;Providing some money to expand the existing infrastructure (to increase storage or computation, etc);&lt;/li&gt;
&lt;li&gt;In general, helping eScriptorium grow, discussing with the owners of the server you are joining and/or the eScriptorium team about what kind of new functionality should be added, and if you can contribute to fund these updates.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This final point is super important: sure, owning your own server sounds appealing, even if it is costly to put in place. However, developing eScriptorium also comes with expenses. Thus, participating in eScriptorium directly -- we think -- is also very beneficial and welcome by the developing team. Open-source is free to use, free of charge but is not appearing out of thin air: developing costs money. And the more people participate in infrastructural costs (servers or software), the better the experience will be.&lt;/p&gt;
&lt;!-- footnotes --&gt;

&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:localhost"&gt;
&lt;p&gt;If you don't know anything about local servers and are curious to learn more, you can check this page: &lt;a href="https://www.freecodecamp.org/news/what-is-localhost/"&gt;https://www.freecodecamp.org/news/what-is-localhost/&lt;/a&gt;. Or you can also take a look at the corresponding &lt;a href="https://en.wikipedia.org/wiki/Localhost"&gt;entry&lt;/a&gt; in Wikipedia!¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:localhost" title="Jump back to footnote 1 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:lovewarref"&gt;
&lt;p&gt;The full reference is: Jean-Baptiste Camps, Nicolas Baumard, Pierre-Carl Langlais, Olivier Morin, Thibault Cl√©rice, et al.. Make Love or War? Monitoring the Thematic Evolution of Medieval French Narratives. Computational Humanities Research (CHR 2023), Dec 2023, Paris, France. ‚ü®hal-04250657‚ü©¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:lovewarref" title="Jump back to footnote 2 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:temporary"&gt;
&lt;p&gt;By temporary, we don't mean that the image file are stored for a few hours only, on the contrary, they can stay on the disk for many years. We mean that it should be ok to consider that they can be erased whenever a user is done working on a corpus and has moved away from the transcription phase.¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:temporary" title="Jump back to footnote 3 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:cremma"&gt;
&lt;p&gt;From now on, "CREMMA" means the server created through the CREMMA project.¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:cremma" title="Jump back to footnote 4 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:storage"&gt;
&lt;p&gt;Safety first! We have 38 Tb available, but there is actually a little more  physically because we have redundancy and spare. We have 2 series of disks working with redundancy (&lt;a href="https://openzfs.github.io/openzfs-docs/Basic%20Concepts/RAIDZ.html"&gt;RaidZ&lt;/a&gt;). In each series two disks are entirely dedicated to redundancy only, and one more is completely unused until something fails (it is used as a safety spare disk). While &lt;code&gt;CREMMA_STORAGE&lt;/code&gt;, as we said before, is not used as a permanent storage solution, it needs to be a little bit safe for the user base.¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:storage" title="Jump back to footnote 5 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:batch_size"&gt;
&lt;p&gt;To understand what the batch size corresponds to and why it is important, you can check this entry in the Stack Exchange forum: &lt;a href="https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network"&gt;https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network&lt;/a&gt;.¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:batch_size" title="Jump back to footnote 6 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:precision"&gt;
&lt;p&gt;To quote &lt;a href="https://kraken.re/main/ketos.html#recognition-model-training"&gt;Kraken's documentation&lt;/a&gt;: "When using an Nvidia GPU, set the --precision option to 16 to use automatic mixed precision (AMP). This can provide significant speedup without any loss in accuracy." Kraken's default value for precision is 32.¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:precision" title="Jump back to footnote 7 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:sysadmin"&gt;
&lt;p&gt;It can be difficult to justify hiring a full-time or even part-time system administrator for a team because it is a very specialized and highly demanded type of profile. For example, public organizations can rarely offer competitive salaries compared to the private sector. In addition, the workload for administrating a web server can be irregular, and it can be difficult to make the skills for system administration meet with other needs faced by a team, complicating even more offering a meaningful full-time job.¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/017/#fnref:sysadmin" title="Jump back to footnote 8 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>CREMMA</category><category>eScriptorium</category><category>HTR</category><category>kraken</category><guid>https://alix-tz.github.io/phd/posts/017/</guid><pubDate>Fri, 22 Dec 2023 13:43:54 GMT</pubDate></item><item><title>014 - RT(F)M for the Peraire Experiment</title><link>https://alix-tz.github.io/phd/posts/014/</link><dc:creator>Alix Chagu√©</dc:creator><description>&lt;p&gt;Turns out, there is more to say on last week's &lt;a href="https://alix-tz.github.io/phd/posts/013/"&gt;experiments on the Peraire dataset&lt;/a&gt;! And I found out while I was working on a completely different dataset. Let me explain!&lt;/p&gt;
&lt;p&gt;This morning, I helped my colleague train a Kraken transcription model for &lt;a href="https://ecrituresnumeriques.ca/fr/Activites/Projets/2016/1/19/Anthologie-grecque"&gt;Greek manuscripts&lt;/a&gt;. They gave me the ground truth and I set and executed the training from the command line. It gave me an opportunity to try fine-tuning a model like &lt;a href="https://zenodo.org/record/7234166"&gt;CREMMA Medieval&lt;/a&gt;, in stead of only training from scratch. &lt;strong&gt;CREMMA Medieval&lt;/strong&gt; was trained on manuscripts written in Latin, whereas the Greek manuscripts were written only, well, in Ancient Greek. I didn't want the resulting model to add Latin letters in the transcription when applied to other Greek documents, so I used Kraken's option to allow the model to forget previously learned characters and to force it to only remember the characters contained in the new training data. This option is called &lt;strong&gt;&lt;code&gt;--resize&lt;/code&gt;&lt;/strong&gt; (check the documentation &lt;a href="https://github.com/mittagessen/kraken/blob/4.3.7/docs/ketos.rst#fine-tuning"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;When I fine-tune a model, I usually follow Kraken's recommendations and keep both the previously learned characters and the new ones coming from the new set of ground truth. When this morning I checked what is the keyword to use to keep only the characters from the new dataset, I realized that I didn't correctly set the training on Peraire last week. I had set it to only keep the new characters!&lt;/p&gt;
&lt;p&gt;Up until Kraken v. 4.3.10, &lt;strong&gt;&lt;code&gt;--resize&lt;/code&gt;&lt;/strong&gt; can take the keywords &lt;strong&gt;&lt;code&gt;both&lt;/code&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;code&gt;add&lt;/code&gt;&lt;/strong&gt;. The ambiguity of these keywords &lt;a href="https://github.com/mittagessen/kraken/issues/478"&gt;has been discussed&lt;/a&gt; in the past, which is the reason why starting from Kraken v. 4.3.10, the keywords respectively become &lt;strong&gt;&lt;code&gt;new&lt;/code&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;code&gt;union&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let's quote the manual:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are two modes dealing with mismatching alphabets, &lt;strong&gt;add&lt;/strong&gt; and &lt;strong&gt;both&lt;/strong&gt;. &lt;strong&gt;add&lt;/strong&gt; resizes the output layer and codec of the loaded model to include all characters in the new training set without removing any characters. &lt;strong&gt;both&lt;/strong&gt; will make the resulting model an exact match with the new training set by both removing unused characters from the model and adding new ones.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I fell for this trap of ambiguity and used &lt;strong&gt;both&lt;/strong&gt; instead of &lt;strong&gt;add&lt;/strong&gt;, thinking &lt;strong&gt;both&lt;/strong&gt; meant I was keep &lt;em&gt;both&lt;/em&gt; character sets. (Again this is the very reason why the keywords were recently changed).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Side note: you should really read &lt;a href="https://alix-tz.github.io/phd/posts/013/"&gt;last week's post&lt;/a&gt; to fully understand the rest of this post!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;At the end of my post last week, I wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;peraire_D&lt;/strong&gt; on the other hand seems to lose it completely on the B series. This is most likely due to the fact that the contrast between the page and the "ink" is too low in the pencil-written series compared to the data used to train &lt;strong&gt;Manu McFrench&lt;/strong&gt; and in the D series. &lt;strong&gt;peraire_D&lt;/strong&gt; even loses 11 points of accuracy to &lt;strong&gt;Manu McFrench&lt;/strong&gt;!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But how could I be sure that it was not actually due to the fact that the model had unlearned some precious characters?&lt;/p&gt;
&lt;p&gt;The only way to know, I thought, was to re-train the models! I used this opportunity to also train the models from scratch because I was curious to see how much noise/improvement was brought by the base model.&lt;/p&gt;
&lt;p&gt;I tried 4 types of models and, like last week, used &lt;a href="https://github.com/WHaverals/CERberus"&gt;CERberus üê∂üê∂üê∂&lt;/a&gt; to measure the character error rates on the predictions made on the test sets:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Models trained "from scratch"&lt;/li&gt;
&lt;li&gt;A model not trained on any data coming from the Peraire dataset (aka &lt;a href="https://zenodo.org/record/6657809"&gt;&lt;strong&gt;Manu McFrench&lt;/strong&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Models obtained from finetuning &lt;strong&gt;Manu McFrench&lt;/strong&gt; using the &lt;strong&gt;add&lt;/strong&gt; resize mode&lt;/li&gt;
&lt;li&gt;Models obtained from finetuning &lt;strong&gt;Manu McFrench&lt;/strong&gt; using the &lt;strong&gt;both&lt;/strong&gt; resize mode&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For each model trained on the Peraire dataset, I used 3 compositions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the full dataset ("ALL")&lt;/li&gt;
&lt;li&gt;only data coming from the B series ("B")&lt;/li&gt;
&lt;li&gt;only data coming from the D series ("D")&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I used the same composition system for the test sets.&lt;/p&gt;
&lt;p&gt;Here are my results in the form of a table:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/peraire_scores.png" alt="a table of the scored obtained on the different train set, test set and resize configurations" widht="400px"&gt;&lt;/p&gt;
&lt;p&gt;Fortunately, it seems that my previous interpretation is not fully contradicted by the results I obtain with this second series of training. Let's focus on two observations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Whenever a model is trained only on the D series, and tested only on the B series, it appears to be completely incapable of predicting anything but gibberish, losing between 32 and 35 points of accuracy. It confirms that the aspect of the documents from the two series are too different. On the other hand, when the model is fine-tuned on the B series only, it maintains a fairly good accuracy when applied to the D series, whichever resize mode is used. I think it confirms that the B series is enough for the model to learn some sort of formal features from Peraire's handwriting, which the models can transfer to documents written with a different writing instrument.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is very interesting is the difference between the models trained on the whole datasets and tested on the B series: when we use the &lt;strong&gt;both&lt;/strong&gt; resize mode (meaning we only keep the characters from the new dataset), the model is very good. On the contrary, the performance of the model trained with the &lt;strong&gt;add&lt;/strong&gt; resize mode (meaning we keep the output layer and the codec from the base model and add the new characters) is as bad as with a model trained only on the D series.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my previous post, I wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;peraire_both&lt;/strong&gt; is able to generalize from seeing both datasets and even benefits from seeing more data thanks to the D series, since it performs better on the B series compared to &lt;strong&gt;peraire_B&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, in the light of my experiment with the &lt;strong&gt;resize&lt;/strong&gt; option, I think this is not correct. Instead, it appears that resetting the output layer by using &lt;strong&gt;both&lt;/strong&gt; (or &lt;strong&gt;new&lt;/strong&gt;) on accident, allowed the model to better take into account the data from the B series (pencil). Contrary to what I observed last week, the model trained on the whole dataset but this time with the &lt;strong&gt;add&lt;/strong&gt; resize mode (or &lt;strong&gt;union&lt;/strong&gt;) doesn't benefit from seeing more data compared to the model trained only on the B series.&lt;/p&gt;
&lt;p&gt;My understanding is that keeping the output layer from the base model with &lt;strong&gt;add&lt;/strong&gt; (or &lt;strong&gt;union&lt;/strong&gt;) probably drowns the specificity of the pencil-written documents into a base knowledge tailored to handle documents with a high contrast (like the ones in the D series &lt;em&gt;and&lt;/em&gt; in &lt;strong&gt;Manu McFrench&lt;/strong&gt;'s training set). Or, to put it differently, when we use &lt;strong&gt;both&lt;/strong&gt; (or &lt;strong&gt;new&lt;/strong&gt;), more attention is given to the pencil written documents, meaning that the model actually gets better at handling this category of data.&lt;/p&gt;
&lt;p&gt;I am extremely curious to see how I can investigate this further, or if any of you, readers, would understand these results differently!&lt;/p&gt;</description><category>experiment</category><category>HTR</category><category>kraken</category><guid>https://alix-tz.github.io/phd/posts/014/</guid><pubDate>Fri, 04 Aug 2023 17:51:14 GMT</pubDate></item><item><title>013 - The Peraire experiment</title><link>https://alix-tz.github.io/phd/posts/013/</link><dc:creator>Alix Chagu√©</dc:creator><description>&lt;p&gt;&lt;em&gt;WARNING: in my &lt;a href="https://alix-tz.github.io/phd/posts/014/"&gt;next post&lt;/a&gt;, I nuance the conclusions drawn in this post, because of a parameter I didn't correctly set during the training of the models described below. You should really read it after reading this post, to get the full picture!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As a small side project during my phD, I have been sharing my expertise (and a bit of my workforce) with the members of the &lt;a href="https://www.pamir.fr/projets-soutenus/spe-vlp/"&gt;DIM SPE-VLP&lt;/a&gt; project. The acronym stands for "Sauver le patrimoine esp√©rantiste : le voyage de Lucien P√©raire (1928-1932)." The project revolves around the digitization, transcription and edition/valorization of &lt;a href="https://fr.wikipedia.org/wiki/Lucien_P%C3%A9raire"&gt;Lucien Peraire&lt;/a&gt;'s archives. He was a French citizen who, in the late 1920s, travelled across the European and the Asian continents, mostly by bike and using &lt;a href="https://en.wikipedia.org/wiki/Esperanto"&gt;Esperanto&lt;/a&gt; to communicate. He kept a diary during his journey (and later published a book about his adventures). His notes are written both in French and in Esperanto and in some documents, he also used &lt;a href="https://en.wikipedia.org/wiki/Shorthand"&gt;stenography&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My contribution to the project has mostly consisted in helping developing transcription models for the French diaries (although I'm also interested in the shorthand and the esperanto). This meant both helping with the production of ground truth and training &lt;a href="https://kraken.re/"&gt;Kraken&lt;/a&gt; models. This post will briefly explain how the ground truth was created and published, as well as present the models that were trained with it.&lt;/p&gt;
&lt;p&gt;Peraire's notebooks are organized in different series, and each series is divided in ensembles regrouping the pages of a notebook. Each ensemble is named after the countries visited while the notebook was used. For example, notebook 11 in the B series forms one ensemble and covers a part of Peraire's travels in Japan. There are 31 notebooks in the B series. The notebooks of this series are written with a blue pencil on (low quality) school papers. On some pages, the pencil is very faded which makes it hard to read the text, let alone to run a successful segmentation task on the image. On the other hand, the D series gathers notes and comments on the diaries, written at the end of the 1960s. This time the handwriting is much easier to read because Peraire mostly used a blue or black ball-point pen. There are 9 ensembles in this series.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/peraire_handwriting.png" alt="two extracts of Peraire's notebooks side by side, on the left the image is taken from the B series, on the right the image is taken from the D series." widht="600px"&gt;&lt;/p&gt;
&lt;p&gt;One aspect that I find particularly interesting with this dataset is that we have a case where the handwriting is similar but the writing tool is different. It means that it is possible to explore how the writing tools and/or writing supports affect the efficiency of a transcription model. On top of that, all the documents were digitized under the same (good) conditions and by the same people.&lt;/p&gt;
&lt;h3&gt;Segmenting, transcribing, aligning and publishing&lt;/h3&gt;
&lt;p&gt;The first version of the dataset was solely focused on the B series. I selected 1 random page from each ensemble (avoiding to take the first page each time) to compose a train set of 33 files&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/013/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;. On top of that, I selected 4 additional pages from B3, B5, B12 and B18 to compose a fixed test set which would never be used as training data.&lt;/p&gt;
&lt;p&gt;I pre-segmented the images with Kraken's default model before correcting the result manually. At this point, I also applied the &lt;a href="https://segmonto.github.io/"&gt;segmOnto&lt;/a&gt; ontology for the lines and regions&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/013/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;. Because of the &lt;a href="https://raw.githubusercontent.com/alix-tz/peraire-ground-truth/master/data/train/B.2.europe-orientale_0007.jpg"&gt;fading ink&lt;/a&gt;, some words could not be transcribed. In order to avoid complicating the transcription rules, I decided to simply segment out the passages that couldn't be read. On the one hand it simplifies the transcription, but on the other hand, it means that a small portion of my segmented documents cannot be re-used by others to train a segmentation model. Since we were not training a segmentation model, it was an easy decision.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/peraire_faded.png" alt="screenshot showing the segmentation and the transcription panels from eScriptorium where we can see that some lines are broken down into several segments and that some segments were left blank" widht="400px"&gt;&lt;/p&gt;
&lt;p&gt;More recently, it was decided to augment the dataset with examples from the D series because the model trained on the B series was not good enough. This time, Gilles P√©rez, a member of the project, took charge of the transcription. I recommended to create a new sample of 30 to 40 images, so he randomly selected series of 4 continuous pages from each ensemble. The transcription of the corresponding 36 pages was sent to me as a Word document. Therefore, on top of taking care of the segmentation of the images, I also went through an alignment phase during which I verified the order of the lines and copy-pasted the transcription. It took longer than I expected but it allowed me to align the transcription with the rules I had followed when creating the first set. I also picked 4 of the 36 pages to add to the test set.&lt;/p&gt;
&lt;p&gt;The dataset is versioned and published applying the principles and tools we developed withing the frame of &lt;a href="https://htr-united.github.io/"&gt;HTR-United&lt;/a&gt;. I also added illustrated segmentation and transcription guidelines.&lt;/p&gt;
&lt;h3&gt;Testing different dataset configurations to train transcription models&lt;/h3&gt;
&lt;p&gt;As I mentioned before, the goal of these datasets was to create transcription models. Taking the opportunity of the recent update of the dataset, I tried different scenarios.&lt;/p&gt;
&lt;p&gt;I never trained the model from scratch because the dataset is too small to get any sort of usable model. Instead, I used &lt;a href="https://zenodo.org/record/6657809"&gt;&lt;strong&gt;Manu McFrench&lt;/strong&gt;&lt;/a&gt; as a base model, fine-tuned with the Peraire dataset. (We were actually able to use Peraire as an example during the &lt;a href="https://www.conftool.pro/dh2023/index.php?page=browseSessions&amp;amp;form_session=76#paperID690"&gt;DH2023&lt;/a&gt; conference&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/013/#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt; earlier this month to show the usefulness of having this kind of base model). I tested fine-tuning only on the B series, only on the D series or on both the B and the D series. Then I used a B-series-only test set, a D-series-only test set and the full test set to see how the models performed.&lt;/p&gt;
&lt;p&gt;Since I wanted to try it after discovering it during DH2023, I used &lt;a href="https://github.com/WHaverals/CERberus"&gt;CERberus üê∂üê∂üê∂&lt;/a&gt; (I talked about it in my &lt;a href="https://alix-tz.github.io/phd/posts/012/"&gt;last post&lt;/a&gt;) to measure the accuracy of the models on the test sets listed above.&lt;/p&gt;
&lt;p&gt;Like &lt;a href="https://huggingface.co/spaces/lterriel/kami-app"&gt;KaMI&lt;/a&gt;, CERberus takes 2 categories of text input: the reference (aka the ground truth) and the prediction (or the hypothesis made by the model). In order to get the prediction, I loaded my models on eScriptorium, as well as the images and transcription of the test set before applying each model to the documents. This way, all the transcription are predicted with the same segmentation, which comes from the ground truth.&lt;/p&gt;
&lt;p&gt;Here are the results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Manu McFrench&lt;/strong&gt;, before fine-tuning, gets a CER of 26.16% when tested on the whole test set, and a score of 27.19% on the documents from the B series, 25.29% on the D series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_both&lt;/strong&gt;, trained on the B and the D series, gets a CER of 4.63% when tested on the whole test set, but a score of 6.41% on the documents from the B series and 3.54% on the D series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_B&lt;/strong&gt;, trained only on the B series, gets a CER of 8.72% on the whole test set, but a score of 7.12% on test-B and 9.67% on test-D.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_D&lt;/strong&gt;, trained only on the D series, gets an CER of 16.38% on the whole test set, but this is because of the enormous descripancy between its score on each sub test set. It skyrockets to a CER of 38,53% on test-B while going as low as 3.65% on test-D.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of this makes sense, though.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ManuMcFrench&lt;/strong&gt; could not be used without fine-tuning, its error rate on both documents is too high.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_both&lt;/strong&gt; is able to generalize from seeing both datasets and even benefits from seeing more data thanks to the D series, since it performs better on the B series compared to &lt;strong&gt;peraire_B&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_B&lt;/strong&gt; which was trained on the more difficult dataset seems to use the knowledge inherited from &lt;strong&gt;Manu McFrench&lt;/strong&gt; and to have learned some formal features from Peraire's handwriting since it is able to maintain a fairly low CER on the D series (it gains 16 points of accuracy compared to &lt;strong&gt;Manu McFrench&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_D&lt;/strong&gt; on the other hand seems to lose it completely on the B series. This is most likely due to the fact that the contrast between the page and the "ink" is too low in the pencil-written series compared to the data used to train &lt;strong&gt;Manu McFrench&lt;/strong&gt; and in the D series. &lt;strong&gt;peraire_D&lt;/strong&gt; even loses 11 points of accuracy to &lt;strong&gt;Manu McFrench&lt;/strong&gt;!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What happens with &lt;strong&gt;peraire_D&lt;/strong&gt; is very interesting because it confirms that it is useful to compose a train set with examples of more difficult documents instead of only showing the ones that are easy to read! Now, the nice thing is that I will soon be working on a little experiment with my colleague Hugo Scheithauer where we will be able to measure the impact of the contrast between the ink and the paper. Stay tuned!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT #1: I added the scores obtained by Manu McFrench alone.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT #2: I added a disclaimer at the beginning of the post.&lt;/em&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;I used 2 images from B2 because one of them was extremely faded and I wanted to include some of these extreme cases in the dataset, and 2 images from B30 because it consisted of shorter lines (table of contents) which I found was interesting to include.¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/013/#fnref:1" title="Jump back to footnote 1 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;As described in the documents, I only used the "InterlinearLine" and "DefaultLine" for the lines, and the "MainZone" and "NumberingZone" for the regions.¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/013/#fnref:2" title="Jump back to footnote 2 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;See the submission and the slides on HAL: &lt;a href="https://inria.hal.science/hal-04094241"&gt;https://inria.hal.science/hal-04094241&lt;/a&gt;.¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/013/#fnref:3" title="Jump back to footnote 3 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>experiment</category><category>HTR</category><category>kraken</category><guid>https://alix-tz.github.io/phd/posts/013/</guid><pubDate>Fri, 28 Jul 2023 15:39:18 GMT</pubDate></item></channel></rss>