<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="https://alix-tz.github.io/phd/assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>A research (b)log (Posts about kraken)</title><link>https://alix-tz.github.io/phd/</link><description></description><atom:link href="https://alix-tz.github.io/phd/categories/kraken.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents ¬© 2023 &lt;a href="https://alix-tz.github.io/phd/"&gt;Alix Chagu√©&lt;/a&gt; CC-BY</copyright><lastBuildDate>Fri, 04 Aug 2023 20:59:57 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>014 - RT(F)M for the Peraire Experiment</title><link>https://alix-tz.github.io/phd/posts/014/</link><dc:creator>Alix Chagu√©</dc:creator><description>&lt;p&gt;Turns out, there is more to say on last week's &lt;a href="https://alix-tz.github.io/phd/posts/013/"&gt;experiments on the Peraire dataset&lt;/a&gt;! And I found out while I was working on a completely different dataset. Let me explain!&lt;/p&gt;
&lt;p&gt;This morning, I helped my colleague train a Kraken transcription model for &lt;a href="https://ecrituresnumeriques.ca/fr/Activites/Projets/2016/1/19/Anthologie-grecque"&gt;Greek manuscripts&lt;/a&gt;. They gave me the ground truth and I set and executed the training from the command line. It gave me an opportunity to try fine-tuning a model like &lt;a href="https://zenodo.org/record/7234166"&gt;CREMMA Medieval&lt;/a&gt;, in stead of only training from scratch. &lt;strong&gt;CREMMA Medieval&lt;/strong&gt; was trained on manuscripts written in Latin, whereas the Greek manuscripts were written only, well, in Ancient Greek. I didn't want the resulting model to add Latin letters in the transcription when applied to other Greek documents, so I used Kraken's option to allow the model to forget previously learned characters and to force it to only remember the characters contained in the new training data. This option is called &lt;strong&gt;&lt;code&gt;--resize&lt;/code&gt;&lt;/strong&gt; (check the documentation &lt;a href="https://github.com/mittagessen/kraken/blob/4.3.7/docs/ketos.rst#fine-tuning"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;When I fine-tune a model, I usually follow Kraken's recommendations and keep both the previously learned characters and the new ones coming from the new set of ground truth. When this morning I checked what is the keyword to use to keep only the characters from the new dataset, I realized that I didn't correctly set the training on Peraire last week. I had set it to only keep the new characters!&lt;/p&gt;
&lt;p&gt;Up until Kraken v. 4.3.10, &lt;strong&gt;&lt;code&gt;--resize&lt;/code&gt;&lt;/strong&gt; can take the keywords &lt;strong&gt;&lt;code&gt;both&lt;/code&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;code&gt;add&lt;/code&gt;&lt;/strong&gt;. The ambiguity of these keywords &lt;a href="https://github.com/mittagessen/kraken/issues/478"&gt;has been discussed&lt;/a&gt; in the past, which is the reason why starting from Kraken v. 4.3.10, the keywords respectively become &lt;strong&gt;&lt;code&gt;new&lt;/code&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;code&gt;union&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let's quote the manual:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are two modes dealing with mismatching alphabets, &lt;strong&gt;add&lt;/strong&gt; and &lt;strong&gt;both&lt;/strong&gt;. &lt;strong&gt;add&lt;/strong&gt; resizes the output layer and codec of the loaded model to include all characters in the new training set without removing any characters. &lt;strong&gt;both&lt;/strong&gt; will make the resulting model an exact match with the new training set by both removing unused characters from the model and adding new ones.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I fell for this trap of ambiguity and used &lt;strong&gt;both&lt;/strong&gt; instead of &lt;strong&gt;add&lt;/strong&gt;, thinking &lt;strong&gt;both&lt;/strong&gt; meant I was keep &lt;em&gt;both&lt;/em&gt; character sets. (Again this is the very reason why the keywords were recently changed).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Side note: you should really read &lt;a href="https://alix-tz.github.io/phd/posts/013/"&gt;last week's post&lt;/a&gt; to fully understand the rest of this post!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;At the end of my post last week, I wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;peraire_D&lt;/strong&gt; on the other hand seems to lose it completely on the B series. This is most likely due to the fact that the contrast between the page and the "ink" is too low in the pencil-written series compared to the data used to train &lt;strong&gt;Manu McFrench&lt;/strong&gt; and in the D series. &lt;strong&gt;peraire_D&lt;/strong&gt; even loses 11 points of accuracy to &lt;strong&gt;Manu McFrench&lt;/strong&gt;!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But how could I be sure that it was not actually due to the fact that the model had unlearned some precious characters?&lt;/p&gt;
&lt;p&gt;The only way to know, I thought, was to re-train the models! I used this opportunity to also train the models from scratch because I was curious to see how much noise/improvement was brought by the base model.&lt;/p&gt;
&lt;p&gt;I tried 4 types of models and, like last week, used &lt;a href="https://github.com/WHaverals/CERberus"&gt;CERberus üê∂üê∂üê∂&lt;/a&gt; to measure the character error rates on the predictions made on the test sets:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Models trained "from scratch"&lt;/li&gt;
&lt;li&gt;A model not trained on any data coming from the Peraire dataset (aka &lt;a href="https://zenodo.org/record/6657809"&gt;&lt;strong&gt;Manu McFrench&lt;/strong&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Models obtained from finetuning &lt;strong&gt;Manu McFrench&lt;/strong&gt; using the &lt;strong&gt;add&lt;/strong&gt; resize mode&lt;/li&gt;
&lt;li&gt;Models obtained from finetuning &lt;strong&gt;Manu McFrench&lt;/strong&gt; using the &lt;strong&gt;both&lt;/strong&gt; resize mode&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For each model trained on the Peraire dataset, I used 3 compositions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the full dataset ("ALL")&lt;/li&gt;
&lt;li&gt;only data coming from the B series ("B")&lt;/li&gt;
&lt;li&gt;only data coming from the D series ("D")&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I used the same composition system for the test sets.&lt;/p&gt;
&lt;p&gt;Here are my results in the form of a table:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/peraire_scores.png" alt="a table of the scored obtained on the different train set, test set and resize configurations" widht="400px"&gt;&lt;/p&gt;
&lt;p&gt;Fortunately, it seems that my previous interpretation is not fully contradicted by the results I obtain with this second series of training. Let's focus on two observations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Whenever a model is trained only on the D series, and tested only on the B series, it appears to be completely incapable of predicting anything but gibberish, losing between 32 and 35 points of accuracy. It confirms that the aspect of the documents from the two series are too different. On the other hand, when the model is fine-tuned on the B series only, it maintains a fairly good accuracy when applied to the D series, whichever resize mode is used. I think it confirms that the B series is enough for the model to learn some sort of formal features from Peraire's handwriting, which the models can transfer to documents written with a different writing instrument.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is very interesting is the difference between the models trained on the whole datasets and tested on the B series: when we use the &lt;strong&gt;both&lt;/strong&gt; resize mode (meaning we only keep the characters from the new dataset), the model is very good. On the contrary, the performance of the model trained with the &lt;strong&gt;add&lt;/strong&gt; resize mode (meaning we keep the output layer and the codec from the base model and add the new characters) is as bad as with a model trained only on the D series.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my previous post, I wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;peraire_both&lt;/strong&gt; is able to generalize from seeing both datasets and even benefits from seeing more data thanks to the D series, since it performs better on the B series compared to &lt;strong&gt;peraire_B&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, in the light of my experiment with the &lt;strong&gt;resize&lt;/strong&gt; option, I think this is not correct. Instead, it appears that resetting the output layer by using &lt;strong&gt;both&lt;/strong&gt; (or &lt;strong&gt;new&lt;/strong&gt;) on accident, allowed the model to better take into account the data from the B series (pencil). Contrary to what I observed last week, the model trained on the whole dataset but this time with the &lt;strong&gt;add&lt;/strong&gt; resize mode (or &lt;strong&gt;union&lt;/strong&gt;) doesn't benefit from seeing more data compared to the model trained only on the B series.&lt;/p&gt;
&lt;p&gt;My understanding is that keeping the output layer from the base model with &lt;strong&gt;add&lt;/strong&gt; (or &lt;strong&gt;union&lt;/strong&gt;) probably drowns the specificity of the pencil-written documents into a base knowledge tailored to handle documents with a high contrast (like the ones in the D series &lt;em&gt;and&lt;/em&gt; in &lt;strong&gt;Manu McFrench&lt;/strong&gt;'s training set). Or, to put it differently, when we use &lt;strong&gt;both&lt;/strong&gt; (or &lt;strong&gt;new&lt;/strong&gt;), more attention is given to the pencil written documents, meaning that the model actually gets better at handling this category of data.&lt;/p&gt;
&lt;p&gt;I am extremely curious to see how I can investigate this further, or if any of you, readers, would understand these results differently!&lt;/p&gt;</description><category>experiment</category><category>HTR</category><category>kraken</category><guid>https://alix-tz.github.io/phd/posts/014/</guid><pubDate>Fri, 04 Aug 2023 17:51:14 GMT</pubDate></item><item><title>013 - The Peraire experiment</title><link>https://alix-tz.github.io/phd/posts/013/</link><dc:creator>Alix Chagu√©</dc:creator><description>&lt;p&gt;As a small side project during my phD, I have been sharing my expertise (and a bit of my workforce) with the members of the &lt;a href="https://www.pamir.fr/projets-soutenus/spe-vlp/"&gt;DIM SPE-VLP&lt;/a&gt; project. The acronym stands for "Sauver le patrimoine esp√©rantiste : le voyage de Lucien P√©raire (1928-1932)." The project revolves around the digitization, transcription and edition/valorization of &lt;a href="https://fr.wikipedia.org/wiki/Lucien_P%C3%A9raire"&gt;Lucien Peraire&lt;/a&gt;'s archives. He was a French citizen who, in the late 1920s, travelled across the European and the Asian continents, mostly by bike and using &lt;a href="https://en.wikipedia.org/wiki/Esperanto"&gt;Esperanto&lt;/a&gt; to communicate. He kept a diary during his journey (and later published a book about his adventures). His notes are written both in French and in Esperanto and in some documents, he also used &lt;a href="https://en.wikipedia.org/wiki/Shorthand"&gt;stenography&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My contribution to the project has mostly consisted in helping developing transcription models for the French diaries (although I'm also interested in the shorthand and the esperanto). This meant both helping with the production of ground truth and training &lt;a href="https://kraken.re/"&gt;Kraken&lt;/a&gt; models. This post will briefly explain how the ground truth was created and published, as well as present the models that were trained with it.&lt;/p&gt;
&lt;p&gt;Peraire's notebooks are organized in different series, and each series is divided in ensembles regrouping the pages of a notebook. Each ensemble is named after the countries visited while the notebook was used. For example, notebook 11 in the B series forms one ensemble and covers a part of Peraire's travels in Japan. There are 31 notebooks in the B series. The notebooks of this series are written with a blue pencil on (low quality) school papers. On some pages, the pencil is very faded which makes it hard to read the text, let alone to run a successful segmentation task on the image. On the other hand, the D series gathers notes and comments on the diaries, written at the end of the 1960s. This time the handwriting is much easier to read because Peraire mostly used a blue or black ball-point pen. There are 9 ensembles in this series.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/peraire_handwriting.png" alt="two extracts of Peraire's notebooks side by side, on the left the image is taken from the B series, on the right the image is taken from the D series." widht="600px"&gt;&lt;/p&gt;
&lt;p&gt;One aspect that I find particularly interesting with this dataset is that we have a case where the handwriting is similar but the writing tool is different. It means that it is possible to explore how the writing tools and/or writing supports affect the efficiency of a transcription model. On top of that, all the documents were digitized under the same (good) conditions and by the same people.&lt;/p&gt;
&lt;h3&gt;Segmenting, transcribing, aligning and publishing&lt;/h3&gt;
&lt;p&gt;The first version of the dataset was solely focused on the B series. I selected 1 random page from each ensemble (avoiding to take the first page each time) to compose a train set of 33 files&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/013/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;. On top of that, I selected 4 additional pages from B3, B5, B12 and B18 to compose a fixed test set which would never be used as training data.&lt;/p&gt;
&lt;p&gt;I pre-segmented the images with Kraken's default model before correcting the result manually. At this point, I also applied the &lt;a href="https://segmonto.github.io/"&gt;segmOnto&lt;/a&gt; ontology for the lines and regions&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/013/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;. Because of the &lt;a href="https://raw.githubusercontent.com/alix-tz/peraire-ground-truth/master/data/train/B.2.europe-orientale_0007.jpg"&gt;fading ink&lt;/a&gt;, some words could not be transcribed. In order to avoid complicating the transcription rules, I decided to simply segment out the passages that couldn't be read. On the one hand it simplifies the transcription, but on the other hand, it means that a small portion of my segmented documents cannot be re-used by others to train a segmentation model. Since we were not training a segmentation model, it was an easy decision.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/peraire_faded.png" alt="screenshot showing the segmentation and the transcription panels from eScriptorium where we can see that some lines are broken down into several segments and that some segments were left blank" widht="400px"&gt;&lt;/p&gt;
&lt;p&gt;More recently, it was decided to augment the dataset with examples from the D series because the model trained on the B series was not good enough. This time, Gilles P√©rez, a member of the project, took charge of the transcription. I recommended to create a new sample of 30 to 40 images, so he randomly selected series of 4 continuous pages from each ensemble. The transcription of the corresponding 36 pages was sent to me as a Word document. Therefore, on top of taking care of the segmentation of the images, I also went through an alignment phase during which I verified the order of the lines and copy-pasted the transcription. It took longer than I expected but it allowed me to align the transcription with the rules I had followed when creating the first set. I also picked 4 of the 36 pages to add to the test set.&lt;/p&gt;
&lt;p&gt;The dataset is versioned and published applying the principles and tools we developed withing the frame of &lt;a href="https://htr-united.github.io/"&gt;HTR-United&lt;/a&gt;. I also added illustrated segmentation and transcription guidelines.&lt;/p&gt;
&lt;h3&gt;Testing different dataset configurations to train transcription models&lt;/h3&gt;
&lt;p&gt;As I mentioned before, the goal of these datasets was to create transcription models. Taking the opportunity of the recent update of the dataset, I tried different scenarios.&lt;/p&gt;
&lt;p&gt;I never trained the model from scratch because the dataset is too small to get any sort of usable model. Instead, I used &lt;a href="https://zenodo.org/record/6657809"&gt;&lt;strong&gt;Manu McFrench&lt;/strong&gt;&lt;/a&gt; as a base model, fine-tuned with the Peraire dataset. (We were actually able to use Peraire as an example during the &lt;a href="https://www.conftool.pro/dh2023/index.php?page=browseSessions&amp;amp;form_session=76#paperID690"&gt;DH2023&lt;/a&gt; conference&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/013/#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt; earlier this month to show the usefulness of having this kind of base model). I tested fine-tuning only on the B series, only on the D series or on both the B and the D series. Then I used a B-series-only test set, a D-series-only test set and the full test set to see how the models performed.&lt;/p&gt;
&lt;p&gt;Since I wanted to try it after discovering it during DH2023, I used &lt;a href="https://github.com/WHaverals/CERberus"&gt;CERberus üê∂üê∂üê∂&lt;/a&gt; (I talked about it in my &lt;a href="https://alix-tz.github.io/phd/posts/012/"&gt;last post&lt;/a&gt;) to measure the accuracy of the models on the test sets listed above.&lt;/p&gt;
&lt;p&gt;Like &lt;a href="https://huggingface.co/spaces/lterriel/kami-app"&gt;KaMI&lt;/a&gt;, CERberus takes 2 categories of text input: the reference (aka the ground truth) and the prediction (or the hypothesis made by the model). In order to get the prediction, I loaded my models on eScriptorium, as well as the images and transcription of the test set before applying each model to the documents. This way, all the transcription are predicted with the same segmentation, which comes from the ground truth.&lt;/p&gt;
&lt;p&gt;Here are the results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Manu McFrench&lt;/strong&gt;, before fine-tuning, gets a CER of 26.16% when tested on the whole test set, and a score of 27.19% on the documents from the B series, 25.29% on the D series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_both&lt;/strong&gt;, trained on the B and the D series, gets a CER of 4.63% when tested on the whole test set, but a score of 6.41% on the documents from the B series and 3.54% on the D series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_B&lt;/strong&gt;, trained only on the B series, gets a CER of 8.72% on the whole test set, but a score of 7.12% on test-B and 9.67% on test-D.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_D&lt;/strong&gt;, trained only on the D series, gets an CER of 16.38% on the whole test set, but this is because of the enormous descripancy between its score on each sub test set. It skyrockets to a CER of 38,53% on test-B while going as low as 3.65% on test-D.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of this makes sense, though.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ManuMcFrench&lt;/strong&gt; could not be used without fine-tuning, its error rate on both documents is too high.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_both&lt;/strong&gt; is able to generalize from seeing both datasets and even benefits from seeing more data thanks to the D series, since it performs better on the B series compared to &lt;strong&gt;peraire_B&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_B&lt;/strong&gt; which was trained on the more difficult dataset seems to use the knowledge inherited from &lt;strong&gt;Manu McFrench&lt;/strong&gt; and to have learned some formal features from Peraire's handwriting since it is able to maintain a fairly low CER on the D series (it gains 16 points of accuracy compared to &lt;strong&gt;Manu McFrench&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_D&lt;/strong&gt; on the other hand seems to lose it completely on the B series. This is most likely due to the fact that the contrast between the page and the "ink" is too low in the pencil-written series compared to the data used to train &lt;strong&gt;Manu McFrench&lt;/strong&gt; and in the D series. &lt;strong&gt;peraire_D&lt;/strong&gt; even loses 11 points of accuracy to &lt;strong&gt;Manu McFrench&lt;/strong&gt;!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What happens with &lt;strong&gt;peraire_D&lt;/strong&gt; is very interesting because it confirms that it is useful to compose a train set with examples of more difficult documents instead of only showing the ones that are easy to read! Now, the nice thing is that I will soon be working on a little experiment with my colleague Hugo Scheithauer where we will be able to measure the impact of the contrast between the ink and the paper. Stay tuned!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT: I added the scores obtained by Manu McFrench alone.&lt;/em&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;I used 2 images from B2 because one of them was extremely faded and I wanted to include some of these extreme cases in the dataset, and 2 images from B30 because it consisted of shorter lines (table of contents) which I found was interesting to include.¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/013/#fnref:1" title="Jump back to footnote 1 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;As described in the documents, I only used the "InterlinearLine" and "DefaultLine" for the lines, and the "MainZone" and "NumberingZone" for the regions.¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/013/#fnref:2" title="Jump back to footnote 2 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;See the submission and the slides on HAL: &lt;a href="https://inria.hal.science/hal-04094241"&gt;https://inria.hal.science/hal-04094241&lt;/a&gt;.¬†&lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/013/#fnref:3" title="Jump back to footnote 3 in the text"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>experiment</category><category>HTR</category><category>kraken</category><guid>https://alix-tz.github.io/phd/posts/013/</guid><pubDate>Fri, 28 Jul 2023 15:39:18 GMT</pubDate></item></channel></rss>