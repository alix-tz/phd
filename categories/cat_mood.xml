<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="https://alix-tz.github.io/phd/assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>A research (b)log (Posts about mood)</title><link>https://alix-tz.github.io/phd/</link><description></description><atom:link href="https://alix-tz.github.io/phd/categories/cat_mood.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents ¬© 2023 &lt;a href="https://alix-tz.github.io/phd/"&gt;Alix Chagu√©&lt;/a&gt; CC-BY</copyright><lastBuildDate>Mon, 17 Jul 2023 12:34:47 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>012 - "It did a very good job"</title><link>https://alix-tz.github.io/phd/posts/012/</link><dc:creator>Alix Chagu√©</dc:creator><description>&lt;p&gt;A few weeks ago, I attended the presentation of an automatic transcription software. The majority of the audience was unfamiliar with the concept of handwritten text recognition (HTR) or had little experience using it. The presentation lasted only an hour, so it couldn't delve into much detail. Its main objective was to demonstrate the software's results. The presenter showed several slides, displaying on one side  images of manuscripts (often in a language unknown to the audience) and on the other side the transcriptions generated by the software. Throughout the presentation, the presenter repeatedly commented on the HTR software saying that "it did a very good job."&lt;/p&gt;
&lt;p&gt;But what does it even mean?&lt;/p&gt;
&lt;p&gt;The very first aspect to explore is what distinguishes a good job from a bad one. Normally, such an evaluation relies on the measurement of the accuracy of the result compared to the ideal transcription. The accuracy can be expressed positively or negatively using the error rates (a 0% error rate is the same as a 100% accuracy).&lt;/p&gt;
&lt;p&gt;Measuring the accuracy of a prediction (another way to call the result of HTR) is commonly done at character level. The character accuracy of a model is equal to the number of matches between the prediction and the ideal transcription. The character error rate (CER) is a very common measure to express a model's theoretical efficiency.&lt;/p&gt;
&lt;p&gt;Sometimes softwares also consider the word error rate (WER), which is the proportion of words in the prediction containing errors. A high score at WER doesn't actually mean that the transcription is bad. It only means that the errors are distributed on all the words. I never use WER alone because it is hard to get an exact impression of the quality of the prediction based on that metric alone.&lt;/p&gt;
&lt;p&gt;There is a paper from &lt;a href="https://dl.acm.org/doi/10.1145/3476887.3476888"&gt;Neudecker et al. (2021)&lt;/a&gt; where they test 5 different software used for evaluating the prediction. They also develop an interesting reflection on alternative metrics such as the "non-stopword accuracy", the "phrase accuracy", the "flexible character accuracy" (which is useful when the line order isn't always the same), the "figure of merit" (which "aims to quantify the effort required for manual post-correction" (p. 15)) or else the "unordered WER".&lt;/p&gt;
&lt;p&gt;When your score is a rate, there is an implicit idea that 100% is both the maximum score and the targeted score (for accuracy of course). But in the case of HTR, 100% accuracy is extremely rare because there are also edge cases where the way a letter was drawn is ambiguous: in such cases the error is not particularly caused by the inaccuracy of the HTR engine but rather by the imperfection of the handwriting in the first place.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.46"&gt;Hodel et al., (2021)&lt;/a&gt;, the authors provided a grid to interpret accuracy scores. They suggest the following three thresholds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CER &amp;lt; 10% == good (it allows efficient post-processing)&lt;/li&gt;
&lt;li&gt;CER &amp;lt; 5% == very good (errors are usually focused on rare or unknown words)&lt;/li&gt;
&lt;li&gt;CER &amp;lt; 2.5% == excellent (but it is usually only reached when the handwriting is very regular)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Personally, I think this grid should also include 20% and 0%. 20% as a threshold, because at 80% of accuracy, the transcription is supposedly good enough for fuzzy search and keyword spotting (missing reference here...); and 0% because it should be reminded that an accuracy of 100% is virtually impossible.&lt;/p&gt;
&lt;p&gt;To complement this, I would like to mention another possible approach to get an interpretable score: during the DH2023 conference, Thibault Cl√©rice and I &lt;a href="https://inria.hal.science/hal-04094241"&gt;presented an experiment&lt;/a&gt; where we trained a model using the same data in the train set and the test set. Our model reached an accuracy close to 90%, which we were able to use as a baseline to define the highest accuracy score possible for the data we had. Thus we were able to consider that a model approaching 90% of accuracy would be an excellent model, as far as that dataset was concerned.&lt;/p&gt;
&lt;p&gt;Still during &lt;a href="https://www.conftool.pro/dh2023/index.php?page=browseSessions&amp;amp;form_session=76#paperID395"&gt;the DH2023 conference&lt;/a&gt;, Wouter Haverals introduced &lt;a href="https://github.com/WHaverals/CERberus"&gt;CERberus üê∂üê∂üê∂&lt;/a&gt;, a web interface which addresses the same type of issues as &lt;a href="https://huggingface.co/spaces/lterriel/kami-app"&gt;KaMI&lt;/a&gt;: the lack of nuance in a plain CER computation. Indeed, in a CER score, every type of error has the same weight. This means that mistaking an "e" for a "√©" costs the same as mistaking a "e" for a "0": in the first case the text is likely still readable or understandable, whereas in the latter, it might not be the case.&lt;/p&gt;
&lt;p&gt;The CER metric is still very useful, but when applied to transcription projects, it is even more valuable when we can filter the types of errors we want to include in the evaluation.&lt;/p&gt;
&lt;p&gt;*EDIT: I should have noted here that my reflection was focused on the evaluation of an automatic transcription in cases where you already have the expected transcription. When we apply an HTR model to a whole new set of documents, we usually don't have the correct transcription at hand (otherwise we wouldn't use HTR in the first place). This is the reason why many researchers try to find ways to evaluate the quality of the transcription without ground truth. One example can be found in &lt;a href="https://enc.hal.science/hal-03828529"&gt;Cl√©rice (2022)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So, to go back to our initial problem, we can see that there are many ways to draw the line between a good job and a bad one. The threshold will depend on the metric used to express the accuracy of the prediction and also (and actually mostly) on the way the generated text will be used down the line. Even though the software presentation I attended was short, I think we should always remind future users of HTR that 100% of accuracy is not always what they are seeking.&lt;/p&gt;
&lt;p&gt;A short reflection to finish this post: I was bothered by the expression used to qualify the transcription. I am still trying to figure out a way to put it into words. On top of lacking accuracy, the expression "it did a good job" was also calling for a vision of HTR as a magic tool at the service of the searchers and students. But, in which other cases do you say that someone did "a good job?" Likely when you delegate a task to a &lt;a href="https://africanarguments.org/2023/03/the-invisible-labour-of-africa-in-the-digital-revolution/"&gt;subaltern&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I see a problem here: in their current state, HTR engines are efficient but not to the point that people can use them without thinking clearly about what they want the engine to produce. It is easy to sell a software pretending that it is a magic servant that will do all the transcription in your place, a tool so smart that you can even consider delegating a part of your responsibility to it. But I think when new users of HTR fail to first reflect on the outcome they can reasonably expect from these engines, it creates disappointment and crappy data and workflows.&lt;/p&gt;</description><category>accuracy</category><category>evaluation</category><category>HTR</category><category>metrics</category><guid>https://alix-tz.github.io/phd/posts/012/</guid><pubDate>Sat, 15 Jul 2023 12:06:20 GMT</pubDate></item><item><title>003 - The Zen of PhD</title><link>https://alix-tz.github.io/phd/posts/003/</link><dc:creator>Alix Chagu√©</dc:creator><description>&lt;p&gt;Long time no see!  &lt;/p&gt;
&lt;p&gt;This 4th post comes after 2 initial attempts which I ended up never publishing because I didn't feel 100% confident about them. I still don't think they would have brought anything interesting to this research blog, but they were very useful to me. This is the reason why I still decided to post a short text about them. Writing these two drafts allowed me to think certain things through. In fact, they turned out to be funnels to better understand the origins of my frustration, which was mainly caused by the fact that I was under the impression that time was flying by while I was not accomplishing much regarding my research.  &lt;/p&gt;
&lt;p&gt;I initially searched for a culprit in the specific of my situation, and the fact that instead of starting from scratch on a research project, I had been transitioning from a position as a research engineer. Yes, on the one hand, it enabled me to be very familiar with my field of research, but on the other hand, it meant that up until this Spring, I was still spending time on tasks that were linked to my previous missions. Anytime my bandwidth shrank and TO-DO list grew, I felt like I was prevented from "focusing on my PhD."  &lt;/p&gt;
&lt;p&gt;Eventually, the first draft turned into a list of all the projects in which I used to be involved, not just the ones that impacted my schedule. The second draft had me conclude that the actual problem was that I lacked a clear orientation for my research. In the end, I realized that I was indeed responsible for my own frustration. If I don't build a clearer frame within which to organize my work, I can't create landmarks to measure how much I progress. I can't clearly define what it means to be working for or "on my PhD."  &lt;/p&gt;
&lt;p&gt;To be honest, as far as I am concerned, this is easier said than done. I think there is a good part of improvisation and instinct in the way I work. It is satisfying when the outcomes are good, but it is certainly not the most efficient way to use my time. Gaining better working habits, more discipline but also more methodical ways of organizing my experiments is in fact one of my long term objectives. After all, doctoral studies are not all about research, they are also dedicated to training and learning new skills...  &lt;/p&gt;</description><guid>https://alix-tz.github.io/phd/posts/003/</guid><pubDate>Tue, 07 Jun 2022 12:32:05 GMT</pubDate></item></channel></rss>