<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="https://alix-tz.github.io/phd/assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>A research (b)log (Posts by Alix Chagué)</title><link>https://alix-tz.github.io/phd/</link><description></description><atom:link href="https://alix-tz.github.io/phd/authors/alix-chague.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2024 &lt;a href="https://alix-tz.github.io/phd/"&gt;Alix Chagué&lt;/a&gt; CC-BY</copyright><lastBuildDate>Wed, 21 Aug 2024 01:03:02 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>019 - The CATMuS Modern dataset #2</title><link>https://alix-tz.github.io/phd/posts/019/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;&lt;a href="https://x.com/JMFradeRue/status/1730191566508060883"&gt;Previous experiments&lt;/a&gt; have shown that conflicting transcription guidelines in training datasets make it less likely that a model will learn to transcribe correctly. This is particularly relevant when it comes to abbreviations and it's something to keep in mind when merging existing datasets. We didn't really address this when we trained the &lt;a href="https://inria.hal.science/hal-04094241"&gt;Manu McFrench model&lt;/a&gt; because it's difficult to retroactively align datasets to follow the same transcription rules. Unless you can afford to manually check every line, of course. In the case of Manu McFrench however, we only merged datasets that didn't solve abbreviations, so we ensured a minimum of cohesion.&lt;/p&gt;
&lt;p&gt;CATMuS was built on the foundation laid by CREMMALab and the &lt;a href="https://hal.science/hal-03716526"&gt;annotation guidelines&lt;/a&gt; developed by Ariane Pinche at the end of a seminar organized in 2021. These guidelines are intended to be generic, meaning they should be compatible with most transcription situations and are not project-specific. Following these guidelines will help data producers create ground truth that is compatible with data from other projects. It will also help those projects save time by not having to create transcription rules from scratch. From my experience, it is indeed easy for the members of a project discovering HTR to get caught up in the specifics of one project and forget what is and is not relevant (or even complicating) in the transcription phase.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;It's worth mentioning that a project can choose to some of the CATMuS guidelines, while maintaining more specific rules for certain cases. If that's the case, the CATMuS guidelines can (should?) be used as a reference point. Ideally, the specific rules defined by a project should be retro-compatible with CATMuS. For example, if a project decides to use a special character will be used to mark the end of each paragraph, then in order to create a CATMuS-compatible version of the dataset, I should only have to replace or remove that character. In such cases, the special character that was chosen should be unambiguous and the rule should be explicitly presented.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As CREMMALab focused on the transcription of medieval manuscripts, so did the first CATMuS dataset and guidelines. As I said in my &lt;a href="https://alix-tz.github.io/phd/posts/018/"&gt;previous post&lt;/a&gt;, I focused on data covering the modern and contemporary periods, for which there was no equivalent to the CREMMALab guidelines. So, when extending CATMuS to these periods, I started with collecting existing guidelines and comparing them. I used the &lt;a href="https://hal.science/hal-03697382"&gt;CREMMA Medieval guidelines&lt;/a&gt;, the &lt;a href="https://gist.github.com/alix-tz/6f89444521bf1cab0522da520f7e4ff4"&gt;CREMMA guidelines for modern and contemporary documents&lt;/a&gt;, &lt;a href="https://hal.science/hal-04281804"&gt;SETAF's guidelines&lt;/a&gt; and &lt;a href="https://hal.science/hal-04557457"&gt;CATMuS Print's guidelines&lt;/a&gt; as a basis to elaborate the transcription rules for McCATMuS.&lt;/p&gt;
&lt;p&gt;For each rubric, I &lt;a href="https://docs.google.com/spreadsheets/d/1bFE-rRk6ZwgIHqXAOgwPo1s1zwQ-UPTLPnzjaRmTMsk/edit?usp=sharing"&gt;compared&lt;/a&gt; what each set of rules suggested, when they covered it. It was rare for all guidelines to align, but some cases were easy to solve. For example, all the guidelines recommended not to differentiate between regular s (&lt;code&gt;⟨s⟩&lt;/code&gt;) and long s (&lt;code&gt;⟨ſ⟩&lt;/code&gt;), except for the rules I had set for the modern and contemporary sources transcribed by CREMMA in 2021, before the CREMMALab seminar. It was thus decided that for McCATMuS there would be no distinction between all types of s's.&lt;/p&gt;
&lt;p&gt;Some rubrics needed to be discussed to figure out why the rule had been chosen in the first place by some of the projects, to decide which one to keep for McCATMuS. In February, I met with Ariane Pinche and Simon Gabay to go over the rubrics that still needed to be set. One example of a rule we discussed is how hyphenations are handled. CATMuS Medieval and the two CREMMA guidelines say to always use the same symbol (&lt;code&gt;⟨-⟩&lt;/code&gt;), whereas for the SETAF and CATMuS Print datasets, inline hyphenations (&lt;code&gt;⟨-⟩&lt;/code&gt;) are differentiated from hyphenations at the end of a line (&lt;code&gt;⟨¬⟩&lt;/code&gt;). Other symbols, like &lt;code&gt;⟨⸗⟩&lt;/code&gt;, were unanimously rejected.&lt;/p&gt;
&lt;p&gt;Two factors were considered when making those decisions: the feasibility of a retro-conversion for the existing datasets and the compatibility of the rule with a maximum of projects. In the case of hyphenations, I eventually decided to follow the same rule as CATMuS Medieval and CREMMA. On top of simplifying the compatibility of McCATMuS with CATMuS Medieval, I found that replacing all &lt;code&gt;⟨¬⟩&lt;/code&gt; with &lt;code&gt;⟨-⟩&lt;/code&gt;, rather than retroactively place &lt;code&gt;⟨¬⟩&lt;/code&gt; where there was indeed an hyphenation at the end of a line&lt;sup id="fnref:hyphen"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/019/#fn:hyphen"&gt;1&lt;/a&gt;&lt;/sup&gt; was much more straightforward.&lt;/p&gt;
&lt;p&gt;Once the set of rules was fixed, I used it to sort between the different datasets I had identified (I'll discuss this in the next post) and to decide which one would be retained for McCATMuS v1. I also defined the transformation scenarios necessary to turn each of these datasets into a CATMuS-compatible version. Then, once McCATMuS v1 was ready, I integrated the modern and contemporary guidelines into the &lt;a href="https://catmus-guidelines.github.io/"&gt;CATMuS website&lt;/a&gt;, where the transcription guidelines for CATMuS medieval were already published.&lt;/p&gt;
&lt;p&gt;Now that I am done integrating the rules set for McCATMuS into the website, I am confident that we have successfully designed rules that are overall compatible across the medieval, modern and contemporary periods, despite some unavoidable exceptions. Two good examples of the impossibility to cover a whole millennium of document production with the same rule are the &lt;a href="https://catmus-guidelines.github.io/html/guidelines/en/abbreviations.html"&gt;abbreviations&lt;/a&gt; and the &lt;a href="https://catmus-guidelines.github.io/html/guidelines/en/punctuation.html"&gt;punctuation signs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I've now explained how the transcription guidelines were established for McCATMuS. Next, I'll cover how they were integrated into existing datasets to create the first version of the McCATMuS dataset.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:hyphen"&gt;
&lt;p&gt;You can't assume that every instance of &lt;code&gt;⟨-⟩&lt;/code&gt; at the end of a line must be replaced with a &lt;code&gt;⟨¬⟩&lt;/code&gt;. In many cases, this can be a simple typographic decoration marking the end of a paragraph or the end of a title. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/019/#fnref:hyphen" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>CATMuS</category><category>guidelines</category><category>HTR</category><guid>https://alix-tz.github.io/phd/posts/019/</guid><pubDate>Wed, 14 Aug 2024 17:55:01 GMT</pubDate></item><item><title>018 - The CATMuS Modern dataset #1</title><link>https://alix-tz.github.io/phd/posts/018/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;Last week, I attended &lt;a href="https://dh2024.adho.org/"&gt;ADHO's annual conference&lt;/a&gt; in Washington DC. I presented a short paper, co-authored with Floriane Chiffoleau and Hugo Scheithauer, about the documentation we wrote for eScriptorium (I wrote &lt;a href="https://alix-tz.github.io/phd/posts/018/010"&gt;a post&lt;/a&gt; about it last year and you can also find our presentation &lt;a href="https://inria.hal.science/hal-04594142"&gt;here&lt;/a&gt;). I was also a co-author on a long paper presented by Ariane Pinche on the &lt;a href="https://inria.hal.science/hal-04346939"&gt;CATMuS Medieval dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;CATMuS, which stands for "Consistent Approach to Transcribing ManuScripts", is a collective initiative and a framework to aggregate ground truth datasets using compatible &lt;a href="https://catmus-guidelines.github.io/"&gt;transcription guidelines&lt;/a&gt; for documents from different period written in romance languages. It started with &lt;a href="https://huggingface.co/datasets/CATMuS/medieval"&gt;CATMuS Medieval&lt;/a&gt;, but since January this year, I have been working on a version of CATMuS for the modern and contemporary period. &lt;/p&gt;
&lt;p&gt;While I should (and will) try to publish a data paper on CATMuS Modern &amp;amp; Contemporary (I'll call it McCatmus from now on), I figured I could start with a series of blog posts here. I want to describe the various steps I followed in order to eventually release &lt;a href="https://huggingface.co/datasets/CATMuS/modern"&gt;a dataset on HuggingFace&lt;/a&gt; and hopefully soon the corresponding transcription model.&lt;/p&gt;
&lt;p&gt;I started working on McCatmus in January, but because of a major personal event (I moved to Canada!), it took seven month of stop-and-go before the release of the V1. This was particularly challenging due to the scale of the project and its technicality (it was hard to get back into McCatmus after several weeks of interruption, which I had to do several times).&lt;/p&gt;
&lt;p&gt;To add to this complexity, McCatmus was also a multi-front operation. Indeed, to create McCatmus, it was necessary to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;define transcription guidelines in collaboration with other data producers,&lt;/li&gt;
&lt;li&gt;identify datasets compatible with the guidelines and set priorities,&lt;/li&gt;
&lt;li&gt;actually make all the dataset compatible with each other and clean some of the data,&lt;/li&gt;
&lt;li&gt;model and collect metadata that made sense for this dataset,&lt;/li&gt;
&lt;li&gt;release the dataset and fix the issues that came up.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To this date, two tasks remain on my to-do list for McCatmus: train a transcription model corresponding to this dataset and compare it with other existing ones, and make sure to have a publication describing this dataset and its usefulness.&lt;/p&gt;
&lt;p&gt;My plan is to dedicate one post to the creation of the guidelines for the dataset, then a post about the identification and collection of the datasets used in McCatmus v1, and then I'll wrap up with a post about the process to create the dataset, the metadata and the release. Stay tuned!&lt;/p&gt;</description><category>CATMuS</category><category>HTR</category><guid>https://alix-tz.github.io/phd/posts/018/</guid><pubDate>Wed, 14 Aug 2024 04:00:00 GMT</pubDate></item><item><title>016 - Text Recognition, Large Models and Expectations</title><link>https://alix-tz.github.io/phd/posts/016/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;Since the boom around ChatGPT almost a year ago, I've heard several people wondering if "tools like ChatGPT" were more efficient than HTR models trained with &lt;a href="https://kraken.re"&gt;Kraken&lt;/a&gt; and the like. The glimmer of hope in their eyes was most likely lit by their own struggle to set successful and/or efficient HTR campaigns with more traditional tools. The capacity of Large Language Models (LLMs) to reformulate a text&lt;sup id="fnref:spina"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/016/#fn:spina"&gt;1&lt;/a&gt;&lt;/sup&gt; or, more specifically, of Large Multimodal Models (LMMs) to generate text based on a visual input may indeed lead people to believe that HTR technologies built on &lt;a href="https://poloclub.github.io/cnn-explainer/"&gt;CNNs&lt;/a&gt; are on the verge of being flipped upside-down.&lt;sup id="fnref:multimodal_turn"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/016/#fn:multimodal_turn"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Annika Rockenberger recently conducted a series of small experiments on the matter and wrote &lt;a href="https://greflinger.hypotheses.org/739"&gt;an interesting blog post&lt;/a&gt; about it. Let's summarize it!&lt;/p&gt;
&lt;p&gt;She signed up for a premium subscription (25$/mo) to be able to chat with &lt;a href="https://openai.com/gpt-4"&gt;GPT4&lt;/a&gt;, which allows users to upload images. Then she submitted printed or handwritten documents she would normally transcribe with &lt;a href="https://readcoop.eu/transkribus"&gt;Transkribus&lt;/a&gt; and assessed the results. She found that GPT4 was fairly good on ancient print (German Fraktur) and that it was even able to follow transcription guidelines if provided with an example. However on a letter bearing handwritten cursive, the model completely hallucinated the content and attempted a transcription in the wrong language. This didn't change when she provided more context on the document. Rockenberger concludes that there is a potential for using ChatGPT for HTR but that the capacity of scaling it up is completely unsure and that learning how to provide good prompts to get the appropriate results is a challenge. I would also add that in the end, Rockenberger paid 25$ to get 10 lines of raw text, whereas with software like Transkribus or eScriptorium, she would also get a standard structured output.&lt;/p&gt;
&lt;p&gt;So, in other words, after reading Rockenberger's post, one can conclude that GPT4 (or, better, similar free and open source models) does have a potential for "quick and dirty-ish" OCR. However, I would argue that users tempted by this strategy might still miss an important point: even LMM-based tools will requires a little bit of organization and precision from the users. This, I find, often lacks in unsuccessful HTR campaigns. LMMs could generate a good output, but you will likely have to pay a counterpart one way or the other(s): with lower text recognition quality, with hallucinated text content, with impoverished non-structured output, with premium fees, etc.&lt;/p&gt;
&lt;p&gt;Earlier this year, an article proposed by &lt;a href="https://arxiv.org/abs/2305.07895"&gt;Liu et al. (2023)&lt;/a&gt;, "On the Hidden Mystery of OCR in Large Multimodal Models", explored almost exactly the same topic but in a more comprehensive way. Their article presents an extensive survey of how well several Large &lt;a href="https://en.wikipedia.org/wiki/Multimodal_learning"&gt;Multimodal&lt;/a&gt; Models (LMMs) performed on "zero-shot" tasks.&lt;/p&gt;
&lt;p&gt;Zero-shot refers to the act of requesting an output from an LLM or a LMM without training it for this task in particular. It is very similar to Rockenberger's first attempt with GPT4, when she uploaded the image of a printed document and asked for its transcription. In such a case, she relied on the capacity of the model to transfer its knowledge to the specific tasks of Text Recognition, on a specific type of documents (historical printed text).&lt;/p&gt;
&lt;p&gt;Other terms are often associated with "zero-shot:" "one-shot" and "few-shot". One-shot is equivalent to Rockenberger's second attempt: when she showed GPT4 an example of the output she expected on the 10 first lines of the documents, and requested that the model copied her strategy to generate the transcription of the 10 next lines. Few-shot would mean showing several pages and several expected output to the model before asking for the transcription of a new document.&lt;sup id="fnref:shot-definition"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/016/#fn:shot-definition"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The paper focused on currently available LMMs representing five different approaches for training LMMs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://huggingface.co/docs/transformers/model_doc/blip-2"&gt;BLIP-2&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2204.14198"&gt;Flamingo&lt;/a&gt;/&lt;a href="https://laion.ai/blog/open-flamingo/"&gt;Open-Flamingo&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://llava-vl.github.io/"&gt;LLaVa&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://minigpt-4.github.io/"&gt;miniGPT4&lt;/a&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;a href="https://huggingface.co/spaces/MAGAer13/mPLUG-Owl"&gt;mPLUG-Owl&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They evaluated the models on 4 tasks: text recognition, text-based visual question answering, key information extraction and handwritten mathematical expression recognition. Here are a few examples of what these tasks entail, as illustrated in the original article (on the images, P stands for Prediction and GT for Ground Truth):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;Task&lt;/th&gt;
&lt;th style="text-align: center;"&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Text Recognition&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;img alt="Examples of failed Text Recognition" src="https://alix-tz.github.io/phd/images/LLM_text_recogntion.png" title="Four images contained printed of handwritten words along with the ground truth (expected transcription) and the prediction generated by the models. For example, the model predicted 'chocolate' when the expected transcription was 'choco'"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Visual Question Answering&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;img alt="Examples of failed Visual Question Answering" src="https://alix-tz.github.io/phd/images/LLM_textVQA.png" title="Two images of real-life views along with a question used as a prompt, the expected answer and the predicted answer. For example, when asked 'What is the yellow number?' on the image of an airport luggage retrieval conveyor belt showing a clear '7' in yellow in the background, the model provided the following answer: 'The yellow number on the luggage trolley is 32"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;*Key Information Extraction&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;img alt="Examples of failed Key Information Extraction" src="https://alix-tz.github.io/phd/images/LLM_keyinfoextraction.png" title="Three images of real-life documents or textual information, along side with a question used as a prompt for the model, the expected answer and the predicted answer. For example, when asked 'what is the Sample No information in the input?', the model is expected to answer '1194-90' but answers 'The sample number is 33340'"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Handwritten Mathematical Expression Recognition&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;img alt="Examples of failed Handwritten Mathematical Expression Recognition" src="https://alix-tz.github.io/phd/images/LLM_HMExpr.png" title="Four example of failed attempts from the LMM to predict a LaTeX representation of handwritten mathematical expression: the numbers are wrong and/or the mathematical structure of the equations is made up"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- "Four images contained printed of handwritten words along with the ground truth (expected transcription) and the prediction generated by the models. For example, the model predicted 'chocolate' when the expected transcription was 'choco'.") | --&gt;
&lt;!--  "Two images of real-life views along with a question used as a prompt, the expected answer and the predicted answer. For example, when asked 'What is the yellow number?' on the image of an airport luggage retrieval conveyor belt showing a clear '7' in yellow in the background, the model provided the following answer: 'The yellow number on the luggage trolley is 32") | --&gt;
&lt;!--  "Three images of real-life documents or textual information, along side with a question used as a prompt for the model, the expected answer and the predicted answer. For example, when asked 'what is the Sample No information in the input?', the model is expected to answer '1194-90' but answers 'The sample number is 33340'") | --&gt;
&lt;!--  "Four example of failed attempts from the LMM to predict a LaTeX representation of handwritten mathematical expression: the numbers are wrong and/or the mathematical structure of the equations is made up") | --&gt;

&lt;p&gt;For each task, they used several datasets presenting different challenges. For each of these datasets and tasks, they retrieved the scores of the state-of-the-art (sota) for supervised methods and used them as a baseline. For example, for text recognition on the &lt;a href="https://fki.tic.heia-fr.ch/databases/iam-handwriting-database"&gt;IAM dataset&lt;/a&gt;, the sota method of AttentionHTR&lt;sup id="fnref:attentionhtr_ref"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/016/#fn:attentionhtr_ref"&gt;4&lt;/a&gt;&lt;/sup&gt; reaches a word accuracy of 91.24%.&lt;sup id="fnref:remark_wer"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/016/#fn:remark_wer"&gt;5&lt;/a&gt;&lt;/sup&gt; In comparison, Liu et al provide the following scores for the tested LMM on this dataset:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;test LMM&lt;/th&gt;
&lt;th style="text-align: center;"&gt;Score on IAM&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;BLIP-2 OPT&lt;sub&gt;6.7b&lt;/sub&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;38.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;BLIP-2 FlanT5&lt;sub&gt;XXL&lt;/sub&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;40.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;OpenFlamingo&lt;/td&gt;
&lt;td style="text-align: center;"&gt;45.53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;LLaVa&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;em&gt;50.40&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;MiniGPT4&lt;/td&gt;
&lt;td style="text-align: center;"&gt;28.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;mPLUG-Owl&lt;/td&gt;
&lt;td style="text-align: center;"&gt;42.53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;---------------&lt;/td&gt;
&lt;td style="text-align: center;"&gt;-----&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Supervised SOTA&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;strong&gt;91.24&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The illustrations provided by the article are all of failed attempts, but it corresponds to the overall impression conveyed by the results of the experiments. Indeed, compared to the state-of-the-art supervised methods, zero-shot tasks prompted to LMMs yield results largely outperformed, similar to what is visible in the case of text recognition on the IAM dataset. The only exception is BLIP-2 on a Text Recognition task on a dataset of artistic text (&lt;a href="https://github.com/xdxie/WordArt#wordart-dataset"&gt;WordArt&lt;/a&gt;) which is more challenging. The authors consider that this is a sign that LMMs have a promising potential for visually complex texts.&lt;/p&gt;
&lt;p&gt;A very important section of their paper is their remarks on the relationship between LMMs and semantics. Submitting non-word images to the LMMs, they find that the LMMs systematically over-correct the prediction and suggest real-words as an answer. Traditional text recognition approaches, on the other hand, are much less sensitive to the notion of likelihood for the words to recognize. Similarly, the need for semantics interferes with the LMMs' output, and they tend to more easily recognize common words and make up additional letters ("choco" is read as "chocolate"). Lastly, LMMs are insensitive to word length: they are unable to count how many letters are in the image of a word. These results are similar to what Rockenberger experienced with the handwritten letter: the model hallucinated words to compose a semantically plausible letter. But using the wrong date, the wrong names, and the wrong language.&lt;/p&gt;
&lt;p&gt;Liu et al conclude their paper reminding us that they experimented with the capacities of the models in the context of zero-shot prompts, whereas there are already successful attempts at fine-tuning LLMs and LMMs on specialized tasks, such as medical prediction. In fact, I think there already exist such attempts in the context of HTR as well: it seems to be the ambition of a model like Transkribus' Text Titan, released at the beginning of the Summer. It is based on a &lt;a href="https://youtu.be/zxQyTK8quyY?feature=shared"&gt;Transformer&lt;/a&gt; coupled with an LLM. Unfortunately, I wasn't able to find more information on this model, aside from the community-oriented communications released by Transkribus on their website (&lt;a href="https://readcoop.eu/introducing-transkribus-super-models-get-access-to-the-text-titan-i/"&gt;here&lt;/a&gt; and &lt;a href="https://help.transkribus.org/super-models"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:spina"&gt;
&lt;p&gt;In stead of a multimodal approach, Salvatore Spina explored the possibility to use a LLM-based tool like ChatGPT3 to post-process the result of HTR and correct the text. See: Spina, S. (2023). &lt;em&gt;Artificial Intelligence in archival and historical scholarship workflow: HTS and ChatGPT&lt;/em&gt; (arXiv:2308.02044). arXiv. &lt;a href="https://doi.org/10.48550/arXiv.2308.02044"&gt;arXiv.2308.02044&lt;/a&gt;. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/016/#fnref:spina" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:multimodal_turn"&gt;
&lt;p&gt;Multimodality is presented by some researchers of the Digital Humanities community as a real epistemological turn for the field. See for example: Smits, T., &amp;amp; Wevers, M. (2023). &lt;em&gt;A multimodal turn in Digital Humanities. Using contrastive machine learning models to explore, enrich, and analyze digital visual historical collections&lt;/em&gt;. Digital Scholarship in the Humanities, fqad008. &lt;a href="https://doi.org/10.1093/llc/fqad008"&gt;doi: 10.1093/llc/fqad008&lt;/a&gt; ; or Impett, L., &amp;amp; Offert, F. (2023). &lt;em&gt;There Is a Digital Art History&lt;/em&gt; (arXiv:2308.07464). arXiv. &lt;a href="https://doi.org/10.48550/arXiv.2308.07464"&gt;arXiv.2308.07464&lt;/a&gt;. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/016/#fnref:multimodal_turn" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:shot-definition"&gt;
&lt;p&gt;There are a few videos offering more or less detailed explanations on these expressions &lt;a href="https://www.youtube.com/watch?v=E6X1Ufhxtf0"&gt;in the context of prompting an LLM&lt;/a&gt;. However, this is not specific to LLM, it is often used in the context of &lt;a href="https://huggingface.co/tasks/zero-shot-classification"&gt;classification&lt;/a&gt; or &lt;a href="https://joeddav.github.io/blog/2020/05/29/ZSL.html"&gt;NLP&lt;/a&gt; tasks for example. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/016/#fnref:shot-definition" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:attentionhtr_ref"&gt;
&lt;p&gt;Kass, D., &amp;amp; Vats, E. (2022). &lt;em&gt;AttentionHTR: Handwritten Text Recognition Based on Attention Encoder-Decoder Networks&lt;/em&gt; (arXiv:2201.09390). arXiv. &lt;a href="https://doi.org/10.48550/arXiv.2201.09390"&gt;arXiv.2201.09390&lt;/a&gt;. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/016/#fnref:attentionhtr_ref" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:remark_wer"&gt;
&lt;p&gt;In this case, the WER is used as a baseline to compare different approaches. However, in general, it is not a good idea to only take into account Word accuracy to understand a model's performance in real life. This is something I discussed in &lt;a href="https://alix-tz.github.io/phd/posts/012/"&gt;this&lt;/a&gt; post. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/016/#fnref:remark_wer" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>Large Language Models</category><category>OCR</category><guid>https://alix-tz.github.io/phd/posts/016/</guid><pubDate>Tue, 28 Nov 2023 10:28:15 GMT</pubDate></item><item><title>015 - Block post and comprehensive Exam</title><link>https://alix-tz.github.io/phd/posts/015/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;When I created this blog last year, I wanted to post regularly on it. Something like once a month or once every other month. I didn't want to put pressure on myself for writing, but I also wanted to make sure that this blog would be alive. I often have ideas for topics for a post. But then, when comes the time to write, I blank out. It's not exactly that I don't know where to start, it's just that I sometimes can't figure out what is the message I want to convey. Like, if I have to summarize my blog post in 2 lines, what's the take-away? I get stuck when I cannot find an answer,but maybe I shouldn't worry that much about it. It's my blog after all, and maybe the message will come by the time I'm done writing. &lt;/p&gt;
&lt;p&gt;So, without further ado, let's dive in: I was &lt;em&gt;super&lt;/em&gt; excited this Summer after passing my comprehensive exam. I really wanted to write a post about it. I had a really packed Spring and beginning of Summer between going back to Montreal, teaching a class there, attending a Summer school, going to the DH2023 conference in Austria where I presented a short paper, a long paper and organized a workshop (big up to Thibault who was by my sides through all these Austrian adventures). And all of it culminated with that comprehensive exam in the middle of August. I really wanted to share how that went.&lt;/p&gt;
&lt;p&gt;But then, vacations, working on new deadlines, more vacations, more deadlines... And now it's already November and I don't know anymore what it was that I wanted to share about that exam. Aside from the fact that I passed it and that it's a pretty big milestone.&lt;/p&gt;
&lt;p&gt;The comprehensive examination, which is called "Examen de synthèse" in French, is not something common in France. In France, we now have a sort of yearly evaluation called the "Comité de Suivi Individuel" (or CSI), which is not a scholar evaluation but more of a check-up with your supervisors and a committee&lt;sup id="fnref:CSI"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/015/#fn:CSI"&gt;1&lt;/a&gt;&lt;/sup&gt; in charge of making sure that everything is alright. The reason I bring it alongside the Examen de Synthèse is because I also had my first CSI this Summer (at the very end of June). In France, you have to have a positive evaluation from the CSI in order to enroll in a new year of doctoral studies. Each year. But, actually the CSI and the Examen de Synthèse are not really that comparable.&lt;/p&gt;
&lt;p&gt;The Examen de Synthèse is a "real" examination and it happens only once during your doctoral curriculum. In my program at the University of Montréal, in 2023, it consisted in several phases.&lt;/p&gt;
&lt;p&gt;First of all, there is a phase dedicated to the composition of the jury. I had the pleasure to be examined not only by my three supervisors (Laurent Romary, Emmanuel Chateau-Dutier and Michael Sinatra), but also by &lt;a href="https://vitalirosati.com/"&gt;Marcello Vitali Rosati&lt;/a&gt;, from the University of Montréal, who acted as president, and &lt;a href="https://www.uqar.ca/universite/a-propos-de-l-uqar/departements/departement-des-lettres-et-humanites/gohier-maxime"&gt;Maxime Gohier&lt;/a&gt; from the University of Quebec in Rimouski. I must signal that my only regret is not to have been able to have a better gender parity in my jury. This is something I really hope to fix for my defense, but I will probably have other occasions to discuss this topic in the future.&lt;/p&gt;
&lt;p&gt;So, once the jury is composed, and once a calendar has been agreed on (I think &lt;em&gt;that&lt;/em&gt; was actually the most stressful part for me because of all the other things I had this Summer), a count down begins. First, I had to turn in three documents:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a 12-15 page-long essay on my research project;&lt;/li&gt;
&lt;li&gt;a 30-reference long bibliography on the field of the Digital Humanities; and&lt;/li&gt;
&lt;li&gt;a short presentation of a proposed "practical" analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then a week later, the jury sent a question.&lt;sup id="fnref:question"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/015/#fn:question"&gt;2&lt;/a&gt;&lt;/sup&gt; I was given 1 week (168h exactly) to think about this question and write a response in the form of a 10-15 page-long essay. The jury had between a week and two weeks to read the response before an oral examination took place (on Zoom).&lt;/p&gt;
&lt;p&gt;The oral examination has some similarities with a PhD defense. It started with a 20 minute long presentation that I gave where I summarized my research project (10 minutes) and presented a technical analysis (10 minutes). I chose to focus my technical presentation on an experiment I have been conducting and on which I hope to communicate more in the near future. Then, after my presentation, there were two rounds of questions about my research project, my experiment or about the answer I formulated in my essay.&lt;sup id="fnref:more"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/015/#fn:more"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;I am very happy that such an examination exists in the North American program. It may seem like a lot of stress (and it is), but I found that it is also a very good milestone to progress a lot towards the formalization of a research project. The oral examination is a great opportunity to present a project to people who don't necessarily know what you have been up to before, and it's a really really great occasion to get feedback.&lt;/p&gt;
&lt;p&gt;For example, the question that is sent by the jury, in the case of my program, is thought as a way to get you to think about a topic or a question that is either not tackled enough by your research proposal, or it's an invitation to consider new angles. You're not expected to turn in the perfect answer, of course, with barely a week to write it. But it forces you to form an opinion, explore possible hypotheses and may turn later into a whole chapter for your thesis.&lt;/p&gt;
&lt;p&gt;The comprehensive exam is a pass/no pass type of examination. There is no grade and if you fail, you can take it a second time. Like I said before at the beginning of this post, I passed. Therefore, starting from Fall 2023, I am now able to enroll as a "en rédaction" student (writing status) which has several consequences. Some seem very symbolic: for example, in English, I can now call myself a PhD candidate instead of a PhD student. But others not so much: tuitions for this new status are &lt;a href="https://registraire.umontreal.ca/droits-de-scolarite/couts-des-etudes/#c14807"&gt;much lower&lt;/a&gt; than when enrolling as a full-time student, dropping from 1,440$CA/trimester to 512$CA/trimester, and I believe this officially gives me the right to teach at graduate level.&lt;/p&gt;
&lt;p&gt;The comprehensive exam also marks the end of the phase during which I had to take courses. Now, with this new status, I am invited to focus solely on the redaction of my thesis, which opens up a whole new chapter for my PhD curriculum.&lt;sup id="fnref:pun"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/015/#fn:pun"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:CSI"&gt;
&lt;p&gt;I want to take this occasion to also thank &lt;a href="https://ciham.cnrs.fr/annuaire/membres_statutaires/ariane-pinche/"&gt;Ariane Pinche&lt;/a&gt; and &lt;a href="https://pro.univ-lille.fr/joana-casenave"&gt;Joana Casenave&lt;/a&gt;, who were willing to be the members of my committee for the CSI, for their precious feedback! :) &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/015/#fnref:CSI" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:question"&gt;
&lt;p&gt;The question was the following: "&lt;em&gt;Dans votre projet de recherche apparaît une tension importante: celle entre la spécificité des besoins particuliers de chaque projet et la volonté -- et la nécessité -- de produire des approches généralisables, qui puissent être employées dans le cadre de plusieurs projets. En vous appuyant sur votre bibliographie, et en vous concentrant notamment sur le cas du HTR, pourriez-vous analyser cette tension en soulevant en particulier la question de la littératie demandée (notamment dans la gestion des données) pour pouvoir personnaliser des approches computationnelles aussi complexes que les technologies HTR?&lt;/em&gt;" &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/015/#fnref:question" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:more"&gt;
&lt;p&gt;I want publish on my blog the documents I created for the comprehensive exam, but I need to find the best way to do it. I'll post an announcement when it will be available. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/015/#fnref:more" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:pun"&gt;
&lt;p&gt;Thank you Jennifer for this wonderful pun! ;) &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/015/#fnref:pun" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>cotutelle</category><category>courses</category><guid>https://alix-tz.github.io/phd/posts/015/</guid><pubDate>Tue, 07 Nov 2023 13:15:06 GMT</pubDate></item><item><title>014 - RT(F)M for the Peraire Experiment</title><link>https://alix-tz.github.io/phd/posts/014/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;Turns out, there is more to say on last week's &lt;a href="https://alix-tz.github.io/phd/posts/013/"&gt;experiments on the Peraire dataset&lt;/a&gt;! And I found out while I was working on a completely different dataset. Let me explain!&lt;/p&gt;
&lt;p&gt;This morning, I helped my colleague train a Kraken transcription model for &lt;a href="https://ecrituresnumeriques.ca/fr/Activites/Projets/2016/1/19/Anthologie-grecque"&gt;Greek manuscripts&lt;/a&gt;. They gave me the ground truth and I set and executed the training from the command line. It gave me an opportunity to try fine-tuning a model like &lt;a href="https://zenodo.org/record/7234166"&gt;CREMMA Medieval&lt;/a&gt;, in stead of only training from scratch. &lt;strong&gt;CREMMA Medieval&lt;/strong&gt; was trained on manuscripts written in Latin, whereas the Greek manuscripts were written only, well, in Ancient Greek. I didn't want the resulting model to add Latin letters in the transcription when applied to other Greek documents, so I used Kraken's option to allow the model to forget previously learned characters and to force it to only remember the characters contained in the new training data. This option is called &lt;strong&gt;&lt;code&gt;--resize&lt;/code&gt;&lt;/strong&gt; (check the documentation &lt;a href="https://github.com/mittagessen/kraken/blob/4.3.7/docs/ketos.rst#fine-tuning"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;When I fine-tune a model, I usually follow Kraken's recommendations and keep both the previously learned characters and the new ones coming from the new set of ground truth. When this morning I checked what is the keyword to use to keep only the characters from the new dataset, I realized that I didn't correctly set the training on Peraire last week. I had set it to only keep the new characters!&lt;/p&gt;
&lt;p&gt;Up until Kraken v. 4.3.10, &lt;strong&gt;&lt;code&gt;--resize&lt;/code&gt;&lt;/strong&gt; can take the keywords &lt;strong&gt;&lt;code&gt;both&lt;/code&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;code&gt;add&lt;/code&gt;&lt;/strong&gt;. The ambiguity of these keywords &lt;a href="https://github.com/mittagessen/kraken/issues/478"&gt;has been discussed&lt;/a&gt; in the past, which is the reason why starting from Kraken v. 4.3.10, the keywords respectively become &lt;strong&gt;&lt;code&gt;new&lt;/code&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;code&gt;union&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let's quote the manual:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are two modes dealing with mismatching alphabets, &lt;strong&gt;add&lt;/strong&gt; and &lt;strong&gt;both&lt;/strong&gt;. &lt;strong&gt;add&lt;/strong&gt; resizes the output layer and codec of the loaded model to include all characters in the new training set without removing any characters. &lt;strong&gt;both&lt;/strong&gt; will make the resulting model an exact match with the new training set by both removing unused characters from the model and adding new ones.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I fell for this trap of ambiguity and used &lt;strong&gt;both&lt;/strong&gt; instead of &lt;strong&gt;add&lt;/strong&gt;, thinking &lt;strong&gt;both&lt;/strong&gt; meant I was keep &lt;em&gt;both&lt;/em&gt; character sets. (Again this is the very reason why the keywords were recently changed).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Side note: you should really read &lt;a href="https://alix-tz.github.io/phd/posts/013/"&gt;last week's post&lt;/a&gt; to fully understand the rest of this post!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;At the end of my post last week, I wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;peraire_D&lt;/strong&gt; on the other hand seems to lose it completely on the B series. This is most likely due to the fact that the contrast between the page and the "ink" is too low in the pencil-written series compared to the data used to train &lt;strong&gt;Manu McFrench&lt;/strong&gt; and in the D series. &lt;strong&gt;peraire_D&lt;/strong&gt; even loses 11 points of accuracy to &lt;strong&gt;Manu McFrench&lt;/strong&gt;!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But how could I be sure that it was not actually due to the fact that the model had unlearned some precious characters?&lt;/p&gt;
&lt;p&gt;The only way to know, I thought, was to re-train the models! I used this opportunity to also train the models from scratch because I was curious to see how much noise/improvement was brought by the base model.&lt;/p&gt;
&lt;p&gt;I tried 4 types of models and, like last week, used &lt;a href="https://github.com/WHaverals/CERberus"&gt;CERberus 🐶🐶🐶&lt;/a&gt; to measure the character error rates on the predictions made on the test sets:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Models trained "from scratch"&lt;/li&gt;
&lt;li&gt;A model not trained on any data coming from the Peraire dataset (aka &lt;a href="https://zenodo.org/record/6657809"&gt;&lt;strong&gt;Manu McFrench&lt;/strong&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Models obtained from finetuning &lt;strong&gt;Manu McFrench&lt;/strong&gt; using the &lt;strong&gt;add&lt;/strong&gt; resize mode&lt;/li&gt;
&lt;li&gt;Models obtained from finetuning &lt;strong&gt;Manu McFrench&lt;/strong&gt; using the &lt;strong&gt;both&lt;/strong&gt; resize mode&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For each model trained on the Peraire dataset, I used 3 compositions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the full dataset ("ALL")&lt;/li&gt;
&lt;li&gt;only data coming from the B series ("B")&lt;/li&gt;
&lt;li&gt;only data coming from the D series ("D")&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I used the same composition system for the test sets.&lt;/p&gt;
&lt;p&gt;Here are my results in the form of a table:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/peraire_scores.png" alt="a table of the scored obtained on the different train set, test set and resize configurations" widht="400px"&gt;&lt;/p&gt;
&lt;p&gt;Fortunately, it seems that my previous interpretation is not fully contradicted by the results I obtain with this second series of training. Let's focus on two observations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Whenever a model is trained only on the D series, and tested only on the B series, it appears to be completely incapable of predicting anything but gibberish, losing between 32 and 35 points of accuracy. It confirms that the aspect of the documents from the two series are too different. On the other hand, when the model is fine-tuned on the B series only, it maintains a fairly good accuracy when applied to the D series, whichever resize mode is used. I think it confirms that the B series is enough for the model to learn some sort of formal features from Peraire's handwriting, which the models can transfer to documents written with a different writing instrument.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is very interesting is the difference between the models trained on the whole datasets and tested on the B series: when we use the &lt;strong&gt;both&lt;/strong&gt; resize mode (meaning we only keep the characters from the new dataset), the model is very good. On the contrary, the performance of the model trained with the &lt;strong&gt;add&lt;/strong&gt; resize mode (meaning we keep the output layer and the codec from the base model and add the new characters) is as bad as with a model trained only on the D series.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my previous post, I wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;peraire_both&lt;/strong&gt; is able to generalize from seeing both datasets and even benefits from seeing more data thanks to the D series, since it performs better on the B series compared to &lt;strong&gt;peraire_B&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, in the light of my experiment with the &lt;strong&gt;resize&lt;/strong&gt; option, I think this is not correct. Instead, it appears that resetting the output layer by using &lt;strong&gt;both&lt;/strong&gt; (or &lt;strong&gt;new&lt;/strong&gt;) on accident, allowed the model to better take into account the data from the B series (pencil). Contrary to what I observed last week, the model trained on the whole dataset but this time with the &lt;strong&gt;add&lt;/strong&gt; resize mode (or &lt;strong&gt;union&lt;/strong&gt;) doesn't benefit from seeing more data compared to the model trained only on the B series.&lt;/p&gt;
&lt;p&gt;My understanding is that keeping the output layer from the base model with &lt;strong&gt;add&lt;/strong&gt; (or &lt;strong&gt;union&lt;/strong&gt;) probably drowns the specificity of the pencil-written documents into a base knowledge tailored to handle documents with a high contrast (like the ones in the D series &lt;em&gt;and&lt;/em&gt; in &lt;strong&gt;Manu McFrench&lt;/strong&gt;'s training set). Or, to put it differently, when we use &lt;strong&gt;both&lt;/strong&gt; (or &lt;strong&gt;new&lt;/strong&gt;), more attention is given to the pencil written documents, meaning that the model actually gets better at handling this category of data.&lt;/p&gt;
&lt;p&gt;I am extremely curious to see how I can investigate this further, or if any of you, readers, would understand these results differently!&lt;/p&gt;</description><category>experiment</category><category>HTR</category><category>kraken</category><guid>https://alix-tz.github.io/phd/posts/014/</guid><pubDate>Fri, 04 Aug 2023 17:51:14 GMT</pubDate></item><item><title>013 - The Peraire experiment</title><link>https://alix-tz.github.io/phd/posts/013/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;&lt;em&gt;WARNING: in my &lt;a href="https://alix-tz.github.io/phd/posts/014/"&gt;next post&lt;/a&gt;, I nuance the conclusions drawn in this post, because of a parameter I didn't correctly set during the training of the models described below. You should really read it after reading this post, to get the full picture!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As a small side project during my phD, I have been sharing my expertise (and a bit of my workforce) with the members of the &lt;a href="https://www.pamir.fr/projets-soutenus/spe-vlp/"&gt;DIM SPE-VLP&lt;/a&gt; project. The acronym stands for "Sauver le patrimoine espérantiste : le voyage de Lucien Péraire (1928-1932)." The project revolves around the digitization, transcription and edition/valorization of &lt;a href="https://fr.wikipedia.org/wiki/Lucien_P%C3%A9raire"&gt;Lucien Peraire&lt;/a&gt;'s archives. He was a French citizen who, in the late 1920s, travelled across the European and the Asian continents, mostly by bike and using &lt;a href="https://en.wikipedia.org/wiki/Esperanto"&gt;Esperanto&lt;/a&gt; to communicate. He kept a diary during his journey (and later published a book about his adventures). His notes are written both in French and in Esperanto and in some documents, he also used &lt;a href="https://en.wikipedia.org/wiki/Shorthand"&gt;stenography&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My contribution to the project has mostly consisted in helping developing transcription models for the French diaries (although I'm also interested in the shorthand and the esperanto). This meant both helping with the production of ground truth and training &lt;a href="https://kraken.re/"&gt;Kraken&lt;/a&gt; models. This post will briefly explain how the ground truth was created and published, as well as present the models that were trained with it.&lt;/p&gt;
&lt;p&gt;Peraire's notebooks are organized in different series, and each series is divided in ensembles regrouping the pages of a notebook. Each ensemble is named after the countries visited while the notebook was used. For example, notebook 11 in the B series forms one ensemble and covers a part of Peraire's travels in Japan. There are 31 notebooks in the B series. The notebooks of this series are written with a blue pencil on (low quality) school papers. On some pages, the pencil is very faded which makes it hard to read the text, let alone to run a successful segmentation task on the image. On the other hand, the D series gathers notes and comments on the diaries, written at the end of the 1960s. This time the handwriting is much easier to read because Peraire mostly used a blue or black ball-point pen. There are 9 ensembles in this series.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/peraire_handwriting.png" alt="two extracts of Peraire's notebooks side by side, on the left the image is taken from the B series, on the right the image is taken from the D series." widht="600px"&gt;&lt;/p&gt;
&lt;p&gt;One aspect that I find particularly interesting with this dataset is that we have a case where the handwriting is similar but the writing tool is different. It means that it is possible to explore how the writing tools and/or writing supports affect the efficiency of a transcription model. On top of that, all the documents were digitized under the same (good) conditions and by the same people.&lt;/p&gt;
&lt;h3&gt;Segmenting, transcribing, aligning and publishing&lt;/h3&gt;
&lt;p&gt;The first version of the dataset was solely focused on the B series. I selected 1 random page from each ensemble (avoiding to take the first page each time) to compose a train set of 33 files&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/013/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;. On top of that, I selected 4 additional pages from B3, B5, B12 and B18 to compose a fixed test set which would never be used as training data.&lt;/p&gt;
&lt;p&gt;I pre-segmented the images with Kraken's default model before correcting the result manually. At this point, I also applied the &lt;a href="https://segmonto.github.io/"&gt;segmOnto&lt;/a&gt; ontology for the lines and regions&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/013/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;. Because of the &lt;a href="https://raw.githubusercontent.com/alix-tz/peraire-ground-truth/master/data/train/B.2.europe-orientale_0007.jpg"&gt;fading ink&lt;/a&gt;, some words could not be transcribed. In order to avoid complicating the transcription rules, I decided to simply segment out the passages that couldn't be read. On the one hand it simplifies the transcription, but on the other hand, it means that a small portion of my segmented documents cannot be re-used by others to train a segmentation model. Since we were not training a segmentation model, it was an easy decision.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/peraire_faded.png" alt="screenshot showing the segmentation and the transcription panels from eScriptorium where we can see that some lines are broken down into several segments and that some segments were left blank" widht="400px"&gt;&lt;/p&gt;
&lt;p&gt;More recently, it was decided to augment the dataset with examples from the D series because the model trained on the B series was not good enough. This time, Gilles Pérez, a member of the project, took charge of the transcription. I recommended to create a new sample of 30 to 40 images, so he randomly selected series of 4 continuous pages from each ensemble. The transcription of the corresponding 36 pages was sent to me as a Word document. Therefore, on top of taking care of the segmentation of the images, I also went through an alignment phase during which I verified the order of the lines and copy-pasted the transcription. It took longer than I expected but it allowed me to align the transcription with the rules I had followed when creating the first set. I also picked 4 of the 36 pages to add to the test set.&lt;/p&gt;
&lt;p&gt;The dataset is versioned and published applying the principles and tools we developed withing the frame of &lt;a href="https://htr-united.github.io/"&gt;HTR-United&lt;/a&gt;. I also added illustrated segmentation and transcription guidelines.&lt;/p&gt;
&lt;h3&gt;Testing different dataset configurations to train transcription models&lt;/h3&gt;
&lt;p&gt;As I mentioned before, the goal of these datasets was to create transcription models. Taking the opportunity of the recent update of the dataset, I tried different scenarios.&lt;/p&gt;
&lt;p&gt;I never trained the model from scratch because the dataset is too small to get any sort of usable model. Instead, I used &lt;a href="https://zenodo.org/record/6657809"&gt;&lt;strong&gt;Manu McFrench&lt;/strong&gt;&lt;/a&gt; as a base model, fine-tuned with the Peraire dataset. (We were actually able to use Peraire as an example during the &lt;a href="https://www.conftool.pro/dh2023/index.php?page=browseSessions&amp;amp;form_session=76#paperID690"&gt;DH2023&lt;/a&gt; conference&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/013/#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt; earlier this month to show the usefulness of having this kind of base model). I tested fine-tuning only on the B series, only on the D series or on both the B and the D series. Then I used a B-series-only test set, a D-series-only test set and the full test set to see how the models performed.&lt;/p&gt;
&lt;p&gt;Since I wanted to try it after discovering it during DH2023, I used &lt;a href="https://github.com/WHaverals/CERberus"&gt;CERberus 🐶🐶🐶&lt;/a&gt; (I talked about it in my &lt;a href="https://alix-tz.github.io/phd/posts/012/"&gt;last post&lt;/a&gt;) to measure the accuracy of the models on the test sets listed above.&lt;/p&gt;
&lt;p&gt;Like &lt;a href="https://huggingface.co/spaces/lterriel/kami-app"&gt;KaMI&lt;/a&gt;, CERberus takes 2 categories of text input: the reference (aka the ground truth) and the prediction (or the hypothesis made by the model). In order to get the prediction, I loaded my models on eScriptorium, as well as the images and transcription of the test set before applying each model to the documents. This way, all the transcription are predicted with the same segmentation, which comes from the ground truth.&lt;/p&gt;
&lt;p&gt;Here are the results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Manu McFrench&lt;/strong&gt;, before fine-tuning, gets a CER of 26.16% when tested on the whole test set, and a score of 27.19% on the documents from the B series, 25.29% on the D series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_both&lt;/strong&gt;, trained on the B and the D series, gets a CER of 4.63% when tested on the whole test set, but a score of 6.41% on the documents from the B series and 3.54% on the D series.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_B&lt;/strong&gt;, trained only on the B series, gets a CER of 8.72% on the whole test set, but a score of 7.12% on test-B and 9.67% on test-D.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_D&lt;/strong&gt;, trained only on the D series, gets an CER of 16.38% on the whole test set, but this is because of the enormous descripancy between its score on each sub test set. It skyrockets to a CER of 38,53% on test-B while going as low as 3.65% on test-D.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of this makes sense, though.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ManuMcFrench&lt;/strong&gt; could not be used without fine-tuning, its error rate on both documents is too high.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_both&lt;/strong&gt; is able to generalize from seeing both datasets and even benefits from seeing more data thanks to the D series, since it performs better on the B series compared to &lt;strong&gt;peraire_B&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_B&lt;/strong&gt; which was trained on the more difficult dataset seems to use the knowledge inherited from &lt;strong&gt;Manu McFrench&lt;/strong&gt; and to have learned some formal features from Peraire's handwriting since it is able to maintain a fairly low CER on the D series (it gains 16 points of accuracy compared to &lt;strong&gt;Manu McFrench&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;peraire_D&lt;/strong&gt; on the other hand seems to lose it completely on the B series. This is most likely due to the fact that the contrast between the page and the "ink" is too low in the pencil-written series compared to the data used to train &lt;strong&gt;Manu McFrench&lt;/strong&gt; and in the D series. &lt;strong&gt;peraire_D&lt;/strong&gt; even loses 11 points of accuracy to &lt;strong&gt;Manu McFrench&lt;/strong&gt;!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What happens with &lt;strong&gt;peraire_D&lt;/strong&gt; is very interesting because it confirms that it is useful to compose a train set with examples of more difficult documents instead of only showing the ones that are easy to read! Now, the nice thing is that I will soon be working on a little experiment with my colleague Hugo Scheithauer where we will be able to measure the impact of the contrast between the ink and the paper. Stay tuned!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT #1: I added the scores obtained by Manu McFrench alone.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT #2: I added a disclaimer at the beginning of the post.&lt;/em&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;I used 2 images from B2 because one of them was extremely faded and I wanted to include some of these extreme cases in the dataset, and 2 images from B30 because it consisted of shorter lines (table of contents) which I found was interesting to include. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/013/#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;As described in the documents, I only used the "InterlinearLine" and "DefaultLine" for the lines, and the "MainZone" and "NumberingZone" for the regions. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/013/#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;See the submission and the slides on HAL: &lt;a href="https://inria.hal.science/hal-04094241"&gt;https://inria.hal.science/hal-04094241&lt;/a&gt;. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/013/#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>experiment</category><category>HTR</category><category>kraken</category><guid>https://alix-tz.github.io/phd/posts/013/</guid><pubDate>Fri, 28 Jul 2023 15:39:18 GMT</pubDate></item><item><title>012 - "It did a very good job"</title><link>https://alix-tz.github.io/phd/posts/012/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;A few weeks ago, I attended the presentation of an automatic transcription software. The majority of the audience was unfamiliar with the concept of handwritten text recognition (HTR) or had little experience using it. The presentation lasted only an hour, so it couldn't delve into much detail. Its main objective was to demonstrate the software's results. The presenter showed several slides, displaying on one side  images of manuscripts (often in a language unknown to the audience) and on the other side the transcriptions generated by the software. Throughout the presentation, the presenter repeatedly commented on the HTR software saying that "it did a very good job."&lt;/p&gt;
&lt;p&gt;But what does it even mean?&lt;/p&gt;
&lt;p&gt;The very first aspect to explore is what distinguishes a good job from a bad one. Normally, such an evaluation relies on the measurement of the accuracy of the result compared to the ideal transcription. The accuracy can be expressed positively or negatively using the error rates (a 0% error rate is the same as a 100% accuracy).&lt;/p&gt;
&lt;p&gt;Measuring the accuracy of a prediction (another way to call the result of HTR) is commonly done at character level. The character accuracy of a model is equal to the number of matches between the prediction and the ideal transcription. The character error rate (CER) is a very common measure to express a model's theoretical efficiency.&lt;/p&gt;
&lt;p&gt;Sometimes softwares also consider the word error rate (WER), which is the proportion of words in the prediction containing errors. A high score at WER doesn't actually mean that the transcription is bad. It only means that the errors are distributed on all the words. I never use WER alone because it is hard to get an exact impression of the quality of the prediction based on that metric alone.&lt;/p&gt;
&lt;p&gt;There is a paper from &lt;a href="https://dl.acm.org/doi/10.1145/3476887.3476888"&gt;Neudecker et al. (2021)&lt;/a&gt; where they test 5 different software used for evaluating the prediction. They also develop an interesting reflection on alternative metrics such as the "non-stopword accuracy", the "phrase accuracy", the "flexible character accuracy" (which is useful when the line order isn't always the same), the "figure of merit" (which "aims to quantify the effort required for manual post-correction" (p. 15)) or else the "unordered WER".&lt;/p&gt;
&lt;p&gt;When your score is a rate, there is an implicit idea that 100% is both the maximum score and the targeted score (for accuracy of course). But in the case of HTR, 100% accuracy is extremely rare because there are also edge cases where the way a letter was drawn is ambiguous: in such cases the error is not particularly caused by the inaccuracy of the HTR engine but rather by the imperfection of the handwriting in the first place.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.46"&gt;Hodel et al., (2021)&lt;/a&gt;, the authors provided a grid to interpret accuracy scores. They suggest the following three thresholds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CER &amp;lt; 10% == good (it allows efficient post-processing)&lt;/li&gt;
&lt;li&gt;CER &amp;lt; 5% == very good (errors are usually focused on rare or unknown words)&lt;/li&gt;
&lt;li&gt;CER &amp;lt; 2.5% == excellent (but it is usually only reached when the handwriting is very regular)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Personally, I think this grid should also include 20% and 0%. 20% as a threshold, because at 80% of accuracy, the transcription is supposedly good enough for fuzzy search and keyword spotting (I should add a reference here, but I can't find it anymore...); and 0% because it should be reminded that an accuracy of 100% is virtually impossible.&lt;/p&gt;
&lt;p&gt;To complement this, I would like to mention another possible approach to get an interpretable score: during the DH2023 conference, Thibault Clérice and I &lt;a href="https://inria.hal.science/hal-04094241"&gt;presented an experiment&lt;/a&gt; where we trained a model using the same data in the train set and the test set. Our model reached an accuracy close to 90%, which we were able to use as a baseline to define the highest accuracy score possible for the data we had. Thus we were able to consider that a model approaching 90% of accuracy would be an excellent model, as far as that dataset was concerned.&lt;/p&gt;
&lt;p&gt;Still during &lt;a href="https://www.conftool.pro/dh2023/index.php?page=browseSessions&amp;amp;form_session=76#paperID395"&gt;the DH2023 conference&lt;/a&gt;, Wouter Haverals introduced &lt;a href="https://github.com/WHaverals/CERberus"&gt;CERberus 🐶🐶🐶&lt;/a&gt;, a web interface which addresses the same type of issues as &lt;a href="https://huggingface.co/spaces/lterriel/kami-app"&gt;KaMI&lt;/a&gt;: the lack of nuance in a plain CER computation. Indeed, in a CER score, every type of error has the same weight. This means that mistaking an "e" for a "é" costs the same as mistaking a "e" for a "0": in the first case the text is likely still readable or understandable, whereas in the latter, it might not be the case.&lt;/p&gt;
&lt;p&gt;The CER metric is still very useful, but when applied to transcription projects, it is even more valuable when we can filter the types of errors we want to include in the evaluation.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT: I should have noted here that my reflection was focused on the evaluation of an automatic transcription in cases where you already have the expected transcription. When we apply an HTR model to a whole new set of documents, we usually don't have the correct transcription at hand (otherwise we wouldn't use HTR in the first place). This is the reason why many researchers try to find ways to evaluate the quality of the transcription without ground truth. One example can be found in &lt;a href="https://enc.hal.science/hal-03828529"&gt;Clérice (2022)&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So, to go back to our initial problem, we can see that there are many ways to draw the line between a good job and a bad one. The threshold will depend on the metric used to express the accuracy of the prediction and also (and actually mostly) on the way the generated text will be used down the line. Even though the software presentation I attended was short, I think we should always remind future users of HTR that 100% of accuracy is not always what they are seeking.&lt;/p&gt;
&lt;p&gt;A short reflection to finish this post: I was bothered by the expression used to qualify the transcription. I am still trying to figure out a way to put it into words. On top of lacking accuracy, the expression "it did a good job" was also calling for a vision of HTR as a magic tool at the service of the searchers and students. But, in which other cases do you say that someone did "a good job?" Likely when you delegate a task to a &lt;a href="https://africanarguments.org/2023/03/the-invisible-labour-of-africa-in-the-digital-revolution/"&gt;subaltern&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I see a problem here: in their current state, HTR engines are efficient but not to the point that people can use them without thinking clearly about what they want the engine to produce. It is easy to sell a software pretending that it is a magic servant that will do all the transcription in your place, a tool so smart that you can even consider delegating a part of your responsibility to it. But I think when new users of HTR fail to first reflect on the outcome they can reasonably expect from these engines, it creates disappointment and crappy data and workflows.&lt;/p&gt;</description><category>accuracy</category><category>evaluation</category><category>HTR</category><category>metrics</category><guid>https://alix-tz.github.io/phd/posts/012/</guid><pubDate>Sat, 15 Jul 2023 12:06:20 GMT</pubDate></item><item><title>011 - Working with synthetic data</title><link>https://alix-tz.github.io/phd/posts/011/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;What we call synthetic data are data generated artificially, as opposed to data taken from real-life samples. In the case of automatic transcription or layout analysis, it corresponds to creating fake documents or samples of text that look more or less like real ones, in stead of manually annotating existing documents.&lt;/p&gt;
&lt;p&gt;One of the main advantages of using synthetic data rather than real data is the fact that it comes already annotated. For automatic transcription for example, the annotation (transcription) is the same as the string of text passed to a text image generator. If you add to that the fact you can, in theory, generate an unlimited amount of pairs of text image and transcription, it represents an incredible opportunity to accelerate the production of training datasets. An example: &lt;a href="https://ieeexplore.ieee.org/document/8486162"&gt;Doush et al., 2018&lt;/a&gt; use this technique to generate PDF containing contemporary printed Arabic texts. The PDFs are printed, then re-scanned and aligned with the transcription that was used to generate the PDFs. The result is the Yarmouk dataset. As we will see later, generating fake handwritten text is a bit more difficult.&lt;/p&gt;
&lt;p&gt;Another advantage of this technique is that it offers an efficient way around the limitations posed by sensitive or confidential data (&lt;a href="https://doi.org/10.1007/978-3-319-11257-2_15"&gt;Hu et al. 2014&lt;/a&gt;). However, let's note that confidentiality is rarely a problem when it comes to training HTR models on historical documents.&lt;/p&gt;
&lt;p&gt;Generating fake data is not specific to computer vision (&lt;a href="https://www.annualreviews.org/doi/10.1146/annurev-statistics-040720-031848"&gt;Raghunathan, 2021&lt;/a&gt;), even though it is frequently used in this case because data for computer vision tasks are costly to produce. In general, it is a fairly frequent method when machine learning techniques are involved, disregarding the field of application (&lt;a href="https://link.springer.com/article/10.1007/s11263-021-01555-8"&gt;Kataoka et al., 2022&lt;/a&gt;). OCR and HTR tasks are not an exception and we can find traces of such experiments rather early (&lt;a href="https://ieeexplore.ieee.org/document/4669952"&gt;Beusekom et al., 2008&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The first time I was exposed to the notion of synthetic data was during a informal conversation with &lt;a href="https://www.tmonnier.com/"&gt;Tom Monnier&lt;/a&gt; in 2019. At that time, he was working on &lt;a href="https://arxiv.org/abs/2012.08191"&gt;docExtractor&lt;/a&gt;, a layout analysis tool that he trained with images of documents generated artificially.&lt;sup id="fnref:icfhr"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/011/#fn:icfhr"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Then sometimes in 2021, while browsing through &lt;a href="https://huggingface.co/spaces/launch"&gt;HuggingFace's spaces&lt;/a&gt;, I found ntt123's application that simulates &lt;a href="https://huggingface.co/spaces/ntt123/handwriting"&gt;handwriting&lt;/a&gt;. The application takes a text prompt as an input and generates an animation where the letters are traced on the page as if someone was writing them live. It's possible to play with two parameters: a value between 0 and 250 determining the writing style, and a weight determining the likelihood of the traced letters (the lower the weight, the higher the risk of &lt;a href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)"&gt;hallucinated&lt;/a&gt; letters; the higher the weight, the more standardized the tracing). It made me think back to my conversation with Tom Monnier and I wondered if it could be used to generate pairs of text and images.&lt;/p&gt;
&lt;p&gt;At the beginning of the year, I dedicated a good part of my time to testing data generation tools I could find online, to see if they could be used to create a set of fake ground truth that I would use later, in other experiments. I will introduce the latter in a future post, so let's first focus on handwritten data generation.&lt;/p&gt;
&lt;p&gt;When I dug a bit more around ntt123's application, I was confronted with two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;unfortunately, ntt123's application was developed in javascript and not documented at all which made it impossible for me to hack,&lt;/li&gt;
&lt;li&gt;but luckily, it wasn't an original idea: instead it was one of many implementations of a proposition introduced by &lt;a href="http://arxiv.org/abs/1308.0850"&gt;Alex Graves in 2014&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Alex Graves uses online&lt;sup id="fnref:online"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/011/#fn:online"&gt;2&lt;/a&gt;&lt;/sup&gt; data from the IAM database (&lt;a href="https://ieeexplore.ieee.org/document/1575685"&gt;Liwicki &amp;amp; Bunke, 2005&lt;/a&gt;) and an &lt;a href="https://en.wikipedia.org/wiki/Long_short-term_memory"&gt;LSTM (Long Short-Term Memory)&lt;/a&gt; to train a model capable of generating series of coordinates that trace letters and words. Initially, the model simply generates random series of letters and words, but it is then improved to take into account a text prompt which forces the models to generate a specific series of letters. As described before, the model also takes a weight (or bias) which normalizes the likelihood of the letters' shape, and can take a "priming line": the image of a handwritten line, whose writing style the model will try to copy. Once the coordinates are generated (including key information such as "ends-of-stroke"), it is easy to place them in an SVG file and visualize the result, with or without animation.&lt;/p&gt;
&lt;p&gt;There are many many implementations of Alex Graves's experiment because it was such an important publication to demonstrate the usefulness of LSTM models. Several can be found on Github if you &lt;a href="https://github.com/search?q=alex%20graves&amp;amp;type=repositories"&gt;search "Alex Graves"&lt;/a&gt;. For my experiment, I didn't want to develop my own adaptation of such a model, but rather to use programs that were ready to be used. This is the reason why I didn't look for papers but instead for recent (or recently updated) repositories on Github. I focused on Python programs because I wanted to be able to understand how they were developed.&lt;/p&gt;
&lt;p&gt;One very promising implementation of Alex Graves' proposition was Evgenii Dolotov's &lt;a href="https://github.com/X-rayLaser/pytorch-handwriting-synthesis-toolkit"&gt;pytorch-handwriting-synthesis-toolkit&lt;/a&gt;. It came with pre-trained models, and a utility scripts to feed the program a text prompt and generate an image. I &lt;a href="https://github.com/alix-tz/pytorch-handwriting-synthesis-toolkit/tree/custom"&gt;customized&lt;/a&gt;&lt;sup id="fnref:customization"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/011/#fn:customization"&gt;3&lt;/a&gt;&lt;/sup&gt; the program a bit to fix a few bugs and try to make it generate several lines maintaining the same handwriting.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/PHST_fail_and_success.png" alt="4 lines stating (or supposed to state) 'did a computer write this' generated by Evgenii Dolotov's program. The fourth line is a failed attempt where several letters like y, n, m can be dinstiguished. The fifth line states 'determined to act upon the assumptions' but contains several garbled letters." widht="600px"&gt;&lt;/p&gt;
&lt;p&gt;Even though the generated images were sometimes impressively realistic, it created a lot of bad output. As suggested by Alex Graves, his solution tends to generate what he calls "garbled letters", letters that no human would likely trace. In other cases, it would randomly skip some letters and be completely incapable of tracing some numbers or punctuation signs. Sometimes, the model would simply draw more or less flat lines. Since I wanted to generate fake &lt;em&gt;gold&lt;/em&gt; data that I could trust and since the results were not reliable enough, I played with the bias and the priming lines before trying to train new models using Evgennii Dolotov's utility scripts. I failed to get better results than the pre-trained models, and failed to find the correct parameters to make sure I would obtain always realistic output.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/PHST_fail_multiline.png" alt="several flat lines that at one point successfully write 'is fin'. This is a failed generated image." width="100px"&gt;&lt;/p&gt;
&lt;!-- change image to add more examples --&gt;

&lt;p&gt;At this point I started exploring &lt;a href="https://en.wikipedia.org/wiki/Generative_adversarial_network"&gt;GANs (Generative Adversarial Networks)&lt;/a&gt; which are models based on game theory. They are capable of generating realistic fake images learning from samples of real images (see &lt;a href="https://doi.org/10.1145/3422622"&gt;Goodfellow et al., 2020&lt;/a&gt;). They are this kind of models used to generate photos of &lt;a href="https://this-person-does-not-exist.com/en"&gt;people who don't exist&lt;/a&gt;. There are Github repositories offering source code to train such models to generate fake handwriting, such as &lt;a href="https://github.com/omni-us/research-GANwriting"&gt;GANwriting&lt;/a&gt; (described in &lt;a href="http://arxiv.org/abs/2003.02567"&gt;Kang et al., 2020&lt;/a&gt;) or Amazon's &lt;a href="https://github.com/amzn/convolutional-handwriting-gan"&gt;ScrabbleGAN&lt;/a&gt; (introduced in &lt;a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fogel_ScrabbleGAN_Semi-Supervised_Varying_Length_Handwritten_Text_Generation_CVPR_2020_paper.html"&gt;Fogel et al., 2020&lt;/a&gt;) but they were only giving instructions to reproduce the corresponding papers and train the models ourselves. Since GANs are costly to train, I left this option out for the moment, even though I do think they can become an interesting solution in the future.&lt;/p&gt;
&lt;p&gt;Eventually, I settled for a solution based on a &lt;a href="https://en.wikipedia.org/wiki/Diffusion_model"&gt;Diffusion model&lt;/a&gt;. This type of model can be found behind applications like &lt;a href="https://openai.com/product/dall-e-2"&gt;OpenAI's DALL-E&lt;/a&gt;. Luhman &amp;amp; Luhman (2020), who created the &lt;a href="https://github.com/tcl9876/Diffusion-Handwriting-Generation"&gt;Diffusion Handwriting Generation&lt;/a&gt; (later called DHG), explain very well how diffusion models work.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Diffusion probabilistic models [...] convert a known distribution (e.g. Gaussian) into a more complex data distribution. A diffusion process converts the data distribution into a simple distribution by iteratively adding Gaussian noise to the data, and the generative model learns to reverse this diffusion process." (Luhman &amp;amp; Luhman, 2020, p. 1)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A great advantage with DHG compared to the LSTM approach was that it was possible to easily fix the priming line and almost always obtain a convincing output. This was essential to create a dataset with a consistent handwriting over hundred of lines. As visible in the following image, even if the diffusion model is not capable of perfectly imitating the handwriting contained in the priming line, it usually successfully captures elements of style such as the slant, or the cursive nature of the text.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/DHG_generated.png" alt="five pairs of priming lines with the resulting generated lines." width="800px"&gt;&lt;/p&gt;
&lt;p&gt;After several tests, I found that the third priming line gave the best results when associated with different text prompts, so I decided to use it along with excerpts from &lt;a href="https://www.gutenberg.org/files/2701/2701-0.txt"&gt;Moby Dick&lt;/a&gt; to create a completely artificially generated dataset. In a few days, I created more than 8,000 images (PNG) associated with a text file (TXT) containing the prompts used to generate them.&lt;/p&gt;
&lt;p&gt;These pairs could have been used "as is" to produce a silver synthetic dataset but, like I said before, I needed a gold dataset where the text and the images would be exact matches. Unfortunately, more than a third of the images did not qualify as gold. After manually reviewing about 2,500 of the lines (with the help of my colleague Hugo Scheithauer), we published a set of 1,280 pairs of lines and text under the name "&lt;a href="https://github.com/alix-tz/spinnerbait"&gt;Spinnerbait&lt;/a&gt;".&lt;/p&gt;
&lt;p&gt;Even though I was able to produce a dataset meeting my main criteria, I was actually disappointed with my results: I wanted a sort of magic button which would allow me to generate, at any time and without having to review it, a perfect set of training data. Instead, in the future, if I want to add more lines to Spinnerbait, I will have to spend a few hours going through each line to filter the bad ones out.&lt;/p&gt;
&lt;p&gt;On the other hand, I decided to take a few hours to manually copy a text taken from Guillaume Apollinaire's poems. I copied the text following a txt file that I would edit every time I would start a new line, I scanned it, segmented it with eScriptorium before copying and pasting the lines from the txt file and exported the result as a series of XML ALTO and images. It gave birth to the &lt;a href="https://github.com/alix-tz/moonshines"&gt;Moonshines&lt;/a&gt; dataset, a set of 1,186 lines (including 170 dedicated to a fixed test subset) of a single hand, thus comparable in size to Spinnerbaits.&lt;/p&gt;
&lt;p&gt;I think generating both datasets took about the same amount of time, if I take into account on the one hand reviewing the generated lines and on the other hand copying the text and passing it through eScriptorium. Moonshines used less computing resources and produced a richer dataset if we consider the aspect of the text. Also, the length of the lines is more varied in Moonshines whereas it is more homogenous (max 5 words) in Spinnerbait, because the generator tended to make more errors on longer prompts.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://alix-tz.github.io/phd/images/spinnerbait_moonshines.png" alt="a line taken from the Spinnerbait dataset and a line taken from the Moonshines dataset" widht="600px"&gt;&lt;/p&gt;
&lt;p&gt;Another important limitation that I have barely addressed at this point it that not only do these tools fail to draw non-ASCII characters, but they also tend to have a greater chance of producing garbled letters when prompted with rare or non-english words&lt;sup id="fnref:iam"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/011/#fn:iam"&gt;4&lt;/a&gt;&lt;/sup&gt;. This is true of all the systems I have tested. Of course, we could imagine training new models on data containing a greater diversity of languages, or simply other scripts or languages.&lt;/p&gt;
&lt;p&gt;As way of a conclusion, I would say that even though I was disappointed with what I obtained down the line, this exploratory adventure was very interesting. I learned a lot and I am convinced that if I had more time and resources (and if it were more crucial for me), I would have found a way to get better results. I know of some coming publications that used GANs to create artificial data that look like lines taken from historical documents and I really look forward reading them.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:icfhr"&gt;
&lt;p&gt;It is possible to find &lt;a href="https://youtu.be/Tuw8uQonW7E?t=145"&gt;here&lt;/a&gt; a recording of the talk given on this tool at ICFHR 2020. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/011/#fnref:icfhr" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:online"&gt;
&lt;p&gt;In the context of handwritten text recognition, a distinction is made between "online" data and "offline" data. Offline data are based on a matrix of pixels containing the image of a text (they are static), whereas online data are vectors containing information about the speed, the points through which a line passes to form a letter, end of stroke points, etc. Online HTR uses data generated with an e-pen and a screen while offline HTR uses images created with a scanner or a camera. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/011/#fnref:online" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:customization"&gt;
&lt;p&gt;One of the customizations consisted in removing non-ASCII characters or characters not supported by the model. It was easy to apply this transformation because in pytorch-handwriting-synthesis-toolkit, each model comes with a little metadata file which contains the character set handled by the model. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/011/#fnref:customization" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:iam"&gt;
&lt;p&gt;All the models were trained using the IAM database, more often the "online" database, but sometimes also with the "offline" version. &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/011/#fnref:iam" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>experiment</category><category>HTR</category><category>synthetic data</category><guid>https://alix-tz.github.io/phd/posts/011/</guid><pubDate>Sun, 21 May 2023 18:12:26 GMT</pubDate></item><item><title>010 - Make and Read the docs</title><link>https://alix-tz.github.io/phd/posts/010/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;During my last contract as a research engineer at Inria, I spent a lot of my time working on the project called &lt;a href="https://lectaurep.hypotheses.org/"&gt;LECTAUREP&lt;/a&gt;, in collaboration with the &lt;a href="https://www.archives-nationales.culture.gouv.fr/"&gt;National Archives in France&lt;/a&gt;. The goal of this project was to explore new ways to index the content of thousands of thousands of notary registries which, put together, form &lt;a href="https://www.siv.archives-nationales.culture.gouv.fr/siv/cms/content/fonds.action?uuid=12b&amp;amp;template=pog/pogLevel2&amp;amp;preview=false"&gt;one of the most used collections&lt;/a&gt; of the National Archives. I joined the project at the end of 2019, during its second phase, almost at the same time as &lt;a href="https://gitlab.com/scripta/escriptorium"&gt;eScriptorium&lt;/a&gt; was initiated. LECTAUREP had worked with &lt;a href="https://readcoop.eu/transkribus/?sc=Transkribus"&gt;Transkribus&lt;/a&gt; during the first phase (in 2018) but, given the connections between &lt;a href="http://almanach.inria.fr/index-en.html"&gt;my research team&lt;/a&gt; and the team behind eScriptorium, we quickly switched to the newer software and contributed to its development.  &lt;/p&gt;
&lt;p&gt;One of my most important contribution is the redaction of &lt;a href="https://lectaurep.hypotheses.org/documentation/prendre-en-main-escriptorium"&gt;a tutorial for the software&lt;/a&gt;, which was initially only intended as an internal resource for our team of annotators. This is the reason why the tutorial was published on LECTAUREP's blog. &lt;a href="https://openiti.org/"&gt;OpenITI&lt;/a&gt;, and in particular &lt;a href="https://twitter.com/Mar_Musa"&gt;Jonathan Allen&lt;/a&gt; rapidly offered &lt;a href="https://lectaurep.hypotheses.org/documentation/escriptorium-tutorial-en"&gt;an English translation&lt;/a&gt; which, eventually, was also published on LECTAUREP's blog. Since the publication of this translation, it is listed on eScriptorium's home page as its official tutorial.  &lt;/p&gt;
&lt;p&gt;Unfortunately, the tutorial hasn't been updated in a long time whereas major updates and new features have been added on eScriptorium's side.  &lt;/p&gt;
&lt;p&gt;LECTAUREP's blog is not a good solution. It is built with &lt;a href="https://wordpress.com/"&gt;Wordpress&lt;/a&gt; and hosted by &lt;a href="https://hypotheses.org/"&gt;Hypotheses&lt;/a&gt; which is very convenient to allow a small, well defined, group of people to collaboratively work on a research blog, but it's too heavy and not adapted to publish the documentation of a software like eScriptorium. The documentation needs to be updated frequently to keep up with the software and, in general, a blog is not a place to publish the extensive documentation of a software. To top it all, it is not even that easy to update for me, so can you imagine someone outside of LECTAUREP trying to offer an update?  &lt;/p&gt;
&lt;p&gt;I have been thinking of finding a better solution since at least 2020, but it was never so urgent that I was able to put it at the top of my to-do lists. Last Summer, I took the advantage of a rather slow couple of weeks in August, when every one but me seemed to have gone on vacations, to put something different in place.  &lt;/p&gt;
&lt;p&gt;&lt;a href="https://readthedocs.org/"&gt;Readthedocs&lt;/a&gt; quickly appeared to me as an ideal solution: the platform is designed for publishing software documentations, it handles software versions and multi-lingual contents. Last but not least, it uses static website generators. This is fundamental because it allows for the publication of the source code on a platforms like &lt;a href="https://github.com/"&gt;Github&lt;/a&gt; and will actually use this public source code to build the website.  &lt;/p&gt;
&lt;p&gt;Github is a platform designed for sharing and opening codes to external contributors. Relying on it solves a major issue with the current tutorial: if anyone can suggest the correction, edition or translation of eScriptorium's documentation, then it is more likely to keep up with the evolutions of the application!  &lt;/p&gt;
&lt;p&gt;In August, I created a new Github repository called &lt;a href="https://github.com/alix-tz/escriptorium-documentation"&gt;escriptorium-documentation&lt;/a&gt;. I &lt;a href="https://docs.readthedocs.io/en/stable/tutorial/#getting-started"&gt;set a basic configuration and connected it to Readthedocs&lt;/a&gt;. As soon as this was done, the website became available at &lt;a href="https://escriptorium.readthedocs.io/"&gt;online&lt;/a&gt; with a URL based on the following structure: &lt;code&gt;{gh_repo_name}.readthedocs.io&lt;/code&gt;. Then, I started rewriting the content of the tutorial... following &lt;a href="https://docs.readthedocs.io/en/stable/intro/getting-started-with-sphinx.html"&gt;Sphinx' syntax&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;It was so painful that I never got back to it after I came back from my own vacations.  &lt;/p&gt;
&lt;p&gt;Why painful? Well, I had discovered Markdown in 2017 and I have used it since. It's so powerful and yet so light! In comparison, Sphinx felt like such a complicated and heavy syntax. Not as heavy as HTML, but less intuitive nonetheless. I had to go through the documentation every time I wanted to add something as simple as a hyperlink or an image!  &lt;/p&gt;
&lt;p&gt;In January, when I gathered enough motivation&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/010/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; to go back to working on eScriptorium's tutorial, I decided to look for an alternative to &lt;a href="https://www.sphinx-doc.org/en/master/usage/builders/index.html#sphinx.builders.html.DirectoryHTMLBuilder"&gt;Sphinx compilers&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;The only non-sphinx-based option available with readthedocs is &lt;a href="https://docs.readthedocs.io/en/stable/intro/getting-started-with-mkdocs.html"&gt;Mkdocs&lt;/a&gt;. Like its name hints at, &lt;a href="https://www.mkdocs.org/"&gt;Mkdocs&lt;/a&gt; is a Markdown compiler, capable to quickly build websites. The set-up is really quick, it's well documented, fairly easy to customize and it's possible to add a lot of &lt;a href="https://squidfunk.github.io/mkdocs-material/setup/extensions/python-markdown-extensions/#emoji"&gt;cool extensions which are based on Python&lt;/a&gt;. It was the bomb!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I liked Mkdocs so much that I also used it to rebuild &lt;a href="https://alix-tz.github.io/"&gt;my personal website&lt;/a&gt;!&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://alix-tz.github.io/phd/posts/010/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Over the past month, I have spent a lot of time working on this new tutorial for eScriptorium. I designed a basic structure, breaking down the features into different categories. Now the pages are progressively being filled and I am very happy to have been joined in my efforts by my colleagues Hugo Scheithauer and Floriane Chiffoleau. As we progressively merge the content of new pages to the main branch, the &lt;a href="https://escriptorium.readthedocs.io/en/latest/"&gt;escriptorium-tutorial&lt;/a&gt; website expands. It will be ready soon for an official release!  &lt;/p&gt;
&lt;p&gt;I really hope that the transparency and simplicity brought by Mkdocs and Markdown will allow many people to add their contributions to the documentation of eScriptorium! Who knows, maybe you will too!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT: we changed the name of the repository to escriptorium-documentation instead of escriptorium-tutorial (all links and mentions were changed in this post). The decision was motivated by the fact the "tutorial" felt like an inexact description of the actual scope/ambition of the project.&lt;/em&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Also when I got more free time after &lt;a href="https://alix-tz.github.io/phd/posts/009/"&gt;my classes were over&lt;/a&gt;! &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/010/#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;It is not necessary to use readthedocs to deploy a website built with Mkdocs. In the case of the tutorial, it simply allows us to have a domain name more meaningful than ".github.io". &lt;a class="footnote-backref" href="https://alix-tz.github.io/phd/posts/010/#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>eScriptorium</category><category>house cleaning</category><category>HTR</category><category>software documentation</category><category>static website</category><guid>https://alix-tz.github.io/phd/posts/010/</guid><pubDate>Tue, 28 Feb 2023 10:41:40 GMT</pubDate></item><item><title>009 - Looking back to 2022</title><link>https://alix-tz.github.io/phd/posts/009/</link><dc:creator>Alix Chagué</dc:creator><description>&lt;p&gt;Over the past decade, I've started a tradition of taking a sort-of-day-long hike on the 1st of January. There is always a chance that the weather won't be on my side, but I like the idea of starting the year peacefully and embracing nature.  &lt;/p&gt;
&lt;p&gt;Before starting 2023 though, and without much originality, I wanted to dedicate a last post for 2022 to looking back to my first full year of phD and focus on its biggest highlights as far as my phD project is concerned.  &lt;/p&gt;
&lt;p&gt;Of course, the year sure didn't look like anything I had expected! I don't think I had envisioned any of what my trips to Montréal have brought me... but also how much they consume of the time available to actually work on my research project. As you will see, it is not necessarily a bad thing - I actually want to focus on the good parts - but it doesn't mean that it was easy to wrap my head around it.  &lt;/p&gt;
&lt;p&gt;I spent two thirds of the 2022 in Montréal: 8 months out of 12, with what looks right now like a super short 4 months of Summer in France in the middle. I specify that it was Summer because, you know, ... July and August, they're not usual months.&lt;/p&gt;
&lt;p&gt;Overall, 2022 passed in the blink of an eye.  &lt;/p&gt;
&lt;p&gt;I spent a lot of time in class or preparing for class or working on final projects. Indeed, I took 4 classes out of the 5 required by the Université de Montréal, and attended 1 as an auditor.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SCI6304 was a bibliometry class taught by Vincent Larivière;&lt;/li&gt;
&lt;li&gt;MSL6523 was a Digital Museology class conducted by Emmanuel Chateau-Dutier;&lt;/li&gt;
&lt;li&gt;LCO6525 was a Compared Chinese Literature class given by Victoria-Oana Lupascu;&lt;/li&gt;
&lt;li&gt;HNU7000 was a class focused on the Epistemology of the Digital Humanities lead by Michael E. Sinatra;&lt;/li&gt;
&lt;li&gt;and SCI6203, the extra class, was about AI and textual data, it was given by Dominic Forest.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you can see, they were very diverse but I think I learned quite a few things from them that I will be able to use more or less directly for my research.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In HNU7000, we often had to write or present critical summaries of articles or conferences and in LCO6525, we were asked to turn in a "annotated bibliography" (at least 400 words for 6 references related to the topic of our final paper) halfway through the semester. With these exercises, I think I found a better way to summarize articles or books I read. I can still improve the "critical" aspect of the exercise, but I think I got better at summarizing what was actually relevant to me when I read. Now, I hope to publish some of these future summaries here.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In SCI6304, my final project consisted in a bibliometric investigation on the publications on HTR over the past 40 years. I would like to publish the result as an article, which is a project for 2023. But I can already see that doing so, I gained a much better understanding of the field(s) related to my research project, I found some keywords that I need to investigate and I added a bunch of references to my potentially-to-read list.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In MSL6523, the final poject consisted in a blog post for &lt;a href="https://medium.com/museonum"&gt;Museonum&lt;/a&gt;. &lt;a href="https://medium.com/museonum/intelligence-artificielle-et-intelligence-collective-des-nouveaux-eldorados-pour-rendre-les-c8c4e214d4e6"&gt;Mine&lt;/a&gt; was focused on the current use of automatic transcription and crowdsourcing by patrimonial institutions. I have already been able to reuse this post as a reading suggestion for a class where I was invited to present eScriptorium.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I had a lot of other activities throughout the year (teaching, presenting at conferences, etc), but, as far as my thesis project is concerned, two of my biggest concerns were 1) reading and 2) refining my research project.  &lt;/p&gt;
&lt;p&gt;I tried different strategies to read more, but I am a slow reader and my schedule is often fragmented. I have started finding the beginning of a solution in keeping one day of the week totally without meetings, and by always keeping it the same day. I started implementing this during the past semester and it seemed to work: Wednesday was the day I was going to the library or simply dedicating to my homework. On Wednesday, I never scheduled meetings (unless absolutely necessary). Now that I am back in France for 4 months, I need to make sure I am able to keep this routine. On the other hand, I am trying to solve the speed problem by better sorting the references I end up reading in order not to waste time on a text that was actually not a priority.  &lt;/p&gt;
&lt;p&gt;One of the things I learned in 2022 and that affects my reading is the usefulness of expanding the types of publications I read. Let me explain. Working at ALMAnaCH, I saw most of my colleagues focus on reading articles and pre-prints. The main objective there is to keep up to date with a state of the art that evolves quickly and for a discipline that is fairly recent. On the other hand, at the Université de Montréal, more emphasis is placed on the conceptual frame of the research. Therefore, many more books and chapters, of disciplines that are not always the same as the envisioned project, make their way into the bibliographies. It might seem obvious to some of you, but it wasn't for me at the start of the year and this is exactly the type of enrichment I was looking for when I went for a &lt;em&gt;cotutelle&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;Now, as far as refining my research project is concerned, this is something that I didn't start solving before December. At the Université de Montréal, when I apply for a grant, I often have to introduce my research project, including my methodology and the conceptual frame. Writing these texts, it often made me feel like I was not progressing in terms of making my project more precise. I had a general idea of the topic I wanted to work on and which issues I wanted to address in general, but no clear strategy.  &lt;/p&gt;
&lt;p&gt;I often thought about it during the year, looking for a way to solve this problem. It wasn't a question of narrowing down my topic or use-case, I think mine is/are already well defined. It is only at the occasion of a 20 minutes presentation assigned during the HNU7000 seminar that I sat at a table and put together my thoughts and the results of my discussions with my supervisors. I will soon dedicate a post to the current state of my research project, but what I think I was lacking the most was &lt;strong&gt;an angle&lt;/strong&gt; to efficiently narrow down the scope of my project and justify future choices.  &lt;/p&gt;
&lt;p&gt;Even though I started focusing on how different 2022 looked compared to what I had imagined, I am actually very happy with this year. I met a ton of people who have broadened my horizons on research and academia in general. And I start 2023 full of ideas for my phD and this blog! Keep an eye out for them in future posts!&lt;/p&gt;</description><category>courses</category><guid>https://alix-tz.github.io/phd/posts/009/</guid><pubDate>Sat, 31 Dec 2022 15:14:57 GMT</pubDate></item></channel></rss>