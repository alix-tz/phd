<!DOCTYPE html>
<html prefix="" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>012 - "It did a very good job" | A research (b)log</title>
<link href="https://alix-tz.github.io/phd/assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="https://alix-tz.github.io/phd/assets/css/code.css" rel="stylesheet" type="text/css">
<link href="https://alix-tz.github.io/phd/assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Share+Tech+Mono" rel="stylesheet">
<!-- custom font --><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Bungee&amp;family=Goldman&amp;family=JetBrains+Mono:wght@200&amp;family=Ubuntu+Mono&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro&amp;family=Syne&amp;display=swap" rel="stylesheet">
<!-- end of custom font --><link href="https://alix-tz.github.io/phd/assets/css/custom.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="https://alix-tz.github.io/phd/rss.xml">
<link rel="canonical" href="https://alix-tz.github.io/phd/posts/012/">
<link rel="icon" href="https://alix-tz.github.io/phd/posts/012/favicon.ico" sizes="32x32">
<!--[if lt IE 9]><script src="https://alix-tz.github.io/phd/assets/js/html5.js"></script><![endif]--><meta name="author" content="Alix Chagu√©">
<link rel="prev" href="https://alix-tz.github.io/phd/posts/011/" title="011 - Working with synthetic data" type="text/html">
<link rel="next" href="https://alix-tz.github.io/phd/posts/013/" title="013 - The Peraire experiment" type="text/html">
<meta property="og:site_name" content="A research (b)log">
<meta property="og:title" content='012 - "It did a very good job"'>
<meta property="og:url" content="https://alix-tz.github.io/phd/posts/012/">
<meta property="og:description" content="A few weeks ago, I attended the presentation of an automatic transcription software. The majority of the audience was unfamiliar with the concept of handwritten text recognition (HTR) or had little ex">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-07-15T08:06:20-04:00">
<meta property="article:tag" content="accuracy">
<meta property="article:tag" content="evaluation">
<meta property="article:tag" content="HTR">
<meta property="article:tag" content="metrics">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <div id="container">
         
    <header id="header"><h1 id="brand"><a href="https://alix-tz.github.io/phd/" title="A research (b)log" rel="home">

        <span id="blog-title">A research (b)log</span>
    </a></h1>

        

        
    <nav id="menu"><ul>
<li><a href="https://alix-tz.github.io/phd/archive.html">Archive</a></li>
                <li><a href="https://alix-tz.github.io/phd/categories/">Tags</a></li>
                <li><a href="https://alix-tz.github.io/phd/rss.xml">RSS feed</a></li>

    

    
    
    </ul></nav></header><main id="content"><article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="https://alix-tz.github.io/phd/posts/012/" class="u-url">012 - "It did a very good job"</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Alix Chagu√©
            </span></p>
            <p class="dateline">
            <a href="https://alix-tz.github.io/phd/posts/012/" rel="bookmark">
            <time class="published dt-published" datetime="2023-07-15T08:06:20-04:00" itemprop="datePublished" title="2023-07-15">2023-07-15</time></a>
            </p>
            
        <p class="sourceline"><a href="https://alix-tz.github.io/phd/posts/012/index.md" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>A few weeks ago, I attended the presentation of an automatic transcription software. The majority of the audience was unfamiliar with the concept of handwritten text recognition (HTR) or had little experience using it. The presentation lasted only an hour, so it couldn't delve into much detail. Its main objective was to demonstrate the software's results. The presenter showed several slides, displaying on one side  images of manuscripts (often in a language unknown to the audience) and on the other side the transcriptions generated by the software. Throughout the presentation, the presenter repeatedly commented on the HTR software saying that "it did a very good job."</p>
<p>But what does it even mean?</p>
<p>The very first aspect to explore is what distinguishes a good job from a bad one. Normally, such an evaluation relies on the measurement of the accuracy of the result compared to the ideal transcription. The accuracy can be expressed positively or negatively using the error rates (a 0% error rate is the same as a 100% accuracy).</p>
<p>Measuring the accuracy of a prediction (another way to call the result of HTR) is commonly done at character level. The character accuracy of a model is equal to the number of matches between the prediction and the ideal transcription. The character error rate (CER) is a very common measure to express a model's theoretical efficiency.</p>
<p>Sometimes softwares also consider the word error rate (WER), which is the proportion of words in the prediction containing errors. A high score at WER doesn't actually mean that the transcription is bad. It only means that the errors are distributed on all the words. I never use WER alone because it is hard to get an exact impression of the quality of the prediction based on that metric alone.</p>
<p>There is a paper from <a href="https://dl.acm.org/doi/10.1145/3476887.3476888">Neudecker et al. (2021)</a> where they test 5 different software used for evaluating the prediction. They also develop an interesting reflection on alternative metrics such as the "non-stopword accuracy", the "phrase accuracy", the "flexible character accuracy" (which is useful when the line order isn't always the same), the "figure of merit" (which "aims to quantify the effort required for manual post-correction" (p. 15)) or else the "unordered WER".</p>
<p>When your score is a rate, there is an implicit idea that 100% is both the maximum score and the targeted score (for accuracy of course). But in the case of HTR, 100% accuracy is extremely rare because there are also edge cases where the way a letter was drawn is ambiguous: in such cases the error is not particularly caused by the inaccuracy of the HTR engine but rather by the imperfection of the handwriting in the first place.</p>
<p>In <a href="https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.46">Hodel et al., (2021)</a>, the authors provided a grid to interpret accuracy scores. They suggest the following three thresholds:</p>
<ul>
<li>CER &lt; 10% == good (it allows efficient post-processing)</li>
<li>CER &lt; 5% == very good (errors are usually focused on rare or unknown words)</li>
<li>CER &lt; 2.5% == excellent (but it is usually only reached when the handwriting is very regular)</li>
</ul>
<p>Personally, I think this grid should also include 20% and 0%. 20% as a threshold, because at 80% of accuracy, the transcription is supposedly good enough for fuzzy search and keyword spotting (I should add a reference here, but I can't find it anymore...); and 0% because it should be reminded that an accuracy of 100% is virtually impossible.</p>
<p>To complement this, I would like to mention another possible approach to get an interpretable score: during the DH2023 conference, Thibault Cl√©rice and I <a href="https://inria.hal.science/hal-04094241">presented an experiment</a> where we trained a model using the same data in the train set and the test set. Our model reached an accuracy close to 90%, which we were able to use as a baseline to define the highest accuracy score possible for the data we had. Thus we were able to consider that a model approaching 90% of accuracy would be an excellent model, as far as that dataset was concerned.</p>
<p>Still during <a href="https://www.conftool.pro/dh2023/index.php?page=browseSessions&amp;form_session=76#paperID395">the DH2023 conference</a>, Wouter Haverals introduced <a href="https://github.com/WHaverals/CERberus">CERberus üê∂üê∂üê∂</a>, a web interface which addresses the same type of issues as <a href="https://huggingface.co/spaces/lterriel/kami-app">KaMI</a>: the lack of nuance in a plain CER computation. Indeed, in a CER score, every type of error has the same weight. This means that mistaking an "e" for a "√©" costs the same as mistaking a "e" for a "0": in the first case the text is likely still readable or understandable, whereas in the latter, it might not be the case.</p>
<p>The CER metric is still very useful, but when applied to transcription projects, it is even more valuable when we can filter the types of errors we want to include in the evaluation.</p>
<p><em>EDIT: I should have noted here that my reflection was focused on the evaluation of an automatic transcription in cases where you already have the expected transcription. When we apply an HTR model to a whole new set of documents, we usually don't have the correct transcription at hand (otherwise we wouldn't use HTR in the first place). This is the reason why many researchers try to find ways to evaluate the quality of the transcription without ground truth. One example can be found in <a href="https://enc.hal.science/hal-03828529">Cl√©rice (2022)</a>.</em></p>
<p>So, to go back to our initial problem, we can see that there are many ways to draw the line between a good job and a bad one. The threshold will depend on the metric used to express the accuracy of the prediction and also (and actually mostly) on the way the generated text will be used down the line. Even though the software presentation I attended was short, I think we should always remind future users of HTR that 100% of accuracy is not always what they are seeking.</p>
<p>A short reflection to finish this post: I was bothered by the expression used to qualify the transcription. I am still trying to figure out a way to put it into words. On top of lacking accuracy, the expression "it did a good job" was also calling for a vision of HTR as a magic tool at the service of the searchers and students. But, in which other cases do you say that someone did "a good job?" Likely when you delegate a task to a <a href="https://africanarguments.org/2023/03/the-invisible-labour-of-africa-in-the-digital-revolution/">subaltern</a>.</p>
<p>I see a problem here: in their current state, HTR engines are efficient but not to the point that people can use them without thinking clearly about what they want the engine to produce. It is easy to sell a software pretending that it is a magic servant that will do all the transcription in your place, a tool so smart that you can even consider delegating a part of your responsibility to it. But I think when new users of HTR fail to first reflect on the outcome they can reasonably expect from these engines, it creates disappointment and crappy data and workflows.</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="https://alix-tz.github.io/phd/categories/accuracy/" rel="tag">accuracy</a></li>
            <li><a class="tag p-category" href="https://alix-tz.github.io/phd/categories/evaluation/" rel="tag">evaluation</a></li>
            <li><a class="tag p-category" href="https://alix-tz.github.io/phd/categories/htr/" rel="tag">HTR</a></li>
            <li><a class="tag p-category" href="https://alix-tz.github.io/phd/categories/metrics/" rel="tag">metrics</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="https://alix-tz.github.io/phd/posts/011/" rel="prev" title="011 - Working with synthetic data">Previous post</a>
            </li>
            <li class="next">
                <a href="https://alix-tz.github.io/phd/posts/013/" rel="next" title="013 - The Peraire experiment">Next post</a>
            </li>
        </ul></nav></aside></article></main><footer id="footer"><p>Contents ¬© 2023         <a href="https://alix-tz.github.io/phd/">Alix Chagu√©</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a> -        CC-BY<br></p>
<div>
<h3>Contact me</h3>
<form name="contact" action="https://formspree.io/mdozpzwz" method="POST">
<div>
<!--<label class="sr-only" for="inputName">Name</label>-->
<input type="text" name="name" id="inputName" placeholder="Your name" required="" style="width: 150px;">
¬†¬†¬†
<!--<label class="sr-only" for="inputEmail">E-mail</label>-->
<input type="email" name="email" id="inputEmail" placeholder="Your e-mail address" required="" style="width: 150px;">
</div>
<div>
<!--<label class="sr-only" for="inputMessage">Message</label>-->
<textarea name="message" id="inputMessage" rows="3" placeholder="Your message" required="" style="width: 50%;"></textarea>
</div>
<button type="submit" style="width: 60px;">Send</button>
</form>
</div>
            
        </footer>
</div>
    
    

    
    
    
</body>
</html>
